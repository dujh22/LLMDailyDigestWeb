<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>多模态 - LLM-DailyDigest</title><meta name="author" content="">
<meta name="author-link" content="">
<meta name="description" content="多模态

上海人工智能实验室、北京理工大学、上海创智学院、东京大学等机构 聚焦世界生成的第一步——世界探索 ，联合推出一个持续迭代的高质量视频数据集项目——Sekai https://mp.weixin.qq.com/s/gNcdw9cu7LDXowtrlrtx-g
谢赛宁团队新作：不用提示词精准实现3D画面控制https://mp.weixin.qq.com/s/QxCaooIDYWFtQMFr_Otqdw
GitHub一周2000星！国产统一图像生成模型神器升级，理解质量双up，还学会了“反思”https://mp.weixin.qq.com/s/WpDHa_YxZIWT4xrXA9sAjA新进展来自智源研究院：一模支持文生图、图像编辑、主题驱动图像生成的 OmniGen ，2.0新版本正式发布。
大模型时代，通用视觉模型将何去何从？https://mp.weixin.qq.com/s/yJ6U367pd1o6pqzEs1xi_Q** 清华大学自动化系鲁继文团队**最近发表于 IJCV 的综述论文系统梳理了该方向的研究进展，涵盖输入统一方法、任务通用策略、模型框架设计、模型评测应用等内容，希望能为未来视觉模型的发展提供参考与启发。
2025-06-30 16:13:39 Monday ｜ 充分激发模态协作，MokA量身打造MLLM微调新范式


论文标题：MokA: Multimodal Low-Rank Adaptation for MLLMs
论文链接：https://arxiv.org/abs/2506.05191
项目主页：https://gewu-lab.github.io/MokA

MokA 在结构上继承了 LoRA 的核心思想，以保持高效的优点。但基于多模态场景对于 A、B 投影矩阵的角色进行了重新定义。如上图所示，MokA 包括三个关键模块：模态特异的 A 矩阵，跨模态注意力机制和模态共享的 B 矩阵。

2025-06-13 18:17:54 Friday | 何恺明改进了谢赛宁的REPA：极大简化但性能依旧强悍https://mp.weixin.qq.com/s/t-fjuuLsCO0kywKrflve9w
CVPR 2025 Award Candidate | 英伟达等Difix3D&#43;：用单步扩散模型修复 3D 重建伪影 机器之心 2025年06月23日 12:05

https://mp.weixin.qq.com/s/C9XQZDuI1D9mQmyiSsOTvg
来自英伟达的研究团队联合提出了一种创新方案 —— Difix3D&#43;，通过单步扩散模型对 3D 渲染结果进行 “图像修复”，显著提升新视角图像的质量和一致性" /><meta name="keywords" content='Hugo, FixIt' />
  <meta itemprop="name" content="多模态">
  <meta itemprop="description" content="多模态 上海人工智能实验室、北京理工大学、上海创智学院、东京大学等机构 聚焦世界生成的第一步——世界探索 ，联合推出一个持续迭代的高质量视频数据集项目——Sekai https://mp.weixin.qq.com/s/gNcdw9cu7LDXowtrlrtx-g 谢赛宁团队新作：不用提示词精准实现3D画面控制https://mp.weixin.qq.com/s/QxCaooIDYWFtQMFr_Otqdw GitHub一周2000星！国产统一图像生成模型神器升级，理解质量双up，还学会了“反思”https://mp.weixin.qq.com/s/WpDHa_YxZIWT4xrXA9sAjA新进展来自智源研究院：一模支持文生图、图像编辑、主题驱动图像生成的 OmniGen ，2.0新版本正式发布。 大模型时代，通用视觉模型将何去何从？https://mp.weixin.qq.com/s/yJ6U367pd1o6pqzEs1xi_Q** 清华大学自动化系鲁继文团队**最近发表于 IJCV 的综述论文系统梳理了该方向的研究进展，涵盖输入统一方法、任务通用策略、模型框架设计、模型评测应用等内容，希望能为未来视觉模型的发展提供参考与启发。 2025-06-30 16:13:39 Monday ｜ 充分激发模态协作，MokA量身打造MLLM微调新范式 论文标题：MokA: Multimodal Low-Rank Adaptation for MLLMs 论文链接：https://arxiv.org/abs/2506.05191 项目主页：https://gewu-lab.github.io/MokA MokA 在结构上继承了 LoRA 的核心思想，以保持高效的优点。但基于多模态场景对于 A、B 投影矩阵的角色进行了重新定义。如上图所示，MokA 包括三个关键模块：模态特异的 A 矩阵，跨模态注意力机制和模态共享的 B 矩阵。
2025-06-13 18:17:54 Friday | 何恺明改进了谢赛宁的REPA：极大简化但性能依旧强悍https://mp.weixin.qq.com/s/t-fjuuLsCO0kywKrflve9w CVPR 2025 Award Candidate | 英伟达等Difix3D&#43;：用单步扩散模型修复 3D 重建伪影 机器之心 2025年06月23日 12:05 https://mp.weixin.qq.com/s/C9XQZDuI1D9mQmyiSsOTvg
来自英伟达的研究团队联合提出了一种创新方案 —— Difix3D&#43;，通过单步扩散模型对 3D 渲染结果进行 “图像修复”，显著提升新视角图像的质量和一致性">
  <meta itemprop="datePublished" content="2025-08-12T00:00:00+08:00">
  <meta itemprop="dateModified" content="2025-08-12T00:00:00+08:00">
  <meta itemprop="wordCount" content="410"><meta property="og:url" content="http://localhost:1313/topic/%E5%A4%9A%E6%A8%A1%E6%80%81/">
  <meta property="og:site_name" content="LLM-DailyDigest">
  <meta property="og:title" content="多模态">
  <meta property="og:description" content="多模态 上海人工智能实验室、北京理工大学、上海创智学院、东京大学等机构 聚焦世界生成的第一步——世界探索 ，联合推出一个持续迭代的高质量视频数据集项目——Sekai https://mp.weixin.qq.com/s/gNcdw9cu7LDXowtrlrtx-g 谢赛宁团队新作：不用提示词精准实现3D画面控制https://mp.weixin.qq.com/s/QxCaooIDYWFtQMFr_Otqdw GitHub一周2000星！国产统一图像生成模型神器升级，理解质量双up，还学会了“反思”https://mp.weixin.qq.com/s/WpDHa_YxZIWT4xrXA9sAjA新进展来自智源研究院：一模支持文生图、图像编辑、主题驱动图像生成的 OmniGen ，2.0新版本正式发布。 大模型时代，通用视觉模型将何去何从？https://mp.weixin.qq.com/s/yJ6U367pd1o6pqzEs1xi_Q** 清华大学自动化系鲁继文团队**最近发表于 IJCV 的综述论文系统梳理了该方向的研究进展，涵盖输入统一方法、任务通用策略、模型框架设计、模型评测应用等内容，希望能为未来视觉模型的发展提供参考与启发。 2025-06-30 16:13:39 Monday ｜ 充分激发模态协作，MokA量身打造MLLM微调新范式 论文标题：MokA: Multimodal Low-Rank Adaptation for MLLMs 论文链接：https://arxiv.org/abs/2506.05191 项目主页：https://gewu-lab.github.io/MokA MokA 在结构上继承了 LoRA 的核心思想，以保持高效的优点。但基于多模态场景对于 A、B 投影矩阵的角色进行了重新定义。如上图所示，MokA 包括三个关键模块：模态特异的 A 矩阵，跨模态注意力机制和模态共享的 B 矩阵。
2025-06-13 18:17:54 Friday | 何恺明改进了谢赛宁的REPA：极大简化但性能依旧强悍https://mp.weixin.qq.com/s/t-fjuuLsCO0kywKrflve9w CVPR 2025 Award Candidate | 英伟达等Difix3D&#43;：用单步扩散模型修复 3D 重建伪影 机器之心 2025年06月23日 12:05 https://mp.weixin.qq.com/s/C9XQZDuI1D9mQmyiSsOTvg
来自英伟达的研究团队联合提出了一种创新方案 —— Difix3D&#43;，通过单步扩散模型对 3D 渲染结果进行 “图像修复”，显著提升新视角图像的质量和一致性">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="topic">
    <meta property="article:published_time" content="2025-08-12T00:00:00+08:00">
    <meta property="article:modified_time" content="2025-08-12T00:00:00+08:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="多模态">
  <meta name="twitter:description" content="多模态 上海人工智能实验室、北京理工大学、上海创智学院、东京大学等机构 聚焦世界生成的第一步——世界探索 ，联合推出一个持续迭代的高质量视频数据集项目——Sekai https://mp.weixin.qq.com/s/gNcdw9cu7LDXowtrlrtx-g 谢赛宁团队新作：不用提示词精准实现3D画面控制https://mp.weixin.qq.com/s/QxCaooIDYWFtQMFr_Otqdw GitHub一周2000星！国产统一图像生成模型神器升级，理解质量双up，还学会了“反思”https://mp.weixin.qq.com/s/WpDHa_YxZIWT4xrXA9sAjA新进展来自智源研究院：一模支持文生图、图像编辑、主题驱动图像生成的 OmniGen ，2.0新版本正式发布。 大模型时代，通用视觉模型将何去何从？https://mp.weixin.qq.com/s/yJ6U367pd1o6pqzEs1xi_Q** 清华大学自动化系鲁继文团队**最近发表于 IJCV 的综述论文系统梳理了该方向的研究进展，涵盖输入统一方法、任务通用策略、模型框架设计、模型评测应用等内容，希望能为未来视觉模型的发展提供参考与启发。 2025-06-30 16:13:39 Monday ｜ 充分激发模态协作，MokA量身打造MLLM微调新范式 论文标题：MokA: Multimodal Low-Rank Adaptation for MLLMs 论文链接：https://arxiv.org/abs/2506.05191 项目主页：https://gewu-lab.github.io/MokA MokA 在结构上继承了 LoRA 的核心思想，以保持高效的优点。但基于多模态场景对于 A、B 投影矩阵的角色进行了重新定义。如上图所示，MokA 包括三个关键模块：模态特异的 A 矩阵，跨模态注意力机制和模态共享的 B 矩阵。
2025-06-13 18:17:54 Friday | 何恺明改进了谢赛宁的REPA：极大简化但性能依旧强悍https://mp.weixin.qq.com/s/t-fjuuLsCO0kywKrflve9w CVPR 2025 Award Candidate | 英伟达等Difix3D&#43;：用单步扩散模型修复 3D 重建伪影 机器之心 2025年06月23日 12:05 https://mp.weixin.qq.com/s/C9XQZDuI1D9mQmyiSsOTvg
来自英伟达的研究团队联合提出了一种创新方案 —— Difix3D&#43;，通过单步扩散模型对 3D 渲染结果进行 “图像修复”，显著提升新视角图像的质量和一致性">
<meta name="application-name" content="LLM-DailyDigest">
<meta name="apple-mobile-web-app-title" content="LLM-DailyDigest"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" href="http://localhost:1313/topic/%E5%A4%9A%E6%A8%A1%E6%80%81/" /><link rel="prev" href="http://localhost:1313/topic/%E6%95%B0%E6%8D%AE/" /><link rel="next" href="http://localhost:1313/topic/%E8%87%AA%E6%88%91%E8%BF%9B%E5%8C%96/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "多模态",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "http:\/\/localhost:1313\/topic\/%E5%A4%9A%E6%A8%A1%E6%80%81\/"
    },"genre": "topic","wordcount":  410 ,
    "url": "http:\/\/localhost:1313\/topic\/%E5%A4%9A%E6%A8%A1%E6%80%81\/","datePublished": "2025-08-12T00:00:00+08:00","dateModified": "2025-08-12T00:00:00+08:00","publisher": {
      "@type": "Organization",
      "name": ""},"author": {
        "@type": "Person",
        "name": "Author"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="sticky" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper" data-page-style="normal"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper">
    <div class="header-title">
      <a href="/" title="LLM-DailyDigest"><img loading="lazy" src="/fixit.svg" data-title="LLM-DailyDigest" data-alt="LLM-DailyDigest" class="logo" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}"/><span id="typeit-header-desktop" class="typeit"></span></a><span class="header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/posts/llm-dailydigest"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 详情</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/resources/"
                
                
              >资源</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/topic/"
                
                
              >主题</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/updates/"
                
                
              >日报</a></li><li class="menu-item delimiter"></li><li class="menu-item theme-switch" title="Switch Theme">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li></ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="LLM-DailyDigest"><img loading="lazy" src="/fixit.svg" data-title="/fixit.svg" data-alt="/fixit.svg" class="logo" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}"/><span id="typeit-header-title-mobile" class="typeit"></span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/posts/llm-dailydigest"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 详情</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/resources/"
                  
                  
                >资源</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/topic/"
                  
                  
                >主题</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/updates/"
                  
                  
                >日报</a></li><li class="menu-item menu-system">
          <span class="menu-system-item theme-switch" title="Switch Theme"><i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i></span></li>
      </ul>
    </nav>
  </div>
</header><main class="container"><aside class="toc" id="toc-auto"><h2 class="toc-title">Contents&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden="true"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom">
    </aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX"><span>多模态</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      </span></span></div>
      <div class="post-meta-line"><span title="published on 2025-08-12 00:00:00"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden="true"></i><time datetime="2025-08-12">2025-08-12</time></span>&nbsp;<span title="Updated on 2025-08-12 00:00:00"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden="true"></i><time datetime="2025-08-12">2025-08-12</time></span>&nbsp;<span title="410 words"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden="true"></i>About 500 words</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden="true"></i>2 minutes</span>&nbsp;</div>
    </div><div class="details toc" id="toc-static" data-kept="false">
        <div class="details-summary toc-title">
          <span>Contents</span><i class="details-icon fa-solid fa-angle-right" aria-hidden="true"></i></div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#综述">综述</a></li>
    <li><a href="#模型">模型</a>
      <ul>
        <li>
          <ul>
            <li><a href="#小红书开源首个多模态大模型dotsvlm1性能直追sota"><a href="https://mp.weixin.qq.com/s/aftyEJCZleUGAp0s5NBk3w">小红书开源首个多模态大模型dots.vlm1，性能直追SOTA！</a></a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#图片理解与生成">图片理解与生成</a></li>
    <li><a href="#视频">视频</a>
      <ul>
        <li>
          <ul>
            <li><a href="#fastwan系视频生成模型">FastWan系视频生成模型</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#视频声音">视频+声音</a></li>
    <li><a href="#3d">3D</a></li>
    <li><a href="#长度扩展">长度扩展</a></li>
    <li><a href="#语音">语音</a></li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><h1 id="多模态">多模态</h1>
<ol>
<li>上海人工智能实验室、北京理工大学、上海创智学院、东京大学等机构 <strong>聚焦世界生成的第一步——世界探索</strong> ，联合推出一个<strong>持续迭代的高质量视频数据集项目——Sekai</strong><a href="https://mp.weixin.qq.com/s/gNcdw9cu7LDXowtrlrtx-g"target="_blank" rel="external nofollow noopener noreferrer"> https://mp.weixin.qq.com/s/gNcdw9cu7LDXowtrlrtx-g</a></li>
<li>谢赛宁团队新作：不用提示词精准实现3D画面控制<a href="https://mp.weixin.qq.com/s/QxCaooIDYWFtQMFr_Otqdw"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/QxCaooIDYWFtQMFr_Otqdw</a></li>
<li>GitHub一周2000星！国产统一图像生成模型神器升级，理解质量双up，还学会了“反思”<a href="https://mp.weixin.qq.com/s/WpDHa_YxZIWT4xrXA9sAjA"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/WpDHa_YxZIWT4xrXA9sAjA</a>新进展来自智源研究院：一模支持文生图、图像编辑、主题驱动图像生成的 <strong>OmniGen</strong> ，2.0新版本正式发布。</li>
<li>大模型时代，通用视觉模型将何去何从？<a href="https://mp.weixin.qq.com/s/yJ6U367pd1o6pqzEs1xi_Q"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/yJ6U367pd1o6pqzEs1xi_Q</a>** 清华大学自动化系鲁继文团队**最近发表于 IJCV 的综述论文系统梳理了该方向的研究进展，涵盖输入统一方法、任务通用策略、模型框架设计、模型评测应用等内容，希望能为未来视觉模型的发展提供参考与启发。</li>
<li>2025-06-30 16:13:39 Monday ｜ 充分激发模态协作，MokA量身打造MLLM微调新范式</li>
</ol>
<ul>
<li>论文标题：MokA: Multimodal Low-Rank Adaptation for MLLMs</li>
<li>论文链接：https://arxiv.org/abs/2506.05191</li>
<li>项目主页：https://gewu-lab.github.io/MokA</li>
</ul>
<p>MokA 在结构上继承了 LoRA 的核心思想，以保持高效的优点。但基于多模态场景对于 A、B 投影矩阵的角色进行了重新定义。如上图所示，MokA 包括三个关键模块：模态特异的 A 矩阵，跨模态注意力机制和模态共享的 B 矩阵。</p>
<ol start="6">
<li>2025-06-13 18:17:54 Friday | 何恺明改进了谢赛宁的REPA：极大简化但性能依旧强悍https://mp.weixin.qq.com/s/t-fjuuLsCO0kywKrflve9w</li>
<li>CVPR 2025 Award Candidate | 英伟达等Difix3D+：用单步扩散模型修复 3D 重建伪影 机器之心 2025年06月23日 12:05</li>
</ol>
<p><a href="https://mp.weixin.qq.com/s/C9XQZDuI1D9mQmyiSsOTvg"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/C9XQZDuI1D9mQmyiSsOTvg</a></p>
<p>来自英伟达的研究团队联合提出了一种创新方案 —— Difix3D+，通过单步扩散模型对 3D 渲染结果进行 “图像修复”，显著提升新视角图像的质量和一致性</p>
<h2 id="综述">综述</h2>
<p>2025-07-03 09:45:11 Thursday ｜ 回顾并总结纯视觉范式下的通用视觉模型研究 <a href="https://mp.weixin.qq.com/s/yJ6U367pd1o6pqzEs1xi_Q"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/yJ6U367pd1o6pqzEs1xi_Q</a></p>
<ol>
<li><strong>清华大学自动化系鲁继文团队</strong>最近发表于 IJCV 的综述论文系统梳理了该方向的研究进展，涵盖输入统一方法、任务通用策略、模型框架设计、模型评测应用等内容，希望能为未来视觉模型的发展提供参考与启发。</li>
<li>论文标题：Vision Generalist Model: A Survey</li>
<li>论文链接：https://arxiv.org/abs/2506.09954</li>
<li>挑战：如何优化统一框架设计、提高训练效率和应对大规模数据等挑战</li>
</ol>
<p>2025-07-02 13:03:30 Wednesday ｜ UofT、UBC、MIT和复旦等联合发布：扩散模型驱动的异常检测与生成全面综述 <a href="https://mp.weixin.qq.com/s/smDyNhlGS-SpM2Wt34J-ng"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/smDyNhlGS-SpM2Wt34J-ng</a></p>
<h2 id="模型">模型</h2>
<h4 id="小红书开源首个多模态大模型dotsvlm1性能直追sota"><a href="https://mp.weixin.qq.com/s/aftyEJCZleUGAp0s5NBk3w"target="_blank" rel="external nofollow noopener noreferrer">小红书开源首个多模态大模型dots.vlm1，性能直追SOTA！</a></h4>
<ol>
<li>
<p>小红书人文智能实验室（Humane Intelligence Lab，hi lab）在昨天低调开源了视觉语言模型dots.vlm1</p>
<ol>
<li>Github Repo：https://github.com/rednote-hilab/dots.vlm1</li>
<li>Huggingface Model：https://huggingface.co/rednote-hilab/dots.vlm1.inst</li>
<li>Demo ：https://huggingface.co/spaces/rednote-hilab/dots-vlm1-demo</li>
</ol>
</li>
<li>
<p>不论是空间关系理解、复杂图表推理、OCR识别、高考题评测、STEM难题、写诗等各个方面，dots.vlm1的表现都远超预期。</p>
</li>
<li>
<p>dots.vlm1由三个核心组件构成：一个全自研的12亿参数的NaViT视觉编码器、一个轻量级的MLP适配器，以及DeepSeek V3 MoE大语言模型。这一架构通过三阶段流程进行训练：<strong>第一阶段：视觉编码器预训练</strong> ：NaViT编码器从头训练，旨在最大化对多样视觉数据的感知能力。一般来说，编码器是否自研是VLM模型性能的分水岭。dots.vlm1再次验证了这一点。<strong>第二阶段：VLM预训练</strong> ：将视觉编码器与DeepSeek V3 LLM联合训练，使用大规模、多样化的多模态数据集。<strong>第三阶段：VLM后训练</strong> ：通过有监督微调（SFT）增强模型的泛化能力，仅使用任务多样的数据进行训练。</p>
</li>
</ol>
<h2 id="图片理解与生成">图片理解与生成</h2>
<p>2025-07-03 11:28:29 Thursday ｜ 超CLIP准确率11%！伯克利港大阐明「LLM文本-视觉」对齐深层机制 <a href="https://mp.weixin.qq.com/s/YCOGZBHbkdmcUF92xHOHYQ"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/YCOGZBHbkdmcUF92xHOHYQ</a></p>
<p>近期，相关工作尝试将预训练的大语言模型（LLM）作为文本编码器融入多模态对齐框架，并在分类和检索任务上观察到性能提升。</p>
<p>然而，性能提升背后的机制尚不清晰，几个关键问题仍未得到系统解答：</p>
<ul>
<li>能力提升的本质：LLM文本编码器的加入究竟增强了多模态模型的哪些具体能力？</li>
<li>数据特征的适配：在哪些类型的训练数据上，LLM文本编码器表现更优，原因为何？</li>
<li>关键组件的贡献：LLM文本编码器的哪些设计选择对跨模态对齐至关重要？</li>
<li>训练流程的简化：若使用LLM作为固定文本编码器，传统对比学习框架能否进一步优化？</li>
</ul>
<p>来自UC伯克利和香港大学的研究团队在最新工作LIFT（Language-Image Alignment with Fixed Text Encoders）中，对上述问题进行了系统性解答。</p>
<p>论文链接：https://arxiv.org/pdf/2506.04209</p>
<p>项目代码：https://github.com/Jingfeng0705/LIFT</p>
<p>该方法采用极简训练范式——直接冻结预训练LLM作为文本编码器，仅优化图像编码器。</p>
<ol start="2">
<li>2025-07-01 10:50:12 Tuesday ｜ 用好视觉Attention局部性，清华、字节提出Token Reorder，无损实现5倍稀疏、4比特量化 <a href="https://mp.weixin.qq.com/s/2ZWAdLD-_XdOz3kL3GRNjw"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/2ZWAdLD-_XdOz3kL3GRNjw</a></li>
</ol>
<p>提出了一种简单且硬件友好的离线 “Token重排” 方案以实现注意力模式的统一化，并设计了针对性的稀疏与量化方法，配合高效的 CUDA 系统设计，展现了更优异的算法性能保持与硬件效率提升</p>
<p>论文标题：PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models</p>
<p>论文链接：https://arxiv.org/abs/2506.16054</p>
<p>项目主页：https://a-suozhang.xyz/paroattn.github.io/</p>
<ol start="3">
<li>2025-06-30 15:49:57 Monday ｜ ICML 2025 Spotlight | 新理论框架解锁流匹配模型的引导生成 <a href="https://mp.weixin.qq.com/s/c4rDuHgOfLF3n3TxWXv2mw"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/c4rDuHgOfLF3n3TxWXv2mw</a></li>
</ol>
<p>对生成模型的 <strong>能量引导</strong> （energy guidance）是一种可控的生成方法，它构造灵活，适用于各种任务，且允许无额外训练条件生成模型。同时 <strong>流匹配</strong> （flow matching）框架作为一种生成模型，近期在分子生成、图片生成等领域中已经展现出巨大潜力。</p>
<ul>
<li>本工作首次提出了流匹配能量引导理论框架。</li>
<li>在本框架指导下，本工作提出三大类无需训练的实用流匹配能量引导算法，并可将经典扩散模型能量引导算法包含为特例。</li>
<li>本工作给出了各个流匹配能量引导算法性能的理论分析和实验比较，为实际应用提供指导。</li>
<li>论文标题：On the Guidance of Flow Matching</li>
<li>论文链接：https://arxiv.org/abs/2502.02150</li>
<li>项目地址：https://github.com/AI4Science-WestlakeU/flow_guidance</li>
</ul>
<ol start="4">
<li>2025-06-23 12:06:35 Monday ｜ 开源版MetaQuery来了！OpenUni用1.1B参数媲美BLIP3-o-8B，数据代码完全开源 <a href="https://mp.weixin.qq.com/s/AsKAqA8NJE-S132cfM44kg"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/AsKAqA8NJE-S132cfM44kg</a></li>
</ol>
<p>随着 GPT-4o 展现出令人印象深刻的多模态能力，将视觉理解和图像生成统一到单一模型中已成为 AI 领域的研究趋势（如MetaQuery 和 BLIP3-o ）。</p>
<p>南洋理工大学 S-Lab 和商汤科技的研究团队推出 OpenUni，一个开源版 MetaQuery，仅用 1.1B 参数达到 8B 模型性能，更将代码、权重、数据全部开源！</p>
<p>技术报告： OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation</p>
<p>机构： 南洋理工大学 S-Lab、商汤科技新加坡研究院</p>
<p>作者： Size Wu*,  Zhonghua Wu*, Zerui Gong* (* 同等贡献), Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy</p>
<p>开源代码：</p>
<p><a href="https://github.com/wusize/OpenUni"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/wusize/OpenUni</a></p>
<p>联系方式： <a href="mailto:size001@e.ntu.edu.sg">size001@e.ntu.edu.sg</a></p>
<h2 id="视频">视频</h2>
<p>2025-07-03 10:24:07 Thursday ｜ 画到哪，动到哪！字节跳动发布视频生成「神笔马良」ATI，已开源！ <a href="https://mp.weixin.qq.com/s/plT9DzxmpAjQv8u87S8wlQ"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/plT9DzxmpAjQv8u87S8wlQ</a></p>
<p>字节跳动提出了 <strong>ATI</strong> ——一种全新的、以「轨迹为指令」的可控视频生成框架。ATI 的核心理念是： <strong>将用户在输入图像上手绘的任意轨迹，转化为驱动物体与摄像机运动的显式控制信号，并以统一的潜在空间建模方式注入视频生成过程。</strong> 这使得视频创作从「参数调控」转变为「可视化创意」，让用户「画到哪，动到哪」，以直观方式实现帧级精准控制。</p>
<p>Title：ATI: Any Trajectory Instruction for Controllable Video Generation</p>
<p>Paper：https://arxiv.org/pdf/2505.22944</p>
<p>Project page：https://anytraj.github.io/</p>
<p>Github：https://github.com/bytedance/ATI</p>
<p>Hugging Face：https://huggingface.co/bytedance-research/ATI</p>
<p>ComfyUI：https://github.com/kijai/ComfyUI-WanVideoWrapper</p>
<ol start="2">
<li>2025-06-30 16:02:37 Monday ｜ 无需训练，即插即用，2倍GPU端到端推理加速——视频扩散模型加速方法DraftAttention <a href="https://mp.weixin.qq.com/s/VbWe5u5WdQfKLYOohzCZaA"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/VbWe5u5WdQfKLYOohzCZaA</a></li>
</ol>
<p>现有视频生成加速方法，如 Sparse VideoGen（https://arxiv.org/abs/2502.01776）和 AdaSpa（https://arxiv.org/abs/2502.21079），多采用稀疏注意力机制，在 GPU 上实现了一定程度的端到端加速</p>
<p>Adobe Research 等机构的研究团队提出了一种 <strong>无需训练、即插即用的，基于动态稀疏注意力的视频扩散模型加速方法 ——DraftAttention</strong> ，显著降低了注意力机制的计算开销，并且在几乎不损失生成质量的前提下，实现高达 2 倍的 GPU 端到端推理加速。</p>
<p>论文标题：</p>
<p>DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance</p>
<p>arXiv 地址：</p>
<p><a href="https://arxiv.org/abs/2505.14708"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2505.14708</a></p>
<p>GitHub 主页：</p>
<p><a href="https://github.com/shawnricecake/draft-attention"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/shawnricecake/draft-attention</a></p>
<ol start="3">
<li>2025-06-20 11:24:24 Friday ｜ 极低成本微调大规模预训练视频模型https://mp.weixin.qq.com/s/MCOrbgJvqWwFnYmavSde6w</li>
</ol>
<p>论文标题：Redefining Temporal Modeling in Video Diffusion: The Vectorized Timestep Approach</p>
<p>FVDM 论文：https://arxiv.org/abs/2410.03160</p>
<p>Pusa 主页 / 代码库: <a href="https://github.com/Yaofang-Liu/Pusa-VidGen"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Yaofang-Liu/Pusa-VidGen</a></p>
<h4 id="fastwan系视频生成模型">FastWan系视频生成模型</h4>
<ol>
<li><a href="https://mp.weixin.qq.com/s/gx76pTlySt0Wd0vnhdby3Q"target="_blank" rel="external nofollow noopener noreferrer">一夜颠覆Sora神话，H200单卡5秒出片！全华人团队开源AI引爆视频圈</a>
<ol>
<li>论文地址：https://arxiv.org/pdf/2505.13389</li>
<li>它的核心采用了「稀疏蒸馏」全新的训练方案，实现了高效生成，让视频去噪速度实现70倍飙升。</li>
</ol>
</li>
</ol>
<h2 id="视频声音">视频+声音</h2>
<p>2025-06-28 18:46:35 Saturday ｜ 音画同步，AI视频也能有完美「原声音」，可灵AI刚上线的！https://mp.weixin.qq.com/s/8h5sZUIxsmce9GRRMDkEBg</p>
<p>他们提出的多模态视频生音效模型名叫 Kling-Foley，能够通过大模型自动生成与视频内容同步的高质量立体声音频。</p>
<p>Kling-Foley 支持基于视频内容与可选文本提示自动生成与视频画面语义相关、时间同步的高质量立体声音频，涵盖音效、背景音乐等多种类型声音内容。它支持生成任意时长的音频内容，还具备立体声渲染的能力，支持空间定向的声源建模和渲染。</p>
<ul>
<li>论文：https://www.arxiv.org/pdf/2506.19774</li>
<li>项目主页：https://klingfoley.github.io/Kling-Foley/</li>
<li>GitHub 链接：https://github.com/klingfoley/Kling-Foley</li>
<li>Benchmark：https://huggingface.co/datasets/klingfoley/Kling-Audio-Eval</li>
</ul>
<h2 id="3d">3D</h2>
<ol>
<li><a href="https://mp.weixin.qq.com/s/h8WVaV_3CXKmHc0UdrLGTA"target="_blank" rel="external nofollow noopener noreferrer">拿下3D生成行业新标杆！昆仑万维Matrix-3D新模型鲨疯了，一张图建模游戏场景</a></li>
</ol>
<h2 id="长度扩展">长度扩展</h2>
<p>2025-06-30 16:18:28 Monday ｜ 打破长视频理解瓶颈：HoPE混合位置编码提升VLM长度泛化能力 <a href="https://mp.weixin.qq.com/s/KQHGw8_v0rEY8pS7jufRbQ"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/KQHGw8_v0rEY8pS7jufRbQ</a></p>
<p>现有多模态 RoPE 泛化能力不足的原因之一是保留 RoPE 中所有频率对长上下文语义建模有负面影响。基于此分析，他们提出的混合位置编码（HoPE, Hybrid of Position Embedding）大幅提升了 VLM 的长度泛化能力，在长视频理解和检索等任务中达到最优表现。</p>
<ul>
<li>论文标题：HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models</li>
<li>arXiv 链接：https://arxiv.org/pdf/2505.20444</li>
<li>代码链接：https://github.com/hrlics/HoPE</li>
</ul>
<h2 id="语音">语音</h2>
<ol start="13">
<li>2025-06-18 08:20:08 Wednesday ｜</li>
</ol>
<p>首个全面梳理语音大模型发展脉络的权威综述，入选ACL 2025主会</p>
<p><a href="https://mp.weixin.qq.com/s/sIa9qIzPuykCysAVgeGxew"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/sIa9qIzPuykCysAVgeGxew</a></p>
<ol start="2">
<li>2025-07-01 10:56:55 Tuesday ｜ ACL 2025 | AI字幕慢半拍，不知道大家在笑什么？新方法让同传性能直逼离线翻译 <a href="https://mp.weixin.qq.com/s/laRnBuuTfF-olWKv42TAgw"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/laRnBuuTfF-olWKv42TAgw</a></li>
</ol>
<p>该方法<strong>将同传任务巧妙地建模为序贯决策过程，通过优化完整的决策序列，显著提升了翻译质量，同时有效控制了延迟，其性能直逼、甚至在某些方面超越了同等大小的离线翻译模型。</strong></p>
<ul>
<li>论文标题: SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation</li>
<li>论文链接：https://arxiv.org/pdf/2505.20622</li>
</ul>
</div></article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">Public - 2025</span><span class="author" itemprop="copyrightHolder">
              <a href="/"></a></span></div><div class="footer-line statistics"></div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="Back to Top"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric d-none">0%</span>
        </div></div><div id="mask"></div><noscript>
    <div class="noscript-warning">Theme FixIt works best with JavaScript enabled.</div>
  </noscript>
</div><link rel="preload" href="/lib/katex/katex.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/katex/katex.min.css"></noscript><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/typeit/index.umd.js" defer></script><script src="/lib/katex/katex.min.js" defer></script><script src="/lib/katex/auto-render.min.js" defer></script><script src="/lib/katex/copy-tex.min.js" defer></script><script src="/lib/katex/mhchem.min.js" defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script>window.config={"code":{"copyTitle":"Copy to clipboard","editLockTitle":"Lock editable code block","editUnLockTitle":"Unlock editable code block","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-desktop":"LLM-DailyDigest 大模型研究日报","typeit-header-title-mobile":"LLM-DailyDigest 大模型研究日报"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-desktop":["typeit-header-desktop"],"typeit-header-title-mobile":["typeit-header-title-mobile"]},"duration":-1,"loop":false,"speed":100}};</script><script src="/js/theme.min.js" defer></script></body>
</html>
