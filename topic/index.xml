<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Topics on LLM-DailyDigest</title>
    <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/</link>
    <description>Recent content in Topics on LLM-DailyDigest</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 13 Aug 2025 00:00:00 +0800</lastBuildDate>
    <atom:link href="https://dujh22.github.io/LLMDailyDigest.github.io/topic/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>自我进化 Self-Evolve</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E8%87%AA%E6%88%91%E8%BF%9B%E5%8C%96/</link>
      <pubDate>Wed, 13 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E8%87%AA%E6%88%91%E8%BF%9B%E5%8C%96/</guid>
      <description>&lt;h1 id=&#34;self-evolve&#34;&gt;Self-Evolve&lt;/h1&gt;&#xA;&lt;h2 id=&#34;2025-08-08&#34;&gt;2025-08-08&lt;/h2&gt;&#xA;&lt;h4 id=&#34;r-zero从零数据自我进化推理-llm&#34;&gt;R-Zero：从零数据自我进化推理 LLM&lt;/h4&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2508.05004&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;#65&lt;/a&gt; &lt;a href=&#34;https://papers.cool/arxiv/2508.05004&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;R-Zero: Self-Evolving Reasoning LLM from Zero Data&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Chengsong Huang、Wenhao Yu、Xiaoyang Wang、Hongming Zhang、Zongxia Li、Ruosen Li、Jiaxin Huang、Haitao Mi、Dong Yu&lt;/p&gt;&#xA;&lt;p&gt;Self-evolving Large Language Models (LLMs) offer a scalable path toward super-intelligence by autonomously generating, refining, and learning from their own experiences. However, existing methods for training such models still rely heavily on vast human-curated tasks and labels, typically via fine-tuning or reinforcement learning, which poses a fundamental bottleneck to advancing AI systems toward capabilities beyond human intelligence. To overcome this limitation, we introduce R-Zero, a fully autonomous framework that generates its own training data from scratch. Starting from a single base LLM, R-Zero initializes two independent models with distinct roles, a Challenger and a Solver. These models are optimized separately and co-evolve through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solver capability, and the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. This process yields a targeted, self-improving curriculum without any pre-existing tasks and labels. Empirically, R-Zero substantially improves reasoning capability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.&#xA;自我进化的大型语言模型（LLMs）通过自主生成、改进并从自身经验中学习，提供了一条通往超智能的可扩展路径。然而，现有训练此类模型的方法仍然在很大程度上依赖大量人工策划的任务和标签，通常通过微调或强化学习来实现，这对将人工智能系统推进到超越人类智能的能力构成了根本性瓶颈。&lt;/p&gt;</description>
    </item>
    <item>
      <title>多模态</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E5%A4%9A%E6%A8%A1%E6%80%81/</link>
      <pubDate>Tue, 12 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E5%A4%9A%E6%A8%A1%E6%80%81/</guid>
      <description>&lt;h1 id=&#34;多模态&#34;&gt;多模态&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;上海人工智能实验室、北京理工大学、上海创智学院、东京大学等机构 &lt;strong&gt;聚焦世界生成的第一步——世界探索&lt;/strong&gt; ，联合推出一个&lt;strong&gt;持续迭代的高质量视频数据集项目——Sekai&lt;/strong&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/gNcdw9cu7LDXowtrlrtx-g&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt; https://mp.weixin.qq.com/s/gNcdw9cu7LDXowtrlrtx-g&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;谢赛宁团队新作：不用提示词精准实现3D画面控制&lt;a href=&#34;https://mp.weixin.qq.com/s/QxCaooIDYWFtQMFr_Otqdw&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/QxCaooIDYWFtQMFr_Otqdw&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;GitHub一周2000星！国产统一图像生成模型神器升级，理解质量双up，还学会了“反思”&lt;a href=&#34;https://mp.weixin.qq.com/s/WpDHa_YxZIWT4xrXA9sAjA&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/WpDHa_YxZIWT4xrXA9sAjA&lt;/a&gt;新进展来自智源研究院：一模支持文生图、图像编辑、主题驱动图像生成的 &lt;strong&gt;OmniGen&lt;/strong&gt; ，2.0新版本正式发布。&lt;/li&gt;&#xA;&lt;li&gt;大模型时代，通用视觉模型将何去何从？&lt;a href=&#34;https://mp.weixin.qq.com/s/yJ6U367pd1o6pqzEs1xi_Q&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/yJ6U367pd1o6pqzEs1xi_Q&lt;/a&gt;** 清华大学自动化系鲁继文团队**最近发表于 IJCV 的综述论文系统梳理了该方向的研究进展，涵盖输入统一方法、任务通用策略、模型框架设计、模型评测应用等内容，希望能为未来视觉模型的发展提供参考与启发。&lt;/li&gt;&#xA;&lt;li&gt;2025-06-30 16:13:39 Monday ｜ 充分激发模态协作，MokA量身打造MLLM微调新范式&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;论文标题：MokA: Multimodal Low-Rank Adaptation for MLLMs&lt;/li&gt;&#xA;&lt;li&gt;论文链接：https://arxiv.org/abs/2506.05191&lt;/li&gt;&#xA;&lt;li&gt;项目主页：https://gewu-lab.github.io/MokA&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;MokA 在结构上继承了 LoRA 的核心思想，以保持高效的优点。但基于多模态场景对于 A、B 投影矩阵的角色进行了重新定义。如上图所示，MokA 包括三个关键模块：模态特异的 A 矩阵，跨模态注意力机制和模态共享的 B 矩阵。&lt;/p&gt;&#xA;&lt;ol start=&#34;6&#34;&gt;&#xA;&lt;li&gt;2025-06-13 18:17:54 Friday | 何恺明改进了谢赛宁的REPA：极大简化但性能依旧强悍https://mp.weixin.qq.com/s/t-fjuuLsCO0kywKrflve9w&lt;/li&gt;&#xA;&lt;li&gt;CVPR 2025 Award Candidate | 英伟达等Difix3D+：用单步扩散模型修复 3D 重建伪影 机器之心 2025年06月23日 12:05&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/C9XQZDuI1D9mQmyiSsOTvg&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/C9XQZDuI1D9mQmyiSsOTvg&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;来自英伟达的研究团队联合提出了一种创新方案 —— Difix3D+，通过单步扩散模型对 3D 渲染结果进行 “图像修复”，显著提升新视角图像的质量和一致性&lt;/p&gt;</description>
    </item>
    <item>
      <title>应用</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E5%BA%94%E7%94%A8/</link>
      <pubDate>Tue, 12 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E5%BA%94%E7%94%A8/</guid>
      <description>&lt;h1 id=&#34;应用&#34;&gt;应用&lt;/h1&gt;&#xA;&lt;h2 id=&#34;社会学研究&#34;&gt;社会学研究&lt;/h2&gt;&#xA;&lt;h3 id=&#34;辩论&#34;&gt;辩论&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;2025-07-03 11:37:35 Thursday ｜&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://papers.cool/arxiv/2507.01936&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;LLM 中理解和说服之间的细线&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;[PDF()]&lt;/strong&gt; &lt;strong&gt;[Copy]&lt;/strong&gt; &lt;strong&gt;[Kimi()]&lt;/strong&gt; &lt;strong&gt;[REL]&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt; : &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Adrian%20de%20Wynter&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Adrian de Wynter&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Tangming%20Yuan&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Tangming Yuan&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;大型语言模型 （LLM） 在维护高层次、令人信服的对话方面非常出色。它们正在被快速部署为敏感领域的聊天机器人和评估器，例如同行评审和心理健康应用程序。这一点，以及关于他们推理能力的不同描述，需要对 LLM 及其对对话的理解进行更仔细的检查。在这项工作中，我们首先评估了 LLM 维持辩论的能力——这是人类交流的最纯粹但最复杂的形式之一。然后我们衡量这种能力如何与他们对所谈论内容的理解相关，即他们对对话结构和语用上下文的理解。我们发现 LLM 能够保持连贯、有说服力的辩论，经常动摇参与者和听众的信念。我们还注意到，对 AI 参与的认识或怀疑会鼓励人们对所提出的论点持更多批评态度。然而，当对 LLM 进行民意调查时，他们无法证明这种理解。我们的研究结果将 LLM 作为评估者的缺点与他们理解上下文的（不）能力联系起来。更广泛地说，对于论证论领域，我们假设，如果一个代理人能够令人信服地维持对话，那么它就没有必要知道它在说什么。因此，语用背景和连贯性的建模是次于有效性的。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;科目&lt;/strong&gt; :  &lt;strong&gt;&lt;a href=&#34;https://papers.cool/arxiv/cs.CL&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;计算和语言&lt;/a&gt;&lt;/strong&gt; , &lt;a href=&#34;https://papers.cool/arxiv/cs.CY&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;计算机与社会&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;决策&#34;&gt;决策&lt;/h3&gt;&#xA;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;2025-07-03 11:38:51 Thursday ｜&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://papers.cool/arxiv/2507.01923&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;面向决策的文本评估&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;[PDF()]&lt;/strong&gt; &lt;strong&gt;[Copy]&lt;/strong&gt; &lt;strong&gt;[Kimi()]&lt;/strong&gt; &lt;strong&gt;[REL]&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt; : &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Yu-Shiang%20Huang&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Yu-Shiang Huang&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Chuan-Ju%20Wang&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Chuan-Ju Wang&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Chung-Chi%20Chen&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Chung-Chi Chen&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>推理</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E6%8E%A8%E7%90%86/</link>
      <pubDate>Tue, 12 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E6%8E%A8%E7%90%86/</guid>
      <description>&lt;h1 id=&#34;推理&#34;&gt;推理&lt;/h1&gt;&#xA;&lt;h2 id=&#34;一般推理&#34;&gt;一般推理&lt;/h2&gt;&#xA;&lt;p&gt;研究发现，只有用强化学习（RL）训练的模型才能将数学推理技能广泛迁移到其他任务上。而用监督微调（SFT）训练的模型则表现出有限的迁移甚至没有迁移。&lt;a href=&#34;https://mp.weixin.qq.com/s/L1vwB7Lj_JcvSfD7cQ5eSQ&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/L1vwB7Lj_JcvSfD7cQ5eSQ&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;数学题干带猫AI就不会了！错误率翻300%，DeepSeek、o1都不能幸免&lt;a href=&#34;https://mp.weixin.qq.com/s/qesEHt47UQNdjnryMLHwGA&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/qesEHt47UQNdjnryMLHwGA&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;物理学家靠生物揭开AI创造力来源：起因竟是“技术缺陷”&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/Lmh2oX-h4xOOyKPeNjxJGg&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/Lmh2oX-h4xOOyKPeNjxJGg&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;2025-06-30 19:09:50 Monday ｜&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://papers.cool/arxiv/2506.21609&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;从思考到输出：推理语言模型中的思维链和文本生成特征&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;[PDF(2)]&lt;/strong&gt; &lt;strong&gt;[Copy]&lt;/strong&gt; &lt;strong&gt;[Kimi(3)]&lt;/strong&gt; &lt;strong&gt;[REL]&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt; : &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Junhao%20Liu&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Junhao Liu&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Zhenhao%20Xu&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Zhenhao Xu&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Yuxin%20Fang&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Yuxin Fang&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Yichuan%20Chen&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Yichuan Chen&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Zuobin%20Ying&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Zuobin Ying&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Wenhan%20Chang&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Wenhan Chang&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;最近，大型语言模型 （LLM） 取得了显着进展，表明它们在复杂推理方面的能力不断增强。然而，现有的研究在很大程度上忽视了 &lt;strong&gt;对这些模型的推理过程和输出的彻底和系统的比较&lt;/strong&gt; ，特别是关于它们的自我反思模式（也称为“顿悟时刻”）和不同领域的相互联系。本文提出了一种新的框架，用于使用关键词统计和 LLM 作为判断范式分析四种尖端大型推理模型（GPT-o1、DeepSeek-R1、Kimi-k1.5 和 Grok-3）的推理特征。我们的方法将他们的内部思考过程与最终产出联系起来。多样化的数据集由基于真实场景的问题组成，涵盖逻辑推论、因果推理和多步骤问题解决。此外，还提出了一组指标来评估推理的连贯性和输出的准确性。研究结果揭示了这些模型如何在推理过程中平衡探索和开发、处理问题和得出结论的各种模式。通过定量和定性比较，确定了这些模型在推理深度、对中间步骤的依赖以及它们的思维过程和输出模式与 GPT-o1 的相似程度等方面的差异。这项工作为计算效率和推理鲁棒性之间的权衡提供了有价值的见解，并为在实际应用中加强模型设计和评估提供了实用建议。我们在以下位置公开发布我们的项目： &lt;a href=&#34;https://github.com/ChangWenhan/FromThinking2Output&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/ChangWenhan/FromThinking2Output&lt;/a&gt;&lt;/p&gt;&#xA;&lt;ol start=&#34;5&#34;&gt;&#xA;&lt;li&gt;20250604｜新泛化跨领域推理框架：General-Reasoner &lt;a href=&#34;https://mp.weixin.qq.com/s/GDe5Dm17ekCCbUwKO475iA&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/GDe5Dm17ekCCbUwKO475iA&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;2025-06-09 10:47:26 Monday ｜Unleashing the Reasoning Potential of Pre-trained LLMs by Critique  Fine-Tuning on One Problem&#xA;&lt;strong&gt;标题&lt;/strong&gt; ： 通过对一个问题的批评微调来释放预训练LLM的推理潜力&#xA;&lt;strong&gt;链接&lt;/strong&gt; ：https://arxiv.org/abs/2506.03295&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;作者&lt;/strong&gt; ： Yubo Wang,  Ping Nie,  Kai Zou,  Lijun Wu,  Wenhu Chen&#xA;&lt;strong&gt;摘要&lt;/strong&gt; ：我们已经见证了强大的LLM，如Qwen-Math，MiMo和Phi-4拥有从预训练阶段继承的巨大推理潜力。通过强化学习（RL），这些模型可以显着改善推理任务。最近的研究表明，即使是针对单个问题的强化学习也可以释放这些模型的推理能力。然而，RL不仅昂贵而且不稳定。即使是一次性的RL也需要数百个GPU小时。这就提出了一个关键问题：是否有更有效的方法来释放这些强大的基础LLM的推理潜力？在这项工作中，我们证明， &lt;strong&gt;批判微调（CFT）只有一个问题，可以有效地释放LLM的推理潜力&lt;/strong&gt; 。我们的方法通过收集不同的模型生成的解决方案，以一个单一的问题，并使用教师LLM提供详细的批评，构建批判数据。我们微调Qwen和Llama家族模型，从1.5B到14 B参数，在CFT数据上，并观察到在不同推理任务中的显着性能增益。例如，仅用5个GPU小时的训练，Qwen-Math-7 B-CFT在六个数学基准测试中平均提高了15%， &lt;strong&gt;在三个逻辑推理基准测试中平均提高了16%&lt;/strong&gt; 。这些结果与RL的结果相当，甚至超过RL的结果，但计算量减少了20倍。消融研究揭示了单次CFT在不同提示问题中的稳健性。这些结果突出了一次性CFT作为一种简单，通用和计算效率高的方法来释放现代LLM的推理能力。&lt;/p&gt;</description>
    </item>
    <item>
      <title>数据</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E6%95%B0%E6%8D%AE/</link>
      <pubDate>Tue, 12 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E6%95%B0%E6%8D%AE/</guid>
      <description>&lt;h1 id=&#34;数据&#34;&gt;数据&lt;/h1&gt;&#xA;&lt;h2 id=&#34;推理&#34;&gt;推理&lt;/h2&gt;&#xA;&lt;h3 id=&#34;科学推理&#34;&gt;科学推理&lt;/h3&gt;&#xA;&lt;h4 id=&#34;史上最大高质量科学推理后训练数据集开源快速让qwen3等变科学家&#34;&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/0Dp8nriVNr4fOEkuMH4JLQ&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;史上最大高质量科学推理后训练数据集开源，快速让Qwen3等变“科学家”&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;最大的&lt;strong&gt;开源&lt;/strong&gt;科学推理后训练数据集&lt;/li&gt;&#xA;&lt;li&gt;上海创智学院、上海交通大学（GAIR Lab）发布 &lt;strong&gt;MegaScience&lt;/strong&gt; 。该数据集包含约 &lt;strong&gt;125万条问答对及其参考答案&lt;/strong&gt; ，广泛覆盖&lt;strong&gt;生物学、化学、计算机科学、经济学、数学、医学、物理学&lt;/strong&gt;等多个学科领域，旨在为通用人工智能系统的科学推理能力训练与评估提供坚实的数据。&lt;/li&gt;&#xA;&lt;li&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;视频生成&#34;&gt;视频生成&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;2025-07-02 14:22:34 Wednesday｜ 1080p飞升4k，浙大开源原生超高清视频生成方案，突破AI视频生成清晰度上限 &lt;a href=&#34;https://mp.weixin.qq.com/s/wVLTTmbvTToW70Qqw7p80g&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/wVLTTmbvTToW70Qqw7p80g&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;浙江大学APRIL实验室联合多家高校推出的 &lt;strong&gt;高质量开源UHD-4K（其中22.4%为8K）文本到视频数据集——UltraVideo&lt;/strong&gt; ，破解了这一困局。&lt;/p&gt;&#xA;&lt;p&gt;该数据集涵盖广泛主题（超过100种），每个视频配备9个结构化字幕及一个总结性字幕（平均824词）。&lt;/p&gt;</description>
    </item>
    <item>
      <title>新模型</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E6%96%B0%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Tue, 12 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E6%96%B0%E6%A8%A1%E5%9E%8B/</guid>
      <description>&lt;h1 id=&#34;新模型&#34;&gt;新模型&lt;/h1&gt;&#xA;&lt;h2 id=&#34;2025-08011&#34;&gt;2025-08011&lt;/h2&gt;&#xA;&lt;h4 id=&#34;智谱glm-45v&#34;&gt;智谱GLM-4.5V&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;相关素材&#xA;&lt;ol&gt;&#xA;&lt;li&gt;体验地址：https://chat.z.ai/&lt;/li&gt;&#xA;&lt;li&gt;HuggingFace 开源地址：https://huggingface.co/zai-org/GLM-4.5V&lt;/li&gt;&#xA;&lt;li&gt;GitHub 开源地址：https://github.com/zai-org/GLM-V&lt;/li&gt;&#xA;&lt;li&gt;桌面助手下载地址：https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App&lt;/li&gt;&#xA;&lt;li&gt;魔搭社区：https://modelscope.cn/collections/GLM-45V-8b471c8f97154e&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;对图像的识别与推理、视频理解：GLM-4.5V 在涵盖图像理解、视频理解、GUI、文档理解等任务的 41 个公开视觉多模态榜单中综合效果达到了开源 SOTA 水平，这和我们在实测中体验到的结果是一致的。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;2025-08-08&#34;&gt;2025-08-08&lt;/h2&gt;&#xA;&lt;h4 id=&#34;gpt-5博士生水平&#34;&gt;GPT-5：博士生水平&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;官方介绍：https://openai.com/index/introducing-gpt-5/&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;gpt-5&lt;/strong&gt; ：专注逻辑推理和多步骤任务&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;gpt-5-mini&lt;/strong&gt; ：轻量级版本，成本敏感型应用&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;gpt-5-nano&lt;/strong&gt; ：速度优化版，超低延迟&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;gpt-5-chat&lt;/strong&gt; ：企业级多模态对话，支持上下文感知&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;GPT-5 是一个一体化系统，包含三个核心部分：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;一个智能高效的基础模型，可解答大多数问题&lt;/li&gt;&#xA;&lt;li&gt;一个深度推理模型（即GPT-5思维模块），用于处理更复杂的难题&lt;/li&gt;&#xA;&lt;li&gt;以及一个实时路由模块，能够基于对话类型、问题复杂度、工具需求及用户显式指令（如prompt含“仔细思考这个问题”）智能调度模型&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/bKo5zwqCxdDXTch187tc2g&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;快来看看GPT-5第一波实测&lt;/a&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;ARC-AGI的成绩单表示GPT-5不如Grok 4&lt;/li&gt;&#xA;&lt;li&gt;SimpleBench上，GPT-5的水平已经超过了人类平均水平，在大模型中尚属首次。这是一个简单常识推理类的数据集，主要特点就是对于人类非常简单，但对大模型比较困难。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/ktVhcQ2gjbUMh5zX260ynA&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;GPT-5来了！人人都能免费用，最强大模型只需最傻瓜式使用&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/gVvvkiIFFT8GWZcVwhWS9Q&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;GPT-5编程成绩有猫腻！自删23道测试题，关键基准还是自己提的&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;2025-08-07&#34;&gt;2025-08-07&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/No7YJsxrIWaVbFZXGd0pbQ&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;端侧｜Qwen紧追OpenAI开源4B端侧大模型，AIME25得分超越Claude 4 Opus&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Qwen3-4B-Instruct-2507：非推理模型，大幅提升通用能力&lt;/li&gt;&#xA;&lt;li&gt;Qwen3-4B-Thinking-2507：高级推理模型，专为专家级任务设计，逻辑、数学、科学及代码中的高级推理能力——专为专家级任务设计。&lt;/li&gt;&#xA;&lt;li&gt;更智能、更精准，并且支持256k上下文，更具上下文感知能力。&lt;/li&gt;&#xA;&lt;li&gt;抱抱脸直通车：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;[1]https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507&lt;/li&gt;&#xA;&lt;li&gt;[2]https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;魔搭社区直通车：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ttps://modelscope.cn/models/Qwen/Qwen3-4B-Instruct-2507&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen3-4B-Thinking-2507&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://modelscope.cn/models/Qwen/Qwen3-4B-Thinking-2507&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;智能体｜Reflection AI已经发布了他们的首款AI智能体Asimov，较Claude Code Sonnet 4等模型，得到了用户更多偏好。&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Asimov是一款专为代码理解打造的，它能对代码仓库、架构文档、GitHub讨论串、对话历史等多种信息进行索引，从而形成对代码库结构、历史及团队知识的全面认知。&lt;/li&gt;&#xA;&lt;li&gt;Asimov &lt;strong&gt;并非单一智能体&lt;/strong&gt; ，而是 &lt;strong&gt;由几个小型智能体协同工作&lt;/strong&gt; 。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;2025-08-06&#34;&gt;2025-08-06&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Claude Opus 4.1&lt;/strong&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/objwQLTeGWyrYnuy93aZFw&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Claude Opus 4.1火速发布！坐稳编程之王，官方：马上还有大更新&lt;/a&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;编程性能&lt;/strong&gt;再次突破天花板，超越Claude Opus 4，拿下SOTA。&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;在SWE-bench上，Opus 4.1超越Opus 4、Gemini 2.5 Pro、o3，将性能提升至74.5%，拿下新SOTA。&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Blog：&lt;/strong&gt;&lt;a href=&#34;https://www.anthropic.com/news/claude-opus-4-1&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://www.anthropic.com/news/claude-opus-4-1&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;System Card：&lt;/strong&gt;&lt;a href=&#34;https://assets.anthropic.com/m/4c024b86c698d3d4/original/Claude-4-1-System-Card.pdf&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://assets.anthropic.com/m/4c024b86c698d3d4/original/Claude-4-1-System-Card.pdf&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;谷歌DeepMind发布了****新一代通用世界模型Genie 3&lt;/strong&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/ulhJGiiq301f1yuPRTl36g&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;谷歌“世界模拟器”深夜上线！一句话生成3D世界，支持分钟级超长记忆&lt;/a&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Genie 3相比上一代大幅升级，支持****720P画质，每秒24帧实时导航，以及分钟级的一致性保持&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;最让谷歌引以为傲的，还要属Genie 3的****长期环境一致性&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;DeepMind十多年来一直在关注模拟环境领域的研究，从训练智能体掌握实时战略游戏， 到开发用于开放式学习和机器人技术的模拟环境。&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&amp;amp;mid=2652617195&amp;amp;idx=1&amp;amp;sn=91f7f14b4c811e2a1cd9bb5cf5652a11&amp;amp;scene=21#wechat_redirect&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;谷歌推出「G」字号第三代世界模型Genie 3，号称「宇宙模拟器」，视频生成更加符合物理定律。&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;OpenAI开源两个推理模型：gpt-oss-120b&lt;/strong&gt;和&lt;strong&gt;gpt-oss-20b&lt;/strong&gt;。&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/bIaUXw9XWR2Sb4dy4i37_Q&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;刚刚，OpenAI开源2个推理模型：笔记本/手机就能跑，性能接近o4-mini&lt;/a&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;gpt-oss，即Open Source Series，意思就是“开源系列”。&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;gpt-oss-120b&lt;/strong&gt;：1170亿参数（MoE架构，激活参数约51亿），可在单张80GB GPU上运行，性能接近闭源的o4-mini。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt;：210亿参数（Moe架构，激活参数约36亿），可在16GB内存的消费级设备上运行，性能接近o3-mini。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;整体来看，这两个模型在工具使用、少样本函数调用、链式思考推理（如Tau-Bench智能评估套件的结果所示）以及HealthBench上表现强劲，甚至超越了包括OpenAI o1和GPT‑4o在内的专有模型。&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;gpt-oss-120b每个token激活5.1B个参数，而gpt-oss-20b激活3.6B个参数。这些模型分别具有117b和21b的总参数。&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;技术博客地址：&lt;/strong&gt;&lt;a href=&#34;https://openai.com/index/introducing-gpt-oss/&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://openai.com/index/introducing-gpt-oss/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;HuggingFace地址：&lt;/strong&gt;&lt;a href=&#34;https://huggingface.co/openai/gpt-oss-120b&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/openai/gpt-oss-120b&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;GtiHub地址：&lt;/strong&gt;&lt;a href=&#34;https://github.com/openai/gpt-oss&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/openai/gpt-oss&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;OpenAI-OSS-120B用起来要谨慎，写代码特别不稳定。OpenAI-OSS-20B在这个参数量大小下反而挺好。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/U2TsYntvP9Hdlg6e9oUpoQ&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;全网开测GPT-oss！技术架构也扒明白了&lt;/a&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;GPT-oss在架构设计上既保留了MoE Transformer的核心架构，又通过细节优化提升性能、降低复杂度，使其成为适合开源模型的基础架构。&lt;/p&gt;</description>
    </item>
    <item>
      <title>逻辑推理</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E9%80%BB%E8%BE%91%E6%8E%A8%E7%90%86/</link>
      <pubDate>Tue, 12 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E9%80%BB%E8%BE%91%E6%8E%A8%E7%90%86/</guid>
      <description>&lt;h1 id=&#34;逻辑推理&#34;&gt;逻辑推理&lt;/h1&gt;&#xA;&lt;p&gt;未来的评估体系将具有高度可扩展的发展路径。&lt;/p&gt;&#xA;&lt;p&gt;挑战：如何优化统一框架设计、提高训练效率和应对大规模数据等挑战。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;老数据也可以有新用途。&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;为此，我正在打造一个可扩展的&lt;strong&gt;通用数据引擎。&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;强调自主决策&lt;/p&gt;&#xA;&lt;p&gt;关注正确率到关注效率、安全与社会价值。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;计算机科学中的逻辑：https://arxiv.org/list/cs.LO/recent&lt;/p&gt;&#xA;&lt;p&gt;计算机科学与博弈论：https://arxiv.org/list/cs.GT/recent&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;250614｜苹果《思考的错觉》再挨批，Claude与人类共著论文指出其三大关键缺陷&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;2025-06-16 12:33:50 Monday ｜更强大的语言模型会产生更多类似人类的错误&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;20250604｜Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in LLMs Across Logical Transformations and Question Answering Tasks&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;标题： 探索真理的几何学：跨逻辑转换和问题解答任务的LLM中真值方向的一致性和概括性&lt;/li&gt;&#xA;&lt;li&gt;链接：&lt;a href=&#34;https://arxiv.org/abs/2506.00823&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://arxiv.org/abs/2506.00823&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;2025-06-10 10:55:11 Tuesday ｜ PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in  Puzzlehunts&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;标题&lt;/strong&gt; ： PuzzleWorld：益智游戏中多模式、开放式推理的基准&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;链接&lt;/strong&gt; ：https://arxiv.org/abs/2506.06211&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;摘要&lt;/strong&gt; ：Puzzlehunts是一种复杂的，多步骤的谜题，缺乏明确的问题定义。与由具有明确指令的任务组成的传统推理基准相比，puzzlehunts需要模型从多模态证据和迭代推理中发现潜在的问题结构，反映现实世界的领域，如科学发现，探索性数据分析或调查性问题解决。尽管基金会模型最近取得了进展，但它们在这种开放式环境中的表现在很大程度上仍未得到检验。在本文中，我们介绍了PuzzleWorld，一个大规模的基准&lt;strong&gt;667拼图狩猎式&lt;/strong&gt;的问题，旨在评估一步一步的，开放式的，创造性的多模态推理。每个谜题都标注了最终解决方案、详细的推理轨迹和认知技能标签，从而实现整体基准测试和细粒度诊断分析。大多数最先进的模型只能达到1-2%的最终答案准确率，最好的模型只能解决14%的难题，逐步准确率达到40%。为了证明我们的推理注释的价值，我们表明，对推理轨迹进行微调可以将逐步推理从4%提高到11%，而仅对最终答案进行训练则会将性能降低到接近零。我们的错误分析表明，目前的模型表现出近视推理，基于语言的推理的局限性，缺乏草图的视觉和空间推理的关键能力。我们在https://github.com/MIT-MI/PuzzleWorld上发布PuzzleWorld，以支持未来构建更通用，开放和创造性推理系统的工作。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;2025-06-11 11:19:47 Wednesday ｜Evaluating Large Language Models on the Frame and Symbol Grounding  Problems: A Zero-shot Benchmark&#xA;&lt;strong&gt;标题&lt;/strong&gt; ： 基于框架和符号基础问题的大型语言模型评估：Zero-Shot基准&#xA;&lt;strong&gt;链接&lt;/strong&gt; ：https://arxiv.org/abs/2506.07896&lt;/p&gt;</description>
    </item>
    <item>
      <title>SWE 软件工程</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/</link>
      <pubDate>Fri, 08 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/</guid>
      <description>&lt;h1 id=&#34;swe-软件工程&#34;&gt;SWE 软件工程&lt;/h1&gt;&#xA;&lt;h1 id=&#34;总览&#34;&gt;总览&lt;/h1&gt;&#xA;&lt;p&gt;2025-06-23 11:46:23 Monday ｜ Amazon Q Developer &lt;a href=&#34;https://mp.weixin.qq.com/s/8JfR11MUxZneJBDnLDCX_g&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/8JfR11MUxZneJBDnLDCX_g&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;250609｜SWE-Flow：以测试驱动方式合成软件工程数据&lt;/p&gt;&#xA;&lt;p&gt;250607｜SWE-Dev：构建具备训练与推理扩展能力的软件工程智能体&lt;/p&gt;&#xA;&lt;p&gt;250516｜SWE-Dev：评估与训练自主功能驱动的软件开发&lt;/p&gt;&#xA;&lt;h1 id=&#34;概念&#34;&gt;概念&lt;/h1&gt;&#xA;&lt;h2 id=&#34;软件开发方法论&#34;&gt;软件开发方法论&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;by豆包&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;是指导软件开发过程的哲学和实践框架，它们各有侧重，适用于不同的项目需求和团队文化。&lt;/p&gt;&#xA;&lt;p&gt;以下是对主流方法论的系统整理，包括核心特点、适用场景和典型工具：&lt;/p&gt;&#xA;&lt;h3 id=&#34;一敏捷开发agile-development&#34;&gt;&lt;strong&gt;一、敏捷开发（Agile Development）&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt; ：快速迭代、客户反馈、团队协作，应对需求变化。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;典型方法&lt;/strong&gt; ：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Scrum&lt;/strong&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;特点&lt;/strong&gt; ：通过短周期迭代（Sprint）、每日站会、产品待办列表（Backlog）管理项目。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;角色&lt;/strong&gt; ：产品负责人（Product Owner）、Scrum Master、开发团队。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;工具&lt;/strong&gt; ：Jira、Trello、Azure DevOps。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Kanban&lt;/strong&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;特点&lt;/strong&gt; ：可视化工作流程（看板），限制在制品（WIP），持续交付。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;适用场景&lt;/strong&gt; ：需求稳定、流程明确的项目。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;工具&lt;/strong&gt; ：Notion、Monday.com、GitHub Projects。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;极限编程（XP, Extreme Programming）&lt;/strong&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;特点&lt;/strong&gt; ：结对编程、测试驱动开发（TDD）、持续集成、现场客户。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;实践&lt;/strong&gt; ：小型发布、简单设计、重构、集体代码所有权。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;二测试驱动开发tdd-test-driven-development&#34;&gt;&lt;strong&gt;二、测试驱动开发（TDD, Test-Driven Development）&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt; ：先写测试，再实现代码，最后优化（红-绿-重构）。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;流程&lt;/strong&gt; ：&lt;/p&gt;&#xA;&lt;ol start=&#34;4&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;编写测试用例&lt;/strong&gt; （失败状态）。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;实现功能代码&lt;/strong&gt; （使测试通过）。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;重构代码&lt;/strong&gt; （优化结构，保持测试通过）。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;工具&lt;/strong&gt; ：JUnit（Java）、PyTest（Python）、Jest（JavaScript）。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;适用场景&lt;/strong&gt; ：对质量要求高、需要频繁重构的项目。&lt;/p&gt;&#xA;&lt;h3 id=&#34;三行为驱动开发bdd-behavior-driven-development&#34;&gt;&lt;strong&gt;三、行为驱动开发（BDD, Behavior-Driven Development）&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt; ：以用户行为为中心，通过自然语言描述需求（Gherkin 语法）。&lt;/p&gt;</description>
    </item>
    <item>
      <title>具身智能</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E5%85%B7%E8%BA%AB/</link>
      <pubDate>Fri, 08 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E5%85%B7%E8%BA%AB/</guid>
      <description>&lt;h1 id=&#34;具身智能&#34;&gt;具身智能&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Genesis是一个生成式物理引擎，由 CMU 联合 20 多所研究实验室历时两年联合开发，能够生成 4D 动态世界、模拟广泛的材料和物理现象，专为通用机器人、具身 AI 和物理 AI 应用而设计。&lt;a href=&#34;https://mp.weixin.qq.com/s/TsmMqip3r9kWxJdg1HfIrw&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/TsmMqip3r9kWxJdg1HfIrw&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;2025-06-26 13:57:58 Thursday&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;论文链接：https://www.roboticsproceedings.org/rss21/p020.pdf&lt;/li&gt;&#xA;&lt;li&gt;论文主页：https://playground.mujoco.org/&lt;/li&gt;&#xA;&lt;li&gt;机构：UC 伯克利、Google DeepMind、多伦多大学、剑桥大学&lt;/li&gt;&#xA;&lt;li&gt;作者：Kevin Zakka, Baruch Tabanpour, Qiayuan Liao, Mustafa Haiderbhai, Samuel Holt, Jing Yuan Luo, Arthur Allshire, Erik Frey, Koushil Sreenath, Lueder Alexander Kahrs, Carmelo Sferrazza, Yuval Tassa, Pieter Abbeel&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;论文摘要：&lt;/strong&gt; 该研究提出了 MuJoCo Playground—— 这是一个基于 MJX 构建的完全开源机器人学习框架，其核心设计目标是大幅简化仿真环境搭建、模型训练以及仿真到现实场景的迁移全流程。研究人员仅需执行简单的「pip install playground」安装命令，即可在单 GPU 硬件上完成分钟级策略训练。&lt;/p&gt;&#xA;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;2025-06-26 14:05:00 Thursday&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;由香港大学与上海AI Lab联合提出的 &lt;strong&gt;VLN-R1&lt;/strong&gt; ，具备将自然语言指令直接转化为第一人称视角下的连续导航动作的能力，无需依赖离散地图，能在复杂环境中灵活感知、决策与行动，实现类人级别的具身智能导航。&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/XhcnUxYUXi2jvX51u3zpsw&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/XhcnUxYUXi2jvX51u3zpsw&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>博弈 Self-Play</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E5%8D%9A%E5%BC%88/</link>
      <pubDate>Fri, 08 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E5%8D%9A%E5%BC%88/</guid>
      <description>&lt;h1 id=&#34;博弈-self-play&#34;&gt;博弈 Self-Play&lt;/h1&gt;&#xA;&lt;h2 id=&#34;强化学习&#34;&gt;强化学习&lt;/h2&gt;&#xA;&lt;h4 id=&#34;spiral零和游戏自对弈成为语言模型推理训练的免费午餐&#34;&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/jAaM3hD46gFEFGFJdLVVJg&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;SPIRAL：零和游戏自对弈成为语言模型推理训练的「免费午餐」&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;p&gt;2025-08-05&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;研究方向为可扩展的自主提升，致力于构建能在未知环境中智能决策的自主智能体&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;通过基于结果的奖励机制，强化学习使模型能够发展出可泛化的推理策略，在复杂问题上取得了监督微调难以企及的进展。&lt;/li&gt;&#xA;&lt;li&gt;本文通过让模型在零和游戏中与自己对弈，自主发现并强化可泛化的推理模式，完全摆脱了对人工监督的依赖。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;论文标题：&lt;/strong&gt; SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning&lt;/li&gt;&#xA;&lt;li&gt;论文链接：https://huggingface.co/papers/2506.24119&lt;/li&gt;&#xA;&lt;li&gt;代码链接：https://github.com/spiral-rl/spiral&lt;/li&gt;&#xA;&lt;li&gt;研究团队的核心洞察是：如果强化学习能够从预训练语言模型中选择出可泛化的思维链（Chain-of-Thought, CoT）模式，那么游戏为这一过程提供了完美的&lt;strong&gt;试炼场&lt;/strong&gt;：它们通过输赢结果提供廉价、可验证的奖励，无需人工标注。通过在这些游戏上进行自对弈，强化学习能够自动发现哪些 CoT 模式在多样化的竞争场景中获得成功，并逐步强化这些模式，创造了一个自主的推理能力提升系统。&lt;/li&gt;&#xA;&lt;li&gt;实验发现，不同游戏确实培养了专门化的认知能力：&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;井字棋专家在空间推理游戏 Snake 上达到 56% 胜率。&lt;/li&gt;&#xA;&lt;li&gt;库恩扑克大师在概率游戏 Pig Dice 上取得惊人的 91.7% 胜率。&lt;/li&gt;&#xA;&lt;li&gt;简单谈判专家在战略优化游戏上表现出色。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;ol start=&#34;9&#34;&gt;&#xA;&lt;li&gt;更有趣的是，当结合多个游戏训练时，技能产生协同效应。&lt;/li&gt;&#xA;&lt;li&gt;SPIRAL 验证了一个关键假设：预训练模型中已经包含了各种推理模式，强化学习的作用是从这些模式中筛选和强化那些真正可泛化的思维链。&lt;/li&gt;&#xA;&lt;li&gt;未来的研究开辟了新方向：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;混合博弈类型：结合零和、合作和混合动机游戏，可能培养更全面的推理能力。&lt;/li&gt;&#xA;&lt;li&gt;元游戏学习：让模型不仅玩游戏，还能创造新游戏，实现真正的创造性推理。&lt;/li&gt;&#xA;&lt;li&gt;跨模态游戏：将语言游戏扩展到包含视觉、音频等多模态信息，培养更丰富的认知能力。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;无需外部数据ai自问自答实现推理能力进化&#34;&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/Q3fc95LXM3PuytdEBnUCSA&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;无需外部数据！AI自问自答实现推理能力进化&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;p&gt;2025-08-08&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;卡内基梅隆大学团队提出的新框架 &lt;strong&gt;SQLM&lt;/strong&gt; ——一种无需外部数据的自我提问模型。&#xA;&lt;ol&gt;&#xA;&lt;li&gt;该框架包含提问者（proposer）和解答者（solver）两个角色，提问者生成与给定主题相关的问题，解答者旨在解决问题。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;研究者提出了 &lt;strong&gt;SQLM框架&lt;/strong&gt; ，一种非对称的自我博弈框架。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;主动学习&#34;&gt;主动学习&lt;/h2&gt;&#xA;&lt;h4 id=&#34;atgen主动文本生成框架&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://papers.cool/arxiv/2506.23342&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;ATGen：主动文本生成框架&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;p&gt;2025-07-01 12:04:39 Tuesday&lt;/p&gt;</description>
    </item>
    <item>
      <title>工具</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E5%B7%A5%E5%85%B7/</link>
      <pubDate>Fri, 08 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E5%B7%A5%E5%85%B7/</guid>
      <description>&lt;h1 id=&#34;工具&#34;&gt;工具&lt;/h1&gt;&#xA;&lt;h2 id=&#34;效率工具&#34;&gt;效率工具&lt;/h2&gt;&#xA;&lt;h3 id=&#34;ai校验&#34;&gt;AI校验&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;1. Binoculars&lt;/strong&gt;这一MGT（机器生成文本）检测器&lt;/p&gt;&#xA;&lt;h3 id=&#34;学习用资料&#34;&gt;学习用资料&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;**Lean 中文文档翻译  **&lt;strong&gt;&lt;a href=&#34;https://github.com/Lean-zh&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/Lean-zh&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;2025-06-11 17:37:53 Wednesday | 提示词自动优化网站 &lt;a href=&#34;https://mp.weixin.qq.com/s/Y0X3z-ARRcPgEDCZffiNfg&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/Y0X3z-ARRcPgEDCZffiNfg&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;agent工具&#34;&gt;Agent工具&lt;/h3&gt;&#xA;&lt;p&gt;2025-07-18 14:11:00 Friday ｜ 刚刚，OpenAI通用智能体ChatGPT Agent正式登场 &lt;a href=&#34;https://mp.weixin.qq.com/s/4xJGBr1dXdIPHgSrPpwW5w&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/4xJGBr1dXdIPHgSrPpwW5w&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;kimi-dev&lt;/p&gt;&#xA;&lt;p&gt;2025-06-30 16:07:47 Monday ｜ 初创公司 Cluely，推出了名为「Cluely」的 AI 工具，旨在为用户提供面试、考试、销售电话等场景的实时辅助https://mp.weixin.qq.com/s/58taCee98HkAxUHTgIWkUA&lt;/p&gt;&#xA;&lt;p&gt;2025-06-28 18:39:39 Saturday ｜ 不靠Agent，4步修复真Bug！蚂蚁CGM登顶SWE-Bench开源榜 &lt;a href=&#34;https://mp.weixin.qq.com/s/fbKB3Y1F4yZbv_mGDVep2g&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/fbKB3Y1F4yZbv_mGDVep2g&lt;/a&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;蚂蚁集团另辟蹊径，给出一个完全不同的新解法：**代码图模型 CGM（ Code Graph Model ） **&lt;/li&gt;&#xA;&lt;li&gt;论文：https://arxiv.org/abs/2505.16901&lt;/li&gt;&#xA;&lt;li&gt;模型：https://huggingface.co/codefuse-ai/CodeFuse-CGM-72B&lt;/li&gt;&#xA;&lt;li&gt;代码：https://github.com/codefuse-ai/CodeFuse-CGM&lt;/li&gt;&#xA;&lt;li&gt;数据：https://huggingface.co/datasets/codefuse-ai/CodeGraph&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;2025-06-28 16:54:14 Saturday ｜ 阿里发布信息检索Agent-WebDancer，可自主上网查资料，GAIA基准超越GPT-4o | 模型&amp;amp;数据开源 &lt;a href=&#34;https://mp.weixin.qq.com/s/LETDaeU96OV-beuoCuJHHQ&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/LETDaeU96OV-beuoCuJHHQ&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;2025-06-26 14:25:53 Thursday｜ Gemini CLI 开源+免费https://mp.weixin.qq.com/s/370Vt6Ly_2kLuyBUJ7vfIw&lt;/p&gt;</description>
    </item>
    <item>
      <title>强化学习</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Fri, 08 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</guid>
      <description>&lt;h1 id=&#34;强化学习&#34;&gt;强化学习&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;2025-07-18 20:24:54 Friday ｜ 真相大白！RL不是魔法，随机奖励有效只是由于数据泄露！ &lt;a href=&#34;https://mp.weixin.qq.com/s/J7dzjfHMHyvQNpKQFpoq7w&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/J7dzjfHMHyvQNpKQFpoq7w&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;🌈 2025-07-18 14:07:23 Friday｜强化学习的两个「大坑」，终于被两篇ICLR论文给解决了 &lt;a href=&#34;https://mp.weixin.qq.com/s/myqak4eAAQ7ONKJKOFxk8g&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/myqak4eAAQ7ONKJKOFxk8g&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;2025-07-17 10:59:26 Thursday ｜ 字节跳动Seed最新强化学习配方POLARIS开源 4B 模型数学推理接近 235B 表现&lt;/li&gt;&#xA;&lt;li&gt;2025-06-30 19:00:34 Monday ｜ 强化学习也能预训练？效果可提升20倍，华人新作引爆RL新范式! &lt;a href=&#34;https://mp.weixin.qq.com/s/WyJuhjkmreZ2clSw1XvHiw&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/WyJuhjkmreZ2clSw1XvHiw&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;研究人员构建的模型将「意图」编码为潜在变量，并通过「流匹配」（flow matching）来预测未来状态的访问概率。&lt;/p&gt;&#xA;&lt;p&gt;论文地址：https://arxiv.org/abs/2506.08902&lt;/p&gt;&#xA;&lt;p&gt;博客地址：https://chongyi-zheng.github.io/infom/&lt;/p&gt;&#xA;&lt;p&gt;由于普通流匹配方法无法拼接多个状态转换，研究者引入基于SARSA的时序差分流匹配损失进行改进。&lt;/p&gt;&#xA;&lt;ol start=&#34;5&#34;&gt;&#xA;&lt;li&gt;2025-06-24 11:28:01 Tuesday ｜ CPGD：只训练数学，却在物理化学生物战胜o1！新强化学习算法带来显著性能提升，还缓解训练崩溃问题 &lt;a href=&#34;https://mp.weixin.qq.com/s/RDUPagBn8l00P7dNPHjBcA&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/RDUPagBn8l00P7dNPHjBcA&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;相比于传统GRPO、RLOO等算法显著缓解了训练不稳定（甚至崩溃）的问题，并带来显著性能提升。&lt;/p&gt;&#xA;&lt;p&gt;基于OpenRLHF，团队构建了一个高效、可扩展的多模态强化学习框架，支持Qwen-VL、InternVL等多种模型与RL算法，包括GRPO、REINFORCE++、RLOO，以及提出的新型RL算法CPGD，并已成功训练出Qwen2.5VL-32B、InternVL2.5-38B等大型模型。&lt;/p&gt;&#xA;&lt;p&gt;该框架相较于已有方案（如R1-V），具备更强的可扩展性与稳定性，为大规模多模态强化学习提供了基础设施支撑。&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;开源代码：&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/ModalMinds/MM-EUREKA&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/ModalMinds/MM-EUREKA&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/ModalMinds/MM-EUREKA/tree/mm-prm&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/ModalMinds/MM-EUREKA/tree/mm-prm&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;技术报告：&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2503.07365&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://arxiv.org/abs/2503.07365&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2505.12504&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://arxiv.org/abs/2505.12504&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2505.13427&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://arxiv.org/abs/2505.13427&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>数据合成</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E6%95%B0%E6%8D%AE%E5%90%88%E6%88%90/</link>
      <pubDate>Fri, 08 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E6%95%B0%E6%8D%AE%E5%90%88%E6%88%90/</guid>
      <description>&lt;h1 id=&#34;数据合成&#34;&gt;&lt;strong&gt;数据合成&lt;/strong&gt;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;结语：合成任务的新时代&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;ViGaL 的成功揭示了一个潜在的新趋势：当高质量人类数据枯竭，简单任务性能饱和的时候，精心设计的游戏，作为一种合成任务，可能为多模态推理能力的发展开辟新道路。&lt;/p&gt;&#xA;&lt;p&gt;与传统的直接训练方法相比，这种游戏化的训练范式展现出独特的优势：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;成本极低：无需人工标注，可无限扩展&lt;/li&gt;&#xA;&lt;li&gt;效果显著：零数学样本超越数学专训模型&lt;/li&gt;&#xA;&lt;li&gt;拓展性强：可以组合多个任务进一步提升性能&lt;/li&gt;&#xA;&lt;li&gt;通用性好：不会造成 &amp;ldquo;偏科&amp;rdquo; 问题，保持模型的全面能力&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;更重要的是，ViGaL 可能揭示了一个朴素但深刻的道理：在直接学习目标任务之外，培养底层的通用推理能力，也许同样有助于模型性能的提升。就像我们不只是通过死记硬背数学公式来培养数学思维，而是通过各种思维训练来发展抽象推理能力一样。&lt;/p&gt;&#xA;&lt;p&gt;在 Scaling Law 可能逐渐面临困境的今天，ViGaL 用一个简单而优雅的想法提醒我们：有时候，让 AI&amp;quot;玩游戏&amp;quot; 可能比让它 &amp;ldquo;刷题&amp;rdquo; 更有效。&lt;/p&gt;&#xA;&lt;h1 id=&#34;通用框架&#34;&gt;通用框架&lt;/h1&gt;&#xA;&lt;h4 id=&#34;----synthetic-data-rl-task-definition-is-all-you-need&#34;&gt;🌈 🌈 🌈  Synthetic Data RL: Task Definition Is All You Need&lt;/h4&gt;&#xA;&lt;p&gt;2025-06-25 10:31:10 Wednesday ｜ Synthetic Data RL: Task Definition Is All You Need （可以参考一下其中的实验部分）https://mp.weixin.qq.com/s/rjNQdHUCZ4YmvRNVveMQ8w&lt;/p&gt;&#xA;&lt;p&gt;传统上，为了让这些模型适应特定领域，最直接的方法是使用大规模的人类标注数据进行微调。然而，这一过程不仅成本高昂、耗时漫长，而且在许多实际应用场景中并不可行。&lt;/p&gt;&#xA;&lt;p&gt;为了解决上述挑战，北京大学、MIT等机构的研究人员提出了「合成数据强化学习」（Synthetic Data RL）框架。这是一个简单而通用的框架，仅从一个任务定义出发，合成大量多样的领域特定样本，然后利用强化学习（RL）对模型进行微调。&lt;/p&gt;&#xA;&lt;p&gt;论文链接：https://arxiv.org/pdf/2505.17063&lt;/p&gt;&#xA;&lt;p&gt;代码仓库：https://github.com/gydpku/Data_Synthesis_RL&lt;/p&gt;&#xA;&lt;p&gt;这种方式实现了参数化的自适应，将领域知识直接嵌入到模型的参数中，并且完全无需任何人类标注的数据。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=MmRkMGJkYmJkODkwODI2ZmZhMjRlNjMzNjJhZWIyM2NfYUlCNWxUMGRSMkFhWWQxbFp3VVVPbXRTemZNcEFaWjhfVG9rZW46QndPeWIwN21Ub3hnbUR4aThTbGMzMDlrbmtkXzE3NTQ0NjI0MzY6MTc1NDQ2NjAzNl9WNA&#34; srcset=&#34;https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=MmRkMGJkYmJkODkwODI2ZmZhMjRlNjMzNjJhZWIyM2NfYUlCNWxUMGRSMkFhWWQxbFp3VVVPbXRTemZNcEFaWjhfVG9rZW46QndPeWIwN21Ub3hnbUR4aThTbGMzMDlrbmtkXzE3NTQ0NjI0MzY6MTc1NDQ2NjAzNl9WNA&amp;amp;size=small, https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=MmRkMGJkYmJkODkwODI2ZmZhMjRlNjMzNjJhZWIyM2NfYUlCNWxUMGRSMkFhWWQxbFp3VVVPbXRTemZNcEFaWjhfVG9rZW46QndPeWIwN21Ub3hnbUR4aThTbGMzMDlrbmtkXzE3NTQ0NjI0MzY6MTc1NDQ2NjAzNl9WNA&amp;amp;size=medium 1.5x, https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=MmRkMGJkYmJkODkwODI2ZmZhMjRlNjMzNjJhZWIyM2NfYUlCNWxUMGRSMkFhWWQxbFp3VVVPbXRTemZNcEFaWjhfVG9rZW46QndPeWIwN21Ub3hnbUR4aThTbGMzMDlrbmtkXzE3NTQ0NjI0MzY6MTc1NDQ2NjAzNl9WNA&amp;amp;size=large 2x&#34; sizes=&#34;auto&#34; data-title=&#34;https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=MmRkMGJkYmJkODkwODI2ZmZhMjRlNjMzNjJhZWIyM2NfYUlCNWxUMGRSMkFhWWQxbFp3VVVPbXRTemZNcEFaWjhfVG9rZW46QndPeWIwN21Ub3hnbUR4aThTbGMzMDlrbmtkXzE3NTQ0NjI0MzY6MTc1NDQ2NjAzNl9WNA&#34; data-alt=&#34;https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=MmRkMGJkYmJkODkwODI2ZmZhMjRlNjMzNjJhZWIyM2NfYUlCNWxUMGRSMkFhWWQxbFp3VVVPbXRTemZNcEFaWjhfVG9rZW46QndPeWIwN21Ub3hnbUR4aThTbGMzMDlrbmtkXzE3NTQ0NjI0MzY6MTc1NDQ2NjAzNl9WNA&#34; class=&#34;suffix-invalid suffix-invalid__small suffix-invalid__large&#34; style=&#34;background: url(/LLMDailyDigest.github.io/svg/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;data-alt&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;data-alt&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>新趋势</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E6%96%B0%E8%B6%8B%E5%8A%BF/</link>
      <pubDate>Fri, 08 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E6%96%B0%E8%B6%8B%E5%8A%BF/</guid>
      <description>&lt;h1 id=&#34;新趋势&#34;&gt;新趋势&lt;/h1&gt;&#xA;&lt;h2 id=&#34;通用验证器&#34;&gt;通用验证器&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;OpenAI正在通过一种名为「Universal Verifier」的技术，让GPT-5在全领域实现稳步提升。翻译过来就是：通用验证器。这项技术的核心思想是：让一个AI 模型充当「验证者」，检查另一个模型的输出质量。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;去年下半年，代号为Orion的模型本应成为GPT-5，但其性能提升远低于预期，最终只能以GPT-4.5的身份发布。原因在于三大技术瓶颈同时出现：&lt;strong&gt;高质量训练数据枯竭、强化学习过程不稳定、模型扩展时的性能退化。&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;这个系统的工作原理类似于生成对抗网络（GAN）：&lt;strong&gt;一个模型负责生成答案，另一个模型负责评判质量。&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;OpenAI此前的论文「&lt;strong&gt;Prover-Verifier Games Improve Legibility of LLM Outputs&lt;/strong&gt;」详细展示了这种方法的威力。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;当时的超级对齐研究团队设计了一个巧妙的游戏：「&lt;strong&gt;证明者&lt;/strong&gt;」模型被赋予两种角色：「helpful」（提供正确答案）和「sneaky」（故意制造错误）。「&lt;strong&gt;验证者&lt;/strong&gt;」模型则需要学会识别哪些答案是正确的。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;compassverifier-llm评估和结果奖励的统一鲁棒验证器24&#34;&gt;&lt;a href=&#34;https://huggingface.co/papers/2508.03686?utm_source=digest-papers&amp;amp;utm_medium=email&amp;amp;utm_campaign=2025-08-06&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;CompassVerifier: llm评估和结果奖励的统一鲁棒验证器（24▲） &lt;/a&gt;&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;CompassVerifier 是一个轻量级、稳健的模型，用于跨多个领域验证 LLM 输出，支持该模型的是 VerifierBench，一个全面的基准数据集。&lt;/li&gt;&#xA;&lt;li&gt;答案验证不仅对于通过将大型语言模型（LLMs）的非结构化输出与标准答案进行匹配来评估其性能至关重要，同时也作为奖励模型指导 LLM 的优化。大多数评估框架依赖正则化匹配或使用通用 LLMs 进行答案验证，这需要对正则表达式规则或评估提示进行大量且重复的定制。&#xA;&lt;ol&gt;&#xA;&lt;li&gt;目前方法存在两个根本性限制：1）缺乏系统评估不同 LLMs 验证能力的综合基准；2）验证器开发尚处于初期阶段，现有方法既缺乏处理复杂边缘案例的鲁棒性，也缺乏跨领域的泛化能力。&lt;/li&gt;&#xA;&lt;li&gt;在本工作中，我们开发了 CompassVerifier，一种准确且鲁棒的轻量级验证模型，用于评估和结果奖励。它展现了涵盖数学、知识及多样推理任务的多领域能力，能够处理多种答案类型，包括多子问题、公式和序列答案，同时有效识别异常/无效响应。&lt;/li&gt;&#xA;&lt;li&gt;我们介绍了 VerifierBench 基准测试，该测试包含从多个数据源收集的模型输出，并通过对元错误模式的人工分析进行增强，以提升 CompassVerifier 的性能。&lt;/li&gt;&#xA;&lt;li&gt;我们期望 CompassVerifier 和 VerifierBench 能够促进答案验证、评估协议和强化学习研究。代码和数据集可在 &lt;a href=&#34;https://github.com/open-compass/CompassVerifier&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/open-compass/CompassVerifier&lt;/a&gt; 获取。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/nzEQ86jx4hEJMUnzj8Mu5A&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;3B模型性能小钢炮，“AI下半场应该训练+验证两条腿跑步”丨上海AI Lab&amp;amp;澳门大学&lt;/a&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;训练AI解决某个任务的难易程度与该任务的可验证性成正比。所有可解决且易于验证的任务，都将被AI解决&lt;/strong&gt; 。&lt;/li&gt;&#xA;&lt;li&gt;VerifierBench：针对验证模型的多领域、高难度基准&lt;/li&gt;&#xA;&lt;li&gt;论文地址：https://arxiv.org/abs/2508.03686&#xA;项目主页：https://open-compass.github.io/CompassVerifier&#xA;Github：https://github.com/open-compass/CompassVerifier&#xA;Model &amp;amp; Dataset：https://huggingface.co/collections/opencompass/compassverifier-686e5a25e8672e603b17c666&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;dllm-扩散语言模型&#34;&gt;dLLM 扩散语言模型&lt;/h2&gt;&#xA;&lt;p&gt;本综述系统梳理了离散扩散方向的研究图谱，呈现了离散扩散语言模型（dLLMs）与离散扩散多模态语言模型（dMLLMs）的理论基础、代表模型、训练与推理技术，以及在推理、视觉、生物等多个领域的应用进展。&lt;a href=&#34;https://mp.weixin.qq.com/s/hcIUmS-jUIVLOIfS7-1gtQ&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/hcIUmS-jUIVLOIfS7-1gtQ&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>智能体</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E6%99%BA%E8%83%BD%E4%BD%93/</link>
      <pubDate>Fri, 08 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E6%99%BA%E8%83%BD%E4%BD%93/</guid>
      <description>&lt;h1 id=&#34;智能体&#34;&gt;智能体&lt;/h1&gt;&#xA;&lt;h2 id=&#34;洞察&#34;&gt;洞察&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;智能体&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;自主决策、执行、洞察、反哺&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;具备任务自主化能力，能够主动拆解目标、智能调度资源，并在交互过程中持续优化策略，展现出强大的动态进化能力。&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;区别推理和规划能力&lt;/li&gt;&#xA;&lt;li&gt;递归自我改进（RSI）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://deepshare.feishu.cn/wiki/Cu5Bw0k4WijJu9keRsIceDOZnpe?from=from_copylink&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;2025年agent前沿研究&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;2025-07-18 14:05:54 Friday ｜ 模仿学习新范式，Chain-of-Action：轨迹自回归实现动作推理 &lt;a href=&#34;https://mp.weixin.qq.com/s/fJXWvpC1s_2FkoUYhnmTCg&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/fJXWvpC1s_2FkoUYhnmTCg&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;2025-07-17 10:59:05 Thursday ｜ 昆仑万维Skywork发布分层多智能体协作框架AgentOrchestra&lt;/p&gt;&#xA;&lt;p&gt;20250604｜开启 AI 自主进化时代，普林斯顿Alita颠覆传统通用智能体，GAIA榜单迎来终章&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/vmp8H-3S_HH6Gvb4dH5FxA&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/vmp8H-3S_HH6Gvb4dH5FxA&lt;/a&gt;&#xA;论文标题：ALITA: GENERALIST AGENT ENABLING SCALABLE AGENTIC REASONING WITH MINIMAL PREDEFINITION AND MAXIMAL SELF-EVOLUTION&lt;/p&gt;&#xA;&lt;p&gt;论文链接：https://arxiv.org/abs/2505.20286&lt;/p&gt;&#xA;&lt;p&gt;Twitter：https://x.com/JiahaoQiu99/status/1927376487285432790&lt;/p&gt;&#xA;&lt;p&gt;GitHub：https://github.com/CharlesQ9/Alita&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;核心：普林斯顿大学 AI Lab 推出了 Alita——一个秉持「&lt;strong&gt;极简即是极致复杂&lt;/strong&gt;」哲学的通用智能体，通过「&lt;strong&gt;最小化预定义&lt;/strong&gt;」与「&lt;strong&gt;最大化自我进化&lt;/strong&gt;」的设计范式，让智能体可以自主思考、搜索和创造其所需要的 MCP 工具。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;现有的主流智能体系统通常依赖大量人工预定义的工具和复杂的工作流，这种方法有三个关键缺陷：覆盖范围有限、创造力受限、适配失配，这些挑战共同限制了现有通用智能体的创造力、可扩展性和泛化能力。&lt;/li&gt;&#xA;&lt;li&gt;与当前日益复杂的趋势相反，Alita 团队认为对于通用智能体而言，「simplicity is the ultimate sophistication」——简单即极致的复杂。遵循这一原则，Alita 实现了可扩展的动态能力、增强的创造力与灵活性，以及跨生态系统的兼容性。Alita 团队由此提出了两大设计范式：&lt;/li&gt;&#xA;&lt;li&gt;**最小化预定义：**仅为智能体配备最核心的基础能力，避免为特定任务或模态设计人工预定义的组件。&lt;/li&gt;&#xA;&lt;li&gt;**最大化自进化：**赋予智能体按需自主创建、优化和复用 MCP 工具的能力，实现自我进化。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;2025-06-09 10:50:14 Monday ｜TextAtari: 100K Frames Game Playing with Language Agents&#xA;&lt;strong&gt;标题&lt;/strong&gt; ： 文本Atari：使用语言代理玩10万帧游戏&#xA;&lt;strong&gt;链接&lt;/strong&gt; ：https://arxiv.org/abs/2506.04098&#xA;&lt;strong&gt;摘要&lt;/strong&gt; ：我们提出了TextAtari，这是一个 &lt;strong&gt;用于评估语言代理在长达10万步的长期决策任务上的基准&lt;/strong&gt; 。通过将经典Atari游戏的视觉状态表示转换为丰富的文本描述，TextAtari创建了一个具有挑战性的测试平台，将顺序决策与自然语言处理联系起来。该基准测试包括近100个不同的任务，具有不同的复杂性，动作空间和规划视野，所有这些任务都通过无监督表示学习框架（AtariARI）呈现为文本。我们评估了三个开源的大型语言模型（Qwen2.5- 7 B，Gemma-7 B和Llama3.1-8B）在三个代理框架（zero-shot，Few-Shot chain-of-thought和reflection reasoning），以评估不同形式的先验知识如何影响这些长期挑战的性能。四个基本的，模糊的，手动增强，并参考为基础的语义理解，指令理解和专家示范代理决策的影响。我们的研究结果揭示了语言智能体和人类玩家在广泛的规划任务中的显着性能差距，突出了顺序推理，状态跟踪和数万个步骤的战略规划方面的挑战。TextAtari提供了标准化的评估协议、基线实现和框架，用于推进语言模型和规划交叉点的研究。&lt;/p&gt;</description>
    </item>
    <item>
      <title>记忆</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E8%AE%B0%E5%BF%86/</link>
      <pubDate>Fri, 08 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E8%AE%B0%E5%BF%86/</guid>
      <description>&lt;h1 id=&#34;记忆&#34;&gt;记忆&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;2025-07-02 17:16:24 Wednesday ｜&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://papers.cool/arxiv/2507.00258&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;微调方法对大型语言模型中记忆的影响&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;[PDF()]&lt;/strong&gt; &lt;strong&gt;[Copy]&lt;/strong&gt; &lt;strong&gt;[Kimi()]&lt;/strong&gt; &lt;strong&gt;[REL]&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt; : &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Jie%20Hou&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Jie Hou&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Chuxiong%20Wu&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Chuxiong Wu&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Lannan%20Luo&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Lannan Luo&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Qiang%20Zeng&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Qiang Zeng&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;随着预训练大型语言模型 （LLM） 的功能不断进步，“预训练和微调”范式越来越成为主流，导致了各种微调方法的发展。然而，在微调过程中记忆所带来的隐私风险受到的关注相对较少。为了解决这一差距，我们对流行的微调方法进行了分类，并通过成员推理攻击 （MIA） 的视角评估了它们对记忆的影响。我们的结果表明，与基于参数的微调相比，基于提示的微调实现了有竞争力的性能，同时表现出较低的 MIA 脆弱性。此外，无论模型规模如何，基于提示的方法都保持低记忆。这些发现表明，基于参数的微调更容易泄露私人信息，而基于提示的微调是一种更能保护隐私的选项。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;科目&lt;/strong&gt; :  &lt;strong&gt;&lt;a href=&#34;https://papers.cool/arxiv/cs.CL&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;计算和语言&lt;/a&gt;&lt;/strong&gt; , &lt;a href=&#34;https://papers.cool/arxiv/cs.AI&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;人工智能&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;发布&lt;/strong&gt; ： 2025-06-30 20：52：15 UTC&lt;/p&gt;&#xA;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;2025-06-30 19:11:06 Monday ｜&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;M&lt;/strong&gt;**&lt;a href=&#34;https://papers.cool/arxiv/2506.21605&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;emBench：对基于 LLM 的代理的内存进行更全面的评估&lt;/a&gt;** &lt;strong&gt;[PDF(1)]&lt;/strong&gt; &lt;strong&gt;[Copy]&lt;/strong&gt; &lt;strong&gt;[Kimi()]&lt;/strong&gt; &lt;strong&gt;[REL]&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt; : &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Haoran%20Tan&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Haoran Tan&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Zeyu%20Zhang&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Zeyu Zhang&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Chen%20Ma&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Chen Ma&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Xu%20Chen&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Xu Chen&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Quanyu%20Dai&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Quanyu Dai&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Zhenhua%20Dong&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Zhenhua Dong&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>元</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E5%85%83/</link>
      <pubDate>Thu, 07 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E5%85%83/</guid>
      <description>&lt;h1 id=&#34;元&#34;&gt;元&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;2025-06-12 12:05:13 Thursday｜ Large Language Models Have Intrinsic Meta-Cognition, but Need a Good  Lens&#xA;&lt;strong&gt;标题&lt;/strong&gt; ： 大型语言模型具有内在的元认知，但需要良好的镜头&#xA;&lt;strong&gt;链接&lt;/strong&gt; ：https://arxiv.org/abs/2506.08410&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;作者&lt;/strong&gt; ： Ziyang Ma,  Qingyue Yuan,  Zhenglin Wang,  Deyu Zhou&#xA;&lt;strong&gt;备注&lt;/strong&gt; ：Preprint&#xA;&lt;strong&gt;摘要&lt;/strong&gt; ：以前的研究主要集中在大型语言模型（LLM）的认知错误检测能力上，通常会促使它们分析推理链中的错误。然而，很少有研究考察LLM的&lt;strong&gt;元认知&lt;/strong&gt;能力（例如，他们的自我意识的步骤错误），这是至关重要的，他们的可靠性。尽管对LLM自我评价的研究提出了&lt;strong&gt;困惑度&lt;/strong&gt;等能够反映答案正确性的测量指标，并将其作为元认知的透镜，但缺乏对LLM自我评价的步骤分析和适应性研究。本文研究了如何用现有的评价方法来评价法学硕士元认知，以及如何改进这些评价方法。具体来说，我们提出了AutoMeco，一个 &lt;strong&gt;自动化的元认知评估框架&lt;/strong&gt; ，用于对现有的镜头进行基准测试。此外，提出了一种无需训练的马尔可夫内在奖励调整策略MIRA，以提高当前的元认知镜头。在三个数学推理数据集和三个LLM上的实验结果表明，AutoMeco验证方法与Best-of-N验证方法相比具有一定的合理性。此外，MIRA可以更好地评估LLM的元认知能力。&lt;/p&gt;&#xA;&lt;h2 id=&#34;认知能力&#34;&gt;认知能力&lt;/h2&gt;&#xA;&lt;p&gt;2025-06-30 19:58:27 Monday ｜&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://papers.cool/arxiv/2506.21571&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;了解大型推理模型的认知习惯&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;[PDF()]&lt;/strong&gt; &lt;strong&gt;[Copy]&lt;/strong&gt; &lt;strong&gt;[Kimi(2)]&lt;/strong&gt; &lt;strong&gt;[REL]&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt; : &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Jianshuo%20Dong&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Jianshuo Dong&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Yujia%20Fu&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Yujia Fu&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Chuanrui%20Hu&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Chuanrui Hu&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Chao%20Zhang&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/search/?searchtype=author&amp;amp;query=Han%20Qiu&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Han Qiu&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>新闻</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E6%96%B0%E9%97%BB/</link>
      <pubDate>Thu, 07 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E6%96%B0%E9%97%BB/</guid>
      <description>&lt;h1 id=&#34;新闻&#34;&gt;新闻&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;2025-07-02 15:30:17 Wednesday ｜ 刚刚，Claude Code推出Hooks功能，让AI 编程从「看心情」到「真工程」&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/7t1sJlLnA_ardC2cK_EEfg&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/7t1sJlLnA_ardC2cK_EEfg&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;这个基于Shell的钩子系统能在关键时刻自动运行，让你能够精确地编排每次编程会话中必须发生的事情。&lt;/p&gt;&#xA;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;2025-07-01 11:35:41 Tuesday ｜ 2025年AI现状报告 &lt;a href=&#34;https://mp.weixin.qq.com/s/XKHqP-dDIaK0ny8iIJK9aA&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/XKHqP-dDIaK0ny8iIJK9aA&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;报告链接：https://cdn.prod.website-files.com/65d0d38fc4ec8ce8a8921654/685ac42fd2ed80e09b44e889_ICONIQ%20Analytics_Insights_The_AI_Builders_Playbook_2025.pdf&lt;/p&gt;&#xA;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;2025-07-01 11:30:31 Tuesday ｜ 7万个模型、1600万开发者，魔搭已建成中国最大AI开源社区 &lt;a href=&#34;https://mp.weixin.qq.com/s/nAa4WcjcQz93neQHuMvo6w&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/nAa4WcjcQz93neQHuMvo6w&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;2025-07-01 11:25:06 Tuesday｜真·全民AI健康管家来了！实测蚂蚁AQ：追问识药看皮肤，还能连医院接硬件  &lt;a href=&#34;https://mp.weixin.qq.com/s/FufBkoQv5Csk9FiG3-GaTQ&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/FufBkoQv5Csk9FiG3-GaTQ&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;2025-06-26 12:07:09 Thursday ｜ 华为开发者大会 2025（HDC 2025）上发布了 CloudRobo 具身智能平台https://mp.weixin.qq.com/s/rtBnAnEvsJr0TIOQzzm30w&lt;/li&gt;&#xA;&lt;li&gt;20250605｜openai上线会议记录 + MCP连接器（Model Context Protocol，把GitHub、Google Drive、Dropbox、SharePoint……这些原本互不搭界的知识源头，全都通过MCP接入 ChatGPT）&lt;/li&gt;&#xA;&lt;li&gt;2025-06-10 10:13:38 Tuesday ｜ 苹果展示了把人工智能融入其所有产品的计划，提供从通话实时翻译到 AI 识物、智能搜索等一系列能力。https://mp.weixin.qq.com/s/E5O6BuHny7vVY2hP0y3IIg&lt;/li&gt;&#xA;&lt;li&gt;2025-06-12 10:53:21 Thursday | meta发布世界模型intphys2 &lt;a href=&#34;https://mp.weixin.qq.com/s/i2lMeFX6VWWxqL_ZKmznfw&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://mp.weixin.qq.com/s/i2lMeFX6VWWxqL_ZKmznfw&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;2025-06-12 10:53:25 Thursday | 欧洲人工智能公司 Mistral AI 发布了 Magistral，这是一个全新的大语言模型（LLM）系列，展现了强大的推理能力。它能够进行不断反思，并解决更复杂的任务。https://mp.weixin.qq.com/s/Go5dXv4DA3hy5lGhxxE8SA&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;观点&#34;&gt;观点&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/VeuaDE9onAlWZ430SRiybw&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Token成本下降，订阅费却飞涨，AI公司怎么了？&lt;/a&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;LLM 成本每年会下降 10 倍。AI 能完成的任务长度，每 6 个月翻一倍。&lt;/li&gt;&#xA;&lt;li&gt;用户只对「最强语言模型」有需求，仅此而已。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;研讨&#34;&gt;研讨&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;2025-07-01 12:14:04 Tuesday｜&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://papers.cool/arxiv/2506.22698&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;人类和人工智能的文本生成和理解：跨学科研讨会报告&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;[PDF(1)]&lt;/strong&gt; &lt;strong&gt;[Copy]&lt;/strong&gt; &lt;strong&gt;[Kimi()]&lt;/strong&gt; &lt;strong&gt;[REL]&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>研究方向</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/</link>
      <pubDate>Thu, 07 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/</guid>
      <description>&lt;h1 id=&#34;研究方向&#34;&gt;研究方向&lt;/h1&gt;&#xA;&lt;h2 id=&#34;持续学习&#34;&gt;持续学习&lt;/h2&gt;&#xA;&lt;h4 id=&#34;gere面向-llm-持续学习中通过通用样本重放实现高效抗遗忘的探索&#34;&gt;&lt;a href=&#34;https://papers.cool/arxiv/2508.04676&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;GeRe：面向 LLM 持续学习中通过通用样本重放实现高效抗遗忘的探索&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://papers.cool/arxiv/2508.04676&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay &lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;**大型语言模型（LLMs）的持续学习能力对于推动通用人工智能的发展至关重要。然而，在不同领域对 LLMs 进行持续微调时，常常会遭遇灾难性遗忘，表现为：1）其通用能力显著下降，2）先前学习任务的性能急剧下降。为了以简单且稳定的方式同时解决这两个问题，我们提出了通用样本重放（General Sample Replay，GeRe）框架，该框架利用常规预训练文本实现高效的抗遗忘。除了在 GeRe 框架下回顾最常见的基于重放的实践外，我们进一步利用神经状态，引入了一种基于阈值边际（TM）损失的增强激活状态约束优化方法，以在重放学习过程中保持激活状态的一致性。我们首次验证了，一小组固定的预先收集的通用重放样本足以解决这两个问题——既保留通用能力，又促进顺序任务的整体性能。事实上，前者本质上可以促进后者。 通过受控实验，我们在 GeRe 框架下系统地比较了 TM 与不同的重放策略，包括普通的标签拟合、通过 KL 散度进行的 logit 模仿以及通过 L1/L2 损失进行的特征模仿。结果表明，TM 始终提升了性能并表现出更好的鲁棒性。我们的工作为未来高效重放 LLMs 铺平了道路。我们的代码和数据可在 **&lt;a href=&#34;https://github.com/Qznan/GeRe&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/Qznan/GeRe&lt;/a&gt; 获取。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Subjects&lt;/strong&gt;: &lt;a href=&#34;https://papers.cool/arxiv/cs.CL&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Computation and Language&lt;/a&gt;, &lt;a href=&#34;https://papers.cool/arxiv/cs.AI&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Artificial Intelligence&lt;/a&gt;, &lt;a href=&#34;https://papers.cool/arxiv/cs.LG&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Machine Learning&lt;/a&gt;&#xA;&lt;strong&gt;主题：计算与语言，人工智能，机器学习&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Publish&lt;/strong&gt;: 2025-08-06 17:42:22 UTC**&#xA;**发布：2025-08-06 17:42:22 UTC&lt;/p&gt;</description>
    </item>
    <item>
      <title>规划</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E8%A7%84%E5%88%92/</link>
      <pubDate>Thu, 07 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E8%A7%84%E5%88%92/</guid>
      <description>&lt;h1 id=&#34;规划&#34;&gt;规划&lt;/h1&gt;&#xA;&lt;h2 id=&#34;训练&#34;&gt;训练&lt;/h2&gt;&#xA;&lt;h3 id=&#34;o1&#34;&gt;o1&lt;/h3&gt;&#xA;&lt;h4 id=&#34;神经-符号融合规划器&#34;&gt;&lt;strong&gt;“神经-符号”融合规划器&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/IpG_Y0Lu767pLWdzTmtLXA&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;“神经-符号”融合规划器性能显著超越o1：借鉴人类运动学习机制｜中国科学院磐石研发团队&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;论文链接：https://www.sciencedirect.com/science/article/abs/pii/S095070512501086X?via%3Dihub&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;混合规划器，同时融合了神经规划系统和符号规划系统的优势。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;借鉴人类的闭环反馈机制，构建 &lt;strong&gt;双向规划机制&lt;/strong&gt; ，在表达能力、适应能力、泛化能力以及可解释性上都实现了显著提升。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;基于 &lt;strong&gt;Knowledge of Result&lt;/strong&gt;  &lt;em&gt;（KR）&lt;/em&gt; 的闭环系统是人类运动学习的关键部分，可以帮助学习者纠正错误，向着目标方向实现有效学习。&lt;/li&gt;&#xA;&lt;li&gt;在运动学习中KR是执行运动后的增强信息，表明既定目标是否成功，而闭环系统是以反馈、错误检测和错误纠正为核心的过程。规划任务中的问题、规划器和动作序列可近似对应于人类运动学习中的试验、学习者和行动序列，规划任务与运动学习有较强的相似性。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;“神经-符号”融合规划器&lt;/strong&gt;通过借鉴人类运动学习中的反馈闭环理念，构建了一种闭环反馈的双向规划机制—— &lt;strong&gt;KRCL&lt;/strong&gt;  &lt;em&gt;(Knowledge-of-Results based Closed-Loop)&lt;/em&gt; ，正向神经规划器生成问题的动作序列与反向KR反馈机制构成动态的错误检测-纠正闭环。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;还能只在正向规划器需要时，自动激活反馈接收，在规划覆盖率和规划效率上均显著优于 &lt;strong&gt;OpenAI o1&lt;/strong&gt; 。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;评测&#34;&gt;评测&lt;/h2&gt;&#xA;&lt;h4 id=&#34;planbench-用于评估大语言模型规划性能的基准数据集&#34;&gt;&lt;strong&gt;PlanBench&lt;/strong&gt; &lt;em&gt;（用于评估大语言模型规划性能的基准数据集）&lt;/em&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;看似简单的规划问题实际上计算复杂倒计时游戏&#34;&gt;看似简单的规划问题实际上计算复杂：倒计时游戏&lt;/h4&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2508.02900&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;#46&lt;/a&gt;&lt;a href=&#34;https://papers.cool/arxiv/2508.02900&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;Seemingly Simple Planning Problems are Computationally Challenging: The Countdown Game&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;普遍认为，当前基础模型和智能体无法制定长期计划是其主要局限之一。然而，现有的规划基准测试远远不足以真正衡量它们的规划能力。大多数现有基准测试要么侧重于像旅行规划这样定义模糊的任务，要么最终利用国际规划竞赛中的现有领域和问题。前者任务难以形式化和验证，后者则专门设计用来测试和挑战现有自动规划器的弱点。为了解决这些不足，我们提出了一种创建以名为 Countdown 的游戏为核心的规划基准测试的方法，该游戏要求玩家通过算术运算从一组输入数字中形成目标数字。我们讨论了该问题如何满足与理想规划能力评估基准相关的多项期望条件。 具体来说，该领域允许对每个问题实例进行直观的自然语言描述，计算上具有挑战性（NP 完全），且实例空间足够丰富，因此我们无需担心记忆问题。我们进行了广泛的理论分析，确立了计算复杂性结果，并展示了我们的实例生成程序相较于公共基准的优势。我们评估了多种现有的 LLM 辅助规划方法在使用我们程序生成的实例上的表现。结果表明，与 24 点游戏（Countdown 的一个特例）等其他领域不同，我们提出的动态基准对现有基于 LLM 的方法仍然极具挑战性。&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;发布：2025-08-04 21:01:03 UTC&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>训练</title>
      <link>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E8%AE%AD%E7%BB%83/</link>
      <pubDate>Thu, 07 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://dujh22.github.io/LLMDailyDigest.github.io/topic/%E8%AE%AD%E7%BB%83/</guid>
      <description>&lt;h1 id=&#34;训练&#34;&gt;训练&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;2025-06-13 18:50:11 Friday ｜&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Magistral&#xA;&lt;strong&gt;标题&lt;/strong&gt; ： Magistral&#xA;&lt;strong&gt;链接&lt;/strong&gt; ：https://arxiv.org/abs/2506.10910&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;作者&lt;/strong&gt; ： Mistral-AI:  Abhinav Rastogi,  Albert Q. Jiang,  Andy Lo,  Gabrielle Berrada,  Guillaume Lample,  Jason Rute,  Joep Barmentlo,  Karmesh Yadav,  Kartik Khandelwal,  Khyathi Raghavi Chandu,  Léonard Blier,  Lucile Saulnier,  Matthieu Dinot,  Maxime Darrin,  Neha Gupta,  Roman Soletskyi,  Sagar Vaze,  Teven Le Scao,  Yihan Wang,  Adam Yang,  Alexander H. Liu,  Alexandre Sablayrolles,  Amélie Héliou,  Amélie Martin,  Andy Ehrenberg,  Anmol Agarwal,  Antoine Roux,  Arthur Darcet,  Arthur Mensch,  Baptiste Bout,  Baptiste Rozière,  Baudouin De Monicault,  Chris Bamford,  Christian Wallenwein,  Christophe Renaudin,  Clémence Lanfranchi,  Darius Dabert,  Devon Mizelle,  Diego de las Casas,  Elliot Chane-Sane,  Emilien Fugier,  Emma Bou Hanna,  Gauthier Delerce,  Gauthier Guinet,  Georgii Novikov,  Guillaume Martin,  et al. (53 additional authors not shown)&#xA;&lt;strong&gt;摘要&lt;/strong&gt; ：我们介绍了Magistral，Mistral的第一个推理模型和我们自己的可扩展强化学习（RL）管道。我们不依赖于现有的实现和从先前模型中提取的RL跟踪，而是遵循一种自上而下的方法，完全依赖于我们自己的模型和基础设施。值得注意的是，我们展示了一个堆栈，使我们能够探索LLM的纯RL训练的限制，提出了一种简单的方法来强制模型的推理语言，并表明仅对文本数据的RL保持了大部分初始检查点的功能。我们发现，RL的文本保持或提高多模态理解，指令遵循和函数调用。我们提出了Magistral Medium，在Mistral Medium 3的基础上单独使用RL进行推理训练，我们开源了Magistral Small（Apache 2.0），其中进一步包括来自Magistral Medium的冷启动数据。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
