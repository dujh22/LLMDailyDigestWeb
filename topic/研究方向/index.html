<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head><script src="/LLMDailyDigestWeb/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=LLMDailyDigestWeb/livereload" data-no-instant defer></script>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>研究方向 - LLM-DailyDigest</title><meta name="author" content="">
<meta name="author-link" content="">
<meta name="description" content="研究方向
持续学习
GeRe：面向 LLM 持续学习中通过通用样本重放实现高效抗遗忘的探索
GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay 
**大型语言模型（LLMs）的持续学习能力对于推动通用人工智能的发展至关重要。然而，在不同领域对 LLMs 进行持续微调时，常常会遭遇灾难性遗忘，表现为：1）其通用能力显著下降，2）先前学习任务的性能急剧下降。为了以简单且稳定的方式同时解决这两个问题，我们提出了通用样本重放（General Sample Replay，GeRe）框架，该框架利用常规预训练文本实现高效的抗遗忘。除了在 GeRe 框架下回顾最常见的基于重放的实践外，我们进一步利用神经状态，引入了一种基于阈值边际（TM）损失的增强激活状态约束优化方法，以在重放学习过程中保持激活状态的一致性。我们首次验证了，一小组固定的预先收集的通用重放样本足以解决这两个问题——既保留通用能力，又促进顺序任务的整体性能。事实上，前者本质上可以促进后者。 通过受控实验，我们在 GeRe 框架下系统地比较了 TM 与不同的重放策略，包括普通的标签拟合、通过 KL 散度进行的 logit 模仿以及通过 L1/L2 损失进行的特征模仿。结果表明，TM 始终提升了性能并表现出更好的鲁棒性。我们的工作为未来高效重放 LLMs 铺平了道路。我们的代码和数据可在 **https://github.com/Qznan/GeRe 获取。
Subjects: Computation and Language, Artificial Intelligence, Machine Learning
主题：计算与语言，人工智能，机器学习
Publish: 2025-08-06 17:42:22 UTC**
**发布：2025-08-06 17:42:22 UTC" /><meta name="keywords" content='Hugo, FixIt' />
  <meta itemprop="name" content="研究方向">
  <meta itemprop="description" content="研究方向 持续学习 GeRe：面向 LLM 持续学习中通过通用样本重放实现高效抗遗忘的探索 GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay **大型语言模型（LLMs）的持续学习能力对于推动通用人工智能的发展至关重要。然而，在不同领域对 LLMs 进行持续微调时，常常会遭遇灾难性遗忘，表现为：1）其通用能力显著下降，2）先前学习任务的性能急剧下降。为了以简单且稳定的方式同时解决这两个问题，我们提出了通用样本重放（General Sample Replay，GeRe）框架，该框架利用常规预训练文本实现高效的抗遗忘。除了在 GeRe 框架下回顾最常见的基于重放的实践外，我们进一步利用神经状态，引入了一种基于阈值边际（TM）损失的增强激活状态约束优化方法，以在重放学习过程中保持激活状态的一致性。我们首次验证了，一小组固定的预先收集的通用重放样本足以解决这两个问题——既保留通用能力，又促进顺序任务的整体性能。事实上，前者本质上可以促进后者。 通过受控实验，我们在 GeRe 框架下系统地比较了 TM 与不同的重放策略，包括普通的标签拟合、通过 KL 散度进行的 logit 模仿以及通过 L1/L2 损失进行的特征模仿。结果表明，TM 始终提升了性能并表现出更好的鲁棒性。我们的工作为未来高效重放 LLMs 铺平了道路。我们的代码和数据可在 **https://github.com/Qznan/GeRe 获取。
Subjects: Computation and Language, Artificial Intelligence, Machine Learning 主题：计算与语言，人工智能，机器学习
Publish: 2025-08-06 17:42:22 UTC** **发布：2025-08-06 17:42:22 UTC">
  <meta itemprop="datePublished" content="2025-08-07T00:00:00+08:00">
  <meta itemprop="dateModified" content="2025-08-07T00:00:00+08:00">
  <meta itemprop="wordCount" content="177"><meta property="og:url" content="http://localhost:1313/LLMDailyDigestWeb/topic/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/">
  <meta property="og:site_name" content="LLM-DailyDigest">
  <meta property="og:title" content="研究方向">
  <meta property="og:description" content="研究方向 持续学习 GeRe：面向 LLM 持续学习中通过通用样本重放实现高效抗遗忘的探索 GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay **大型语言模型（LLMs）的持续学习能力对于推动通用人工智能的发展至关重要。然而，在不同领域对 LLMs 进行持续微调时，常常会遭遇灾难性遗忘，表现为：1）其通用能力显著下降，2）先前学习任务的性能急剧下降。为了以简单且稳定的方式同时解决这两个问题，我们提出了通用样本重放（General Sample Replay，GeRe）框架，该框架利用常规预训练文本实现高效的抗遗忘。除了在 GeRe 框架下回顾最常见的基于重放的实践外，我们进一步利用神经状态，引入了一种基于阈值边际（TM）损失的增强激活状态约束优化方法，以在重放学习过程中保持激活状态的一致性。我们首次验证了，一小组固定的预先收集的通用重放样本足以解决这两个问题——既保留通用能力，又促进顺序任务的整体性能。事实上，前者本质上可以促进后者。 通过受控实验，我们在 GeRe 框架下系统地比较了 TM 与不同的重放策略，包括普通的标签拟合、通过 KL 散度进行的 logit 模仿以及通过 L1/L2 损失进行的特征模仿。结果表明，TM 始终提升了性能并表现出更好的鲁棒性。我们的工作为未来高效重放 LLMs 铺平了道路。我们的代码和数据可在 **https://github.com/Qznan/GeRe 获取。
Subjects: Computation and Language, Artificial Intelligence, Machine Learning 主题：计算与语言，人工智能，机器学习
Publish: 2025-08-06 17:42:22 UTC** **发布：2025-08-06 17:42:22 UTC">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="topic">
    <meta property="article:published_time" content="2025-08-07T00:00:00+08:00">
    <meta property="article:modified_time" content="2025-08-07T00:00:00+08:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="研究方向">
  <meta name="twitter:description" content="研究方向 持续学习 GeRe：面向 LLM 持续学习中通过通用样本重放实现高效抗遗忘的探索 GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay **大型语言模型（LLMs）的持续学习能力对于推动通用人工智能的发展至关重要。然而，在不同领域对 LLMs 进行持续微调时，常常会遭遇灾难性遗忘，表现为：1）其通用能力显著下降，2）先前学习任务的性能急剧下降。为了以简单且稳定的方式同时解决这两个问题，我们提出了通用样本重放（General Sample Replay，GeRe）框架，该框架利用常规预训练文本实现高效的抗遗忘。除了在 GeRe 框架下回顾最常见的基于重放的实践外，我们进一步利用神经状态，引入了一种基于阈值边际（TM）损失的增强激活状态约束优化方法，以在重放学习过程中保持激活状态的一致性。我们首次验证了，一小组固定的预先收集的通用重放样本足以解决这两个问题——既保留通用能力，又促进顺序任务的整体性能。事实上，前者本质上可以促进后者。 通过受控实验，我们在 GeRe 框架下系统地比较了 TM 与不同的重放策略，包括普通的标签拟合、通过 KL 散度进行的 logit 模仿以及通过 L1/L2 损失进行的特征模仿。结果表明，TM 始终提升了性能并表现出更好的鲁棒性。我们的工作为未来高效重放 LLMs 铺平了道路。我们的代码和数据可在 **https://github.com/Qznan/GeRe 获取。
Subjects: Computation and Language, Artificial Intelligence, Machine Learning 主题：计算与语言，人工智能，机器学习
Publish: 2025-08-06 17:42:22 UTC** **发布：2025-08-06 17:42:22 UTC">
<meta name="application-name" content="LLM-DailyDigest">
<meta name="apple-mobile-web-app-title" content="LLM-DailyDigest"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" href="http://localhost:1313/LLMDailyDigestWeb/topic/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/" /><link rel="prev" href="http://localhost:1313/LLMDailyDigestWeb/topic/%E8%A7%84%E5%88%92/" /><link rel="next" href="http://localhost:1313/LLMDailyDigestWeb/topic/%E5%85%83/" /><link rel="stylesheet" href="/LLMDailyDigestWeb/css/style.min.css"><link rel="preload" href="/LLMDailyDigestWeb/lib/fontawesome-free/all.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/LLMDailyDigestWeb/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/LLMDailyDigestWeb/lib/animate/animate.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/LLMDailyDigestWeb/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "研究方向",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "http:\/\/localhost:1313\/LLMDailyDigestWeb\/topic\/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91\/"
    },"genre": "topic","wordcount":  177 ,
    "url": "http:\/\/localhost:1313\/LLMDailyDigestWeb\/topic\/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91\/","datePublished": "2025-08-07T00:00:00+08:00","dateModified": "2025-08-07T00:00:00+08:00","publisher": {
      "@type": "Organization",
      "name": ""},"author": {
        "@type": "Person",
        "name": "Author"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="sticky" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper" data-page-style="normal"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper">
    <div class="header-title">
      <a href="/LLMDailyDigestWeb/" title="LLM-DailyDigest"><img loading="lazy" src="/LLMDailyDigestWeb/fixit.svg" data-title="LLM-DailyDigest" data-alt="LLM-DailyDigest" class="logo" style="background: url(/LLMDailyDigestWeb/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}"/><span id="typeit-header-desktop" class="typeit"></span></a><span class="header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/LLMDailyDigestWeb/posts/llmdailydigest"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 详情</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/LLMDailyDigestWeb/resources/"
                
                
              >资源</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/LLMDailyDigestWeb/topic/"
                
                
              >主题</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/LLMDailyDigestWeb/updates/"
                
                
              >日报</a></li><li class="menu-item delimiter"></li><li class="menu-item theme-switch" title="Switch Theme">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li></ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/LLMDailyDigestWeb/" title="LLM-DailyDigest"><img loading="lazy" src="/LLMDailyDigestWeb/fixit.svg" data-title="/LLMDailyDigestWeb/fixit.svg" data-alt="/LLMDailyDigestWeb/fixit.svg" class="logo" style="background: url(/LLMDailyDigestWeb/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}"/><span id="typeit-header-title-mobile" class="typeit"></span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/LLMDailyDigestWeb/posts/llmdailydigest"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 详情</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/LLMDailyDigestWeb/resources/"
                  
                  
                >资源</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/LLMDailyDigestWeb/topic/"
                  
                  
                >主题</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/LLMDailyDigestWeb/updates/"
                  
                  
                >日报</a></li><li class="menu-item menu-system">
          <span class="menu-system-item theme-switch" title="Switch Theme"><i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i></span></li>
      </ul>
    </nav>
  </div>
</header><main class="container"><aside class="toc" id="toc-auto"><h2 class="toc-title">Contents&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden="true"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom">
    </aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX"><span>研究方向</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      </span></span></div>
      <div class="post-meta-line"><span title="published on 2025-08-07 00:00:00"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden="true"></i><time datetime="2025-08-07">2025-08-07</time></span>&nbsp;<span title="Updated on 2025-08-07 00:00:00"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden="true"></i><time datetime="2025-08-07">2025-08-07</time></span>&nbsp;<span title="177 words"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden="true"></i>About 200 words</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden="true"></i>One minute</span>&nbsp;</div>
    </div><div class="details toc" id="toc-static" data-kept="false">
        <div class="details-summary toc-title">
          <span>Contents</span><i class="details-icon fa-solid fa-angle-right" aria-hidden="true"></i></div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#持续学习">持续学习</a>
      <ul>
        <li>
          <ul>
            <li><a href="#gere面向-llm-持续学习中通过通用样本重放实现高效抗遗忘的探索"><a href="https://papers.cool/arxiv/2508.04676">GeRe：面向 LLM 持续学习中通过通用样本重放实现高效抗遗忘的探索</a></a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#涌现--scalinglaw">涌现 &amp; ScalingLaw</a>
      <ul>
        <li>
          <ul>
            <li><a href="#大型语言模型和涌现复杂系统的视角">大型语言模型和涌现：复杂系统的视角</a></li>
            <li><a href="#缩放定律对于下游任务不可靠现实检验"><strong><a href="https://papers.cool/arxiv/2507.00885">缩放定律对于下游任务不可靠：现实检验</a></strong></a></li>
            <li><a href="#为什么-llms-的能力是涌现的">为什么 LLMs 的能力是涌现的？</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><h1 id="研究方向">研究方向</h1>
<h2 id="持续学习">持续学习</h2>
<h4 id="gere面向-llm-持续学习中通过通用样本重放实现高效抗遗忘的探索"><a href="https://papers.cool/arxiv/2508.04676"target="_blank" rel="external nofollow noopener noreferrer">GeRe：面向 LLM 持续学习中通过通用样本重放实现高效抗遗忘的探索</a></h4>
<p><a href="https://papers.cool/arxiv/2508.04676"target="_blank" rel="external nofollow noopener noreferrer">GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay </a></p>
<p>**大型语言模型（LLMs）的持续学习能力对于推动通用人工智能的发展至关重要。然而，在不同领域对 LLMs 进行持续微调时，常常会遭遇灾难性遗忘，表现为：1）其通用能力显著下降，2）先前学习任务的性能急剧下降。为了以简单且稳定的方式同时解决这两个问题，我们提出了通用样本重放（General Sample Replay，GeRe）框架，该框架利用常规预训练文本实现高效的抗遗忘。除了在 GeRe 框架下回顾最常见的基于重放的实践外，我们进一步利用神经状态，引入了一种基于阈值边际（TM）损失的增强激活状态约束优化方法，以在重放学习过程中保持激活状态的一致性。我们首次验证了，一小组固定的预先收集的通用重放样本足以解决这两个问题——既保留通用能力，又促进顺序任务的整体性能。事实上，前者本质上可以促进后者。 通过受控实验，我们在 GeRe 框架下系统地比较了 TM 与不同的重放策略，包括普通的标签拟合、通过 KL 散度进行的 logit 模仿以及通过 L1/L2 损失进行的特征模仿。结果表明，TM 始终提升了性能并表现出更好的鲁棒性。我们的工作为未来高效重放 LLMs 铺平了道路。我们的代码和数据可在 **<a href="https://github.com/Qznan/GeRe"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Qznan/GeRe</a> 获取。</p>
<p><strong>Subjects</strong>: <a href="https://papers.cool/arxiv/cs.CL"target="_blank" rel="external nofollow noopener noreferrer">Computation and Language</a>, <a href="https://papers.cool/arxiv/cs.AI"target="_blank" rel="external nofollow noopener noreferrer">Artificial Intelligence</a>, <a href="https://papers.cool/arxiv/cs.LG"target="_blank" rel="external nofollow noopener noreferrer">Machine Learning</a>
<strong>主题：计算与语言，人工智能，机器学习</strong></p>
<p><strong>Publish</strong>: 2025-08-06 17:42:22 UTC**
**发布：2025-08-06 17:42:22 UTC</p>
<h2 id="涌现--scalinglaw">涌现 &amp; ScalingLaw</h2>
<h4 id="大型语言模型和涌现复杂系统的视角">大型语言模型和涌现：复杂系统的视角</h4>
<p>2025-06-16 12:03:18 Monday</p>
<p>Large Language Models and Emergence: A Complex Systems Perspective
<strong>标题</strong> ： 大型语言模型和涌现：复杂系统的视角
<strong>链接</strong> ：https://arxiv.org/abs/2506.11135</p>
<p><strong>作者</strong> ： David C. Krakauer,  John W. Krakauer,  Melanie Mitchell
<strong>摘要</strong> ： <strong>涌现是复杂性科学中的一个概念</strong> ，描述了多体系统如何表现出新的更高层次的属性，这些属性可以通过用低维有效变量和理论取代高维机制来描述。这一点被“ <strong>更多即是不同</strong> ”的理念所捕捉。智能是一种完美的突现属性，它表现出越来越高效、更便宜、更快地利用突现能力来解决问题。这是由“ <strong>少即是多</strong> ”的理念所体现的。在本文中，我们首先研究声称，大型语言模型表现出涌现能力，审查几种方法来量化涌现，其次问LLM是否具有涌现智能。</p>
<ol start="3">
<li>2025-06-18 10:56:22 Wednesday ｜ Capability Salience Vector: Fine-grained Alignment of Loss and  Capabilities for Downstream Task Scaling Law
<strong>标题</strong> ： 能力显著性载体：下游任务缩放定律的损失和能力的细粒度对齐
<strong>链接</strong> ：https://arxiv.org/abs/2506.13216</li>
<li>2025-06-18 10:56:54 Wednesday ｜ AI Flow: Perspectives, Scenarios, and Approaches
<strong>标题</strong> ： 人工智能流程：观点、场景和方法</li>
</ol>
<p><strong>链接</strong> ：https://arxiv.org/abs/2506.12479</p>
<h4 id="缩放定律对于下游任务不可靠现实检验"><strong><a href="https://papers.cool/arxiv/2507.00885"target="_blank" rel="external nofollow noopener noreferrer">缩放定律对于下游任务不可靠：现实检验</a></strong></h4>
<p>2025-07-02 15:37:55 Wednesday</p>
<p><strong>Authors</strong> : <a href="https://arxiv.org/search/?searchtype=author&amp;query=Nicholas%20Lourie"target="_blank" rel="external nofollow noopener noreferrer">Nicholas Lourie</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Michael%20Y.%20Hu"target="_blank" rel="external nofollow noopener noreferrer">Michael Y. Hu</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kyunghyun%20Cho"target="_blank" rel="external nofollow noopener noreferrer">Kyunghyun Cho</a></p>
<p>下游缩放定律旨在通过较小规模的预训练损失来预测较大规模的任务性能。这种预测是否可能尚不清楚：一些工作表明，任务性能在转换下遵循明显的线性扩展趋势，而另一些工作则指出了下游扩展定律的根本挑战，例如涌现和逆扩展。在这项工作中，我们对下游缩放定律的现有数据进行了荟萃分析，发现接近线性缩放定律只发生在少数情况下：39% 的时间。此外，对实验设置的看似良性的变化可能会完全改变缩放趋势。我们的分析强调了了解缩放定律成功的条件的必要性。为了完全模拟预训练损失和下游任务性能之间的关系，我们必须接受扩展行为偏离线性趋势的情况。</p>
<p><strong>科目</strong> :  <strong><a href="https://papers.cool/arxiv/cs.CL"target="_blank" rel="external nofollow noopener noreferrer">计算和语言</a></strong> , <a href="https://papers.cool/arxiv/cs.LG"target="_blank" rel="external nofollow noopener noreferrer">机器学习</a></p>
<p><strong>发布</strong> ： 2025-07-01 15：52：55 UTC</p>
<h4 id="为什么-llms-的能力是涌现的">为什么 LLMs 的能力是涌现的？</h4>
<p><a href="https://arxiv.org/abs/2508.04401"target="_blank" rel="external nofollow noopener noreferrer">#21</a><a href="https://papers.cool/arxiv/2508.04401"target="_blank" rel="external nofollow noopener noreferrer">Why are LLMs' abilities emergent?</a></p>
<p>大型语言模型（LLMs）在生成任务中的显著成功引发了关于其所获得能力本质的根本性问题，这些能力常常在没有明确训练的情况下意外出现。本文通过理论分析和实证观察，探讨了深度神经网络（DNNs）的涌现特性，回应了当代人工智能发展中“无理解的创造”这一认识论挑战。我们探讨了神经方法依赖非线性、随机过程的本质区别于符号计算范式，造就了其宏观行为无法从微观神经元活动中解析推导的系统。通过对规模定律、grokking 现象以及模型能力相变的分析，我展示了涌现能力源自高度敏感非线性系统的复杂动力学，而非仅仅是参数规模的简单扩展。我的研究揭示，目前关于指标、预训练损失阈值和上下文学习的争论忽视了 DNN 涌现的根本本体性质。 我认为这些系统表现出真正的涌现特性，类似于其他复杂自然现象中发现的特性，其中系统能力是由简单组件之间的协作互动产生的，且无法简化为其个体行为。本文结论指出，理解 LLM 的能力需要将深度神经网络（DNN）视为一个新的<strong>复杂动力系统</strong>领域，该领域受涌现的普遍原理支配，类似于物理、化学和生物学中运作的原理。这一视角将关注点从纯粹的现象学涌现定义转向理解使这些系统获得超越其个体组件能力的内部动态转变。</p>
<p>发布：2025-08-06 12:43:04 UTC</p>
</div></article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">Public - 2025</span><span class="author" itemprop="copyrightHolder">
              <a href="/LLMDailyDigestWeb/"></a></span></div><div class="footer-line statistics"></div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="Back to Top"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric d-none">0%</span>
        </div></div><div id="mask"></div><noscript>
    <div class="noscript-warning">Theme FixIt works best with JavaScript enabled.</div>
  </noscript>
</div><link rel="preload" href="/LLMDailyDigestWeb/lib/katex/katex.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/LLMDailyDigestWeb/lib/katex/katex.min.css"></noscript><link rel="stylesheet" href="/LLMDailyDigestWeb/lib/cookieconsent/cookieconsent.min.css"><script src="/LLMDailyDigestWeb/lib/sharer/sharer.min.js" async defer></script><script src="/LLMDailyDigestWeb/lib/typeit/index.umd.js" defer></script><script src="/LLMDailyDigestWeb/lib/katex/katex.min.js" defer></script><script src="/LLMDailyDigestWeb/lib/katex/auto-render.min.js" defer></script><script src="/LLMDailyDigestWeb/lib/katex/copy-tex.min.js" defer></script><script src="/LLMDailyDigestWeb/lib/katex/mhchem.min.js" defer></script><script src="/LLMDailyDigestWeb/lib/cookieconsent/cookieconsent.min.js" defer></script><script>window.config={"code":{"copyTitle":"Copy to clipboard","editLockTitle":"Lock editable code block","editUnLockTitle":"Unlock editable code block","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-desktop":"LLM-DailyDigest 大模型研究日报","typeit-header-title-mobile":"LLM-DailyDigest 大模型研究日报"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-desktop":["typeit-header-desktop"],"typeit-header-title-mobile":["typeit-header-title-mobile"]},"duration":-1,"loop":false,"speed":100}};</script><script src="/LLMDailyDigestWeb/js/theme.min.js" defer></script></body>
</html>
