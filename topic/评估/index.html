<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head><script src="/LLMDailyDigestWeb/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=LLMDailyDigestWeb/livereload" data-no-instant defer></script>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>评估 - LLM-DailyDigest</title><meta name="author" content="">
<meta name="author-link" content="">
<meta name="description" content="评估

2025-06-27 14:20:08 Friday

#3 大型语言模型中的波将金式理解 [PDF (1) ] [复制] [Kimi (2)  **] [相关] ** #3Potemkin Understanding in Large Language Models [PDF(1)] [Copy] [Kimi(2)] [REL]
作者：玛丽娜·曼科里迪斯、贝卡·威克斯、凯永·瓦法、森迪尔·穆莱纳坦
大型语言模型 （LLM） 定期使用基准数据集进行评估。但是，有什么理由根据 LLM 对一系列精选问题的回答来推断 LLM 的能力呢？本文首先介绍了一个正式的框架来解决这个问题。关键是要注意，用于测试 LLM 的基准（例如 AP 考试）也是用于测试人的基准。然而，这提出了一个含义：只有当 LLM 以反映人类误解的方式误解概念时，这些基准才是有效的测试。否则，基准测试的成功只能证明波将金理解：由与任何人解释概念的方式不可调和的答案驱动的理解幻觉。我们提出了两个量化 potemkins 存在的程序：一个在三个领域使用专门设计的基准，另一个使用提供其流行率下限的通用程序。我们发现 potemkins 在模型、任务和领域中无处不在。我们还发现，这些失败不仅反映了不正确的理解，还反映了概念表示中更深层次的内部不连贯性。

20250604｜BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation

标题： BenchHub：用于整体和可定制LLM评估的统一基准套件
链接：https://arxiv.org/abs/2506.00482


2025-06-12 13:43:03 Thursday ｜

EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in  Realistic Web Environments
标题 ： EcoWebArena：现实Web环境中对自治代理进行经济任务的基准测试
链接 ：https://arxiv.org/abs/2506.08136" /><meta name="keywords" content='Hugo, FixIt' />
  <meta itemprop="name" content="评估">
  <meta itemprop="description" content="评估 2025-06-27 14:20:08 Friday #3 大型语言模型中的波将金式理解 [PDF (1) ] [复制] [Kimi (2) **] [相关] ** #3Potemkin Understanding in Large Language Models [PDF(1)] [Copy] [Kimi(2)] [REL]
作者：玛丽娜·曼科里迪斯、贝卡·威克斯、凯永·瓦法、森迪尔·穆莱纳坦
大型语言模型 （LLM） 定期使用基准数据集进行评估。但是，有什么理由根据 LLM 对一系列精选问题的回答来推断 LLM 的能力呢？本文首先介绍了一个正式的框架来解决这个问题。关键是要注意，用于测试 LLM 的基准（例如 AP 考试）也是用于测试人的基准。然而，这提出了一个含义：只有当 LLM 以反映人类误解的方式误解概念时，这些基准才是有效的测试。否则，基准测试的成功只能证明波将金理解：由与任何人解释概念的方式不可调和的答案驱动的理解幻觉。我们提出了两个量化 potemkins 存在的程序：一个在三个领域使用专门设计的基准，另一个使用提供其流行率下限的通用程序。我们发现 potemkins 在模型、任务和领域中无处不在。我们还发现，这些失败不仅反映了不正确的理解，还反映了概念表示中更深层次的内部不连贯性。
20250604｜BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation 标题： BenchHub：用于整体和可定制LLM评估的统一基准套件 链接：https://arxiv.org/abs/2506.00482 2025-06-12 13:43:03 Thursday ｜ EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments 标题 ： EcoWebArena：现实Web环境中对自治代理进行经济任务的基准测试 链接 ：https://arxiv.org/abs/2506.08136">
  <meta itemprop="datePublished" content="2025-08-15T00:00:00+08:00">
  <meta itemprop="dateModified" content="2025-08-15T00:00:00+08:00">
  <meta itemprop="wordCount" content="1173"><meta property="og:url" content="http://localhost:1313/LLMDailyDigestWeb/topic/%E8%AF%84%E4%BC%B0/">
  <meta property="og:site_name" content="LLM-DailyDigest">
  <meta property="og:title" content="评估">
  <meta property="og:description" content="评估 2025-06-27 14:20:08 Friday #3 大型语言模型中的波将金式理解 [PDF (1) ] [复制] [Kimi (2) **] [相关] ** #3Potemkin Understanding in Large Language Models [PDF(1)] [Copy] [Kimi(2)] [REL]
作者：玛丽娜·曼科里迪斯、贝卡·威克斯、凯永·瓦法、森迪尔·穆莱纳坦
大型语言模型 （LLM） 定期使用基准数据集进行评估。但是，有什么理由根据 LLM 对一系列精选问题的回答来推断 LLM 的能力呢？本文首先介绍了一个正式的框架来解决这个问题。关键是要注意，用于测试 LLM 的基准（例如 AP 考试）也是用于测试人的基准。然而，这提出了一个含义：只有当 LLM 以反映人类误解的方式误解概念时，这些基准才是有效的测试。否则，基准测试的成功只能证明波将金理解：由与任何人解释概念的方式不可调和的答案驱动的理解幻觉。我们提出了两个量化 potemkins 存在的程序：一个在三个领域使用专门设计的基准，另一个使用提供其流行率下限的通用程序。我们发现 potemkins 在模型、任务和领域中无处不在。我们还发现，这些失败不仅反映了不正确的理解，还反映了概念表示中更深层次的内部不连贯性。
20250604｜BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation 标题： BenchHub：用于整体和可定制LLM评估的统一基准套件 链接：https://arxiv.org/abs/2506.00482 2025-06-12 13:43:03 Thursday ｜ EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments 标题 ： EcoWebArena：现实Web环境中对自治代理进行经济任务的基准测试 链接 ：https://arxiv.org/abs/2506.08136">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="topic">
    <meta property="article:published_time" content="2025-08-15T00:00:00+08:00">
    <meta property="article:modified_time" content="2025-08-15T00:00:00+08:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="评估">
  <meta name="twitter:description" content="评估 2025-06-27 14:20:08 Friday #3 大型语言模型中的波将金式理解 [PDF (1) ] [复制] [Kimi (2) **] [相关] ** #3Potemkin Understanding in Large Language Models [PDF(1)] [Copy] [Kimi(2)] [REL]
作者：玛丽娜·曼科里迪斯、贝卡·威克斯、凯永·瓦法、森迪尔·穆莱纳坦
大型语言模型 （LLM） 定期使用基准数据集进行评估。但是，有什么理由根据 LLM 对一系列精选问题的回答来推断 LLM 的能力呢？本文首先介绍了一个正式的框架来解决这个问题。关键是要注意，用于测试 LLM 的基准（例如 AP 考试）也是用于测试人的基准。然而，这提出了一个含义：只有当 LLM 以反映人类误解的方式误解概念时，这些基准才是有效的测试。否则，基准测试的成功只能证明波将金理解：由与任何人解释概念的方式不可调和的答案驱动的理解幻觉。我们提出了两个量化 potemkins 存在的程序：一个在三个领域使用专门设计的基准，另一个使用提供其流行率下限的通用程序。我们发现 potemkins 在模型、任务和领域中无处不在。我们还发现，这些失败不仅反映了不正确的理解，还反映了概念表示中更深层次的内部不连贯性。
20250604｜BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation 标题： BenchHub：用于整体和可定制LLM评估的统一基准套件 链接：https://arxiv.org/abs/2506.00482 2025-06-12 13:43:03 Thursday ｜ EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments 标题 ： EcoWebArena：现实Web环境中对自治代理进行经济任务的基准测试 链接 ：https://arxiv.org/abs/2506.08136">
<meta name="application-name" content="LLM-DailyDigest">
<meta name="apple-mobile-web-app-title" content="LLM-DailyDigest"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" href="http://localhost:1313/LLMDailyDigestWeb/topic/%E8%AF%84%E4%BC%B0/" /><link rel="prev" href="http://localhost:1313/LLMDailyDigestWeb/topic/%E9%80%BB%E8%BE%91%E6%8E%A8%E7%90%86/" /><link rel="next" href="http://localhost:1313/LLMDailyDigestWeb/topic/%E6%99%BA%E8%83%BD%E4%BD%93/" /><link rel="stylesheet" href="/LLMDailyDigestWeb/css/style.min.css"><link rel="preload" href="/LLMDailyDigestWeb/lib/fontawesome-free/all.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/LLMDailyDigestWeb/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/LLMDailyDigestWeb/lib/animate/animate.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/LLMDailyDigestWeb/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "评估",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "http:\/\/localhost:1313\/LLMDailyDigestWeb\/topic\/%E8%AF%84%E4%BC%B0\/"
    },"genre": "topic","wordcount":  1173 ,
    "url": "http:\/\/localhost:1313\/LLMDailyDigestWeb\/topic\/%E8%AF%84%E4%BC%B0\/","datePublished": "2025-08-15T00:00:00+08:00","dateModified": "2025-08-15T00:00:00+08:00","publisher": {
      "@type": "Organization",
      "name": ""},"author": {
        "@type": "Person",
        "name": "Author"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="sticky" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper" data-page-style="normal"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper">
    <div class="header-title">
      <a href="/LLMDailyDigestWeb/" title="LLM-DailyDigest"><img loading="lazy" src="/LLMDailyDigestWeb/fixit.svg" data-title="LLM-DailyDigest" data-alt="LLM-DailyDigest" class="logo" style="background: url(/LLMDailyDigestWeb/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}"/><span id="typeit-header-desktop" class="typeit"></span></a><span class="header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/LLMDailyDigestWeb/posts/llmdailydigest"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 详情</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/LLMDailyDigestWeb/resources/"
                
                
              >资源</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/LLMDailyDigestWeb/topic/"
                
                
              >主题</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/LLMDailyDigestWeb/updates/"
                
                
              >日报</a></li><li class="menu-item delimiter"></li><li class="menu-item theme-switch" title="Switch Theme">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li></ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/LLMDailyDigestWeb/" title="LLM-DailyDigest"><img loading="lazy" src="/LLMDailyDigestWeb/fixit.svg" data-title="/LLMDailyDigestWeb/fixit.svg" data-alt="/LLMDailyDigestWeb/fixit.svg" class="logo" style="background: url(/LLMDailyDigestWeb/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}"/><span id="typeit-header-title-mobile" class="typeit"></span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/LLMDailyDigestWeb/posts/llmdailydigest"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 详情</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/LLMDailyDigestWeb/resources/"
                  
                  
                >资源</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/LLMDailyDigestWeb/topic/"
                  
                  
                >主题</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/LLMDailyDigestWeb/updates/"
                  
                  
                >日报</a></li><li class="menu-item menu-system">
          <span class="menu-system-item theme-switch" title="Switch Theme"><i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i></span></li>
      </ul>
    </nav>
  </div>
</header><main class="container"><aside class="toc" id="toc-auto"><h2 class="toc-title">Contents&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden="true"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom">
    </aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX"><span>评估</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      </span></span></div>
      <div class="post-meta-line"><span title="published on 2025-08-15 00:00:00"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden="true"></i><time datetime="2025-08-15">2025-08-15</time></span>&nbsp;<span title="Updated on 2025-08-15 00:00:00"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden="true"></i><time datetime="2025-08-15">2025-08-15</time></span>&nbsp;<span title="1173 words"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden="true"></i>About 1200 words</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden="true"></i>6 minutes</span>&nbsp;</div>
    </div><div class="details toc" id="toc-static" data-kept="false">
        <div class="details-summary toc-title">
          <span>Contents</span><i class="details-icon fa-solid fa-angle-right" aria-hidden="true"></i></div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#榜单">榜单</a></li>
    <li><a href="#公平性">公平性</a></li>
    <li><a href="#评估可靠性">评估可靠性</a></li>
    <li><a href="#证明">证明</a></li>
    <li><a href="#数学">数学</a></li>
    <li><a href="#推理">推理</a>
      <ul>
        <li>
          <ul>
            <li><a href="#逻辑-gpt-5grok-4o3-pro都零分史上最难ai评测基准换它了">逻辑 ｜<a href="https://mp.weixin.qq.com/s/cyOJ_Id606REj97nCXYqhg">GPT-5、Grok 4、o3 Pro都零分，史上最难AI评测基准换它了</a></a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#自动化评估">自动化评估</a></li>
    <li><a href="#代码">代码</a></li>
    <li><a href="#科学">科学</a></li>
    <li><a href="#工具使用">工具使用</a></li>
    <li><a href="#创意写作">创意写作</a></li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><h1 id="评估">评估</h1>
<ol>
<li>2025-06-27 14:20:08 Friday</li>
</ol>
<p><strong>#3 大型语言模型中的波将金式理解 [PDF</strong> <strong>(1)</strong> <strong>] [复制] [Kimi</strong> <strong>(2)</strong>  **] [相关] ** <strong><a href="https://arxiv.org/abs/2506.21521"target="_blank" rel="external nofollow noopener noreferrer">#3</a><strong><strong><a href="https://papers.cool/arxiv/2506.21521"target="_blank" rel="external nofollow noopener noreferrer">Potemkin Understanding in Large Language Models</a></strong></strong> [PDF(1)] [Copy] [Kimi(2)] [REL]</strong></p>
<p>作者：玛丽娜·曼科里迪斯、贝卡·威克斯、凯永·瓦法、森迪尔·穆莱纳坦</p>
<p>大型语言模型 （LLM） 定期使用基准数据集进行评估。但是，有什么理由根据 LLM 对一系列精选问题的回答来推断 LLM 的能力呢？本文首先介绍了一个正式的框架来解决这个问题。关键是要注意，用于测试 LLM 的基准（例如 AP 考试）也是用于测试人的基准。然而，这提出了一个含义：只有当 LLM 以反映人类误解的方式误解概念时，这些基准才是有效的测试。否则，基准测试的成功只能证明波将金理解：由与任何人解释概念的方式不可调和的答案驱动的理解幻觉。我们提出了两个量化 potemkins 存在的程序：一个在三个领域使用专门设计的基准，另一个使用提供其流行率下限的通用程序。我们发现 potemkins 在模型、任务和领域中无处不在。我们还发现，这些失败不仅反映了不正确的理解，还反映了概念表示中更深层次的内部不连贯性。</p>
<ol start="2">
<li>20250604｜BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation
<ol>
<li>标题： BenchHub：用于整体和可定制LLM评估的统一基准套件</li>
<li>链接：<a href="https://arxiv.org/abs/2506.00482"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2506.00482</a></li>
</ol>
</li>
<li>2025-06-12 13:43:03 Thursday ｜</li>
</ol>
<p>EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in  Realistic Web Environments
<strong>标题</strong> ： EcoWebArena：现实Web环境中对自治代理进行经济任务的基准测试
<strong>链接</strong> ：https://arxiv.org/abs/2506.08136</p>
<p><strong>作者</strong> ： Zefang Liu,  Yinzhu Quan
<strong>摘要</strong> ：我们介绍EconWebArena，在现实的网络环境中评估复杂的，多模态的经济任务的自主代理的基准。该基准包含来自82个权威网站的360个策划任务，涵盖宏观经济，劳动力，金融，贸易和公共政策等领域。每项任务都要求智能体浏览实时网站，解释结构化和视觉内容，与真实界面交互，并通过多步工作流程提取精确的时间敏感数据。我们通过 <strong>促使多个大型语言模型（LLM）生成候选任务来构建基准，然后进行严格的人工策展，以确保清晰度，可行性和源代码可靠性</strong> 。与以前的工作不同，EconWebArena强调对权威数据源的忠实性和基于网络的经济推理的需要。我们评估了一组不同的国家的最先进的多模态LLM作为网络代理，分析失败的情况下，并进行消融研究，以评估视觉接地，基于计划的推理和交互设计的影响。我们的研究结果揭示了巨大的性能差距，并强调了接地，导航和多模式理解方面的持续挑战，将EconWebArena定位为经济网络智能的严格测试平台。</p>
<ol start="4">
<li>2025-06-16 12:06:32 Monday ｜</li>
</ol>
<p>AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language  Models
<strong>标题</strong> ： AssertBench：评估大型语言模型中自我断言的基准
<strong>链接</strong> ：https://arxiv.org/abs/2506.11110</p>
<p><strong>作者</strong> ： Jaeho Lee,  Atharv Chowdhary
<strong>备注</strong> ：15 pages, 4 figures, appendix contains 2 additional figures and 2 tables
<strong>摘要</strong> ：最近的基准已经探索了大语言模型（LLM）中的事实一致性和修辞鲁棒性。然而，关于事实上真实的陈述的方向框架如何影响模型协议，LLM用户的常见情况，存在知识差距。AssertBench通过从FEVEROUS（一个事实验证数据集）中抽取证据支持的事实来解决这个问题。对于每个（证据支持的）事实，我们构建两个框架提示：一个是用户声称陈述事实上是正确的，另一个是用户声称它是不正确的。然后，我们记录模型的一致性和推理。期望的结果是模型断言自己，在两个框架中保持一致的真实评估，而不是切换其评估以与用户达成一致。AssertBench通过基于中立呈现时模型对相同主张的准确性对结果进行分层，将框架引起的变异性与模型的底层事实知识隔离开来。在这样做的过程中，这个基准测试的目的是衡量一个LLM的能力，“坚持自己的枪”时，提出了矛盾的用户断言相同的事实。完整的源代码可以在https://github.com/achowd32/assert-bench上找到。</p>
<p>2025-06-18 10:47:38 Wednesday ｜ OneEval: Benchmarking LLM Knowledge-intensive Reasoning over Diverse  Knowledge Bases
<strong>标题</strong> ： OneEval：对多元化知识库进行LLM知识密集型推理的基准
<strong>链接</strong> ：https://arxiv.org/abs/2506.12577</p>
<p>2025-06-18 10:56:02 Wednesday ｜ Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact  Verifiers
<strong>标题</strong> ： 验证验证者：揭露事实验证者中的陷阱和潜力
<strong>链接</strong> ：https://arxiv.org/abs/2506.13342</p>
<p>🌈 2025-06-18 10:57:30 Wednesday ｜ Hatevolution: What Static Benchmarks Don&rsquo;t Tell Us
<strong>标题</strong> ： 仇恨革命：静态基准没有告诉我们什么
<strong>链接</strong> ：https://arxiv.org/abs/2506.12148</p>
<h2 id="榜单">榜单</h2>
<p>2025-07-17 11:00:07 Thursday  ｜ <a href="https://livebench.ai/#/"target="_blank" rel="external nofollow noopener noreferrer">https://livebench.ai/#/</a></p>
<p>LiveBench：A Challenging, Contamination-Free LLM Benchmark ｜ LiveBench：一个具有挑战性、无污染的大语言模型基准测试</p>
<h2 id="公平性">公平性</h2>
<p>2025-07-02 17:15:15 Wednesday ｜</p>
<p><strong><a href="https://papers.cool/arxiv/2507.00460"target="_blank" rel="external nofollow noopener noreferrer">使用开放基准测试评估语言模型的陷阱</a></strong> <strong>[PDF(1)]</strong> <strong>[Copy]</strong> <strong>[Kimi()]</strong> <strong>[REL]</strong></p>
<p><strong>Authors</strong> : <a href="https://arxiv.org/search/?searchtype=author&amp;query=Md.%20Najib%20Hasan"target="_blank" rel="external nofollow noopener noreferrer">Md. Najib Hasan</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mohammad%20Fakhruddin%20Babar"target="_blank" rel="external nofollow noopener noreferrer">Mohammad Fakhruddin Babar</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Souvika%20Sarkar"target="_blank" rel="external nofollow noopener noreferrer">Souvika Sarkar</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Monowar%20Hasan"target="_blank" rel="external nofollow noopener noreferrer">Monowar Hasan</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Santu%20Karmaker"target="_blank" rel="external nofollow noopener noreferrer">Santu Karmaker</a></p>
<p>开放式大型语言模型 （LLM） 基准测试（如 HELM 和 BIG-bench）提供标准化、透明的协议，有助于语言模型 （LM） 的公平比较、可重复性和迭代进步。然而，他们的开放性也带来了关键的和未被充分探索的陷阱。本研究通过系统构建“作弊”模型来揭示这些弱点——BART、T5 和 GPT-2 的较小变体直接在公共测试集上进行微调——尽管泛化性差且实际实用性有限，但在突出的开放、整体基准 （HELM） 上获得最高排名。我们的研究结果强调了三个关键见解：\ca 在开放基准测试中的高排行榜表现可能并不总是反映现实世界的有效性;\cb 私人或动态基准必须与公开评估相辅相成，以维护诚信;以及 \cc 对当前基准实践进行根本性的重新评估对于确保稳健和值得信赖的 LM 评估至关重要。</p>
<p><strong>主题</strong> : <strong><a href="https://papers.cool/arxiv/cs.CL"target="_blank" rel="external nofollow noopener noreferrer">计算和语言</a></strong></p>
<p><strong>发布</strong> ： 2025-07-01 06：17：48 UTC</p>
<h2 id="评估可靠性">评估可靠性</h2>
<ol start="4">
<li>2025-07-01 11:57:21 Tuesday ｜</li>
</ol>
<p><strong><a href="https://papers.cool/arxiv/2506.23864"target="_blank" rel="external nofollow noopener noreferrer">垃圾输入，推理出来？为什么基准测试分数不可靠以及如何应对</a></strong> <strong>[PDF(2)]</strong> <strong>[Copy]</strong> <strong>[Kimi()]</strong> <strong>[REL]</strong></p>
<p><strong>Authors</strong> : <a href="https://arxiv.org/search/?searchtype=author&amp;query=Seyed%20Mahed%20Mousavi"target="_blank" rel="external nofollow noopener noreferrer">Seyed Mahed Mousavi</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Edoardo%20Cecchinato"target="_blank" rel="external nofollow noopener noreferrer">Edoardo Cecchinato</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lucia%20Hornikova"target="_blank" rel="external nofollow noopener noreferrer">Lucia Hornikova</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Giuseppe%20Riccardi"target="_blank" rel="external nofollow noopener noreferrer">Giuseppe Riccardi</a></p>
<p>我们对三个广泛使用的推理基准 SocialIQa、FauxPas-EAI 和 ToMi 进行了系统审计，并发现了基准项目和评估方法中普遍存在的缺陷。使用五个 LLM（GPT-{3、3.5、4、o1} 和 LLaMA 3.1）作为诊断工具，我们确定了基准设计中的结构、语义和语用问题（例如，重复的项目、模棱两可的措辞和难以置信的答案），以及优先考虑输出形式而不是推理过程的评分程序。通过对清理后的基准子集进行系统的人工注释和重新评估，我们发现模型分数的提高通常不是由于不稳定的表面措辞变化，也不是由于推理的改进。事实上，进一步的分析表明，模型性能对微小的输入变化（如上下文可用性和措辞）高度敏感，这表明高分可能反映了与特定格式线索的一致性，而不是基于输入的一致推理。这些发现挑战了当前基于 LLM 中推理的基于基准的声明的有效性，并强调了评估协议的必要性，这些协议将推理评估为从可用信息中提取推理的过程，而不是静态输出选择。我们发布了经过审计的数据和评估工具，以支持对模型推理进行更具可解释性和诊断性的评估。</p>
<ol start="2">
<li>2025-06-30 19:03:52 Monday ｜</li>
</ol>
<p><strong><a href="https://papers.cool/arxiv/2506.22316"target="_blank" rel="external nofollow noopener noreferrer">评估 LLM-as-a-Judge 中的评分偏差</a></strong> <strong>[PDF(4)]</strong> <strong>[Copy]</strong> <strong>[Kimi(2)]</strong> <strong>[REL]</strong></p>
<p><strong>Authors</strong> : <a href="https://arxiv.org/search/?searchtype=author&amp;query=Qingquan%20Li"target="_blank" rel="external nofollow noopener noreferrer">Qingquan Li</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shaoyu%20Dou"target="_blank" rel="external nofollow noopener noreferrer">Shaoyu Dou</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kailai%20Shao"target="_blank" rel="external nofollow noopener noreferrer">Kailai Shao</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chao%20Chen"target="_blank" rel="external nofollow noopener noreferrer">Chao Chen</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Haixiang%20Hu"target="_blank" rel="external nofollow noopener noreferrer">Haixiang Hu</a></p>
<p>大型语言模型 （LLM） 的卓越表现催生了“LLM-as-a-Judge”，其中 LLM 被用作复杂任务的评估者。此外，它已被广泛用于自然语言处理 （NLP）、偏好学习和各种特定领域等领域。然而，LLM as-a-Judge 中存在各种偏见，这对判决的公平性和可靠性产生了不利影响。目前关于评估或减轻 LLM-as-a-Judge 中偏见的研究主要集中在基于比较的评估上，而对基于评分的评估中偏见的系统调查仍然有限。因此，我们在 LLM-as-a-Judge 中将评分偏倚定义为当评分法官模型与偏倚相关的扰动时分数不同，并提供了一个精心设计的框架来全面评估评分偏倚。我们通过数据合成来增强现有的 LLM as-a-Judge 基准，以构建我们的评估数据集并设计多方面的评估指标。我们的实验结果表明，现有 Judge 模型的评分稳定性受到评分偏倚的破坏。进一步的探索性实验和讨论为评分提示模板的设计以及减轻评分量规、分数 ID 和参考答案选择等方面的评分偏差提供了有价值的见解。</p>
<p><strong>主题</strong> : <strong><a href="https://papers.cool/arxiv/cs.CL"target="_blank" rel="external nofollow noopener noreferrer">计算和语言</a></strong></p>
<ol start="3">
<li>2025-06-18 10:41:35 Wednesday ｜ An Empirical Study of LLM-as-a-Judge: How Design Choices Impact  Evaluation Reliability
<strong>标题</strong> ： 法学硕士担任评委的实证研究：设计选择如何影响评估可靠性
<strong>链接</strong> ：https://arxiv.org/abs/2506.13639</li>
</ol>
<p><strong>作者</strong> ： Yusuke Yamauchi,  Taro Yano,  Masafumi Oyamada
<strong>摘要</strong> ：随着大型语言模型（LLM）的不断发展，可靠的评估方法对于开放式的，遵循规则的任务至关重要。LLM-as-a-Judge允许使用LLM作为评估器进行自动评估，但其可靠性仍然不确定。在这项工作中，我们分析了影响其可信度的关键因素，重点是与人类的判断和评价一致性。使用BIGGENBench和EvalBiasBench，我们研究了评估设计、解码策略和尝试链（CoT）推理在评估中的影响。我们的研究结果表明，<strong>评估标准</strong>是至关重要的可靠性，非确定性采样提高了与人类的偏好确定性评估对齐，CoT推理提供了最小的收益时，明确的评估标准。</p>
<h2 id="证明">证明</h2>
<ol start="5">
<li>Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities</li>
</ol>
<p>标题： Inequ-Comp：在不等式自动定理证明中对人类直觉组合推理进行基准测试</p>
<p>链接：https://arxiv.org/abs/2505.12680</p>
<ol start="2">
<li>2025-06-18 09:53:18 Wednesday ｜ 形式化证明迈向多模态，MLLM正确率仅4%！港科大等推出全新基准 <a href="https://mp.weixin.qq.com/s/aGRgRkq3kz_ZKrlSnI95Yg"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/aGRgRkq3kz_ZKrlSnI95Yg</a></li>
</ol>
<p>MATP-BENCH: Can MLLM Be a Good Automated Theorem Prover for Multimodal Problems?</p>
<p>论文地址：https://arxiv.org/pdf/2506.06034</p>
<p>项目主页：https://matpbench.github.io/</p>
<p>MATP-BENCH是一个新推出的多模态自动定理证明基准，旨在评估多模态大模型（MLLMs）在处理包含图像和文本的几何定理证明中的能力。实验表明，尽管模型在将图文信息转化为形式化定理方面有一定能力，但在构建完整证明时面临重大挑战，尤其是复杂逻辑推理和辅助线构造方面。</p>
<p>近年来，自动定理证明（ATP）取得了显著进展，但大部分工作都集中在处理纯文本形式的定理。</p>
<p>MATP-BENCH任务与传统ATP任务的对比。传统ATP仅依赖文本化的定理陈述，而MATP-BENCH要求模型必须结合图像和自然语言，并从中提取文本中未明确说明的关键前提（如图中「From diagram」部分所示），才能构建完整的形式化定理 。</p>
<p>三种主流的形式化证明语言：Lean 4、Coq和Isabelle。</p>
<p>为了精准评估模型在不同阶段的能力，MATP-BENCH 设置了两个关联的核心任务：（1）多模态自动定理证明 (Multimodal Automated Theorem Proving)：模拟人类专家的端到端形式化定理及证明过程；（2）多模态定理形式化 (Multimodal Theorem Formalization)：单独评估模型理解和翻译多模态信息为形式化定理的能力。</p>
<h2 id="数学">数学</h2>
<ol start="6">
<li>2025-06-13 18:25:44 Friday｜</li>
</ol>
<p>MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for  Text-to-Image Reasoning
<strong>标题</strong> ： MMMG：用于文本到图像推理的大规模、多学科、多层生成基准
<strong>链接</strong> ：https://arxiv.org/abs/2506.10963</p>
<p><strong>作者</strong> ： Yuxuan Luo,  Yuhui Yuan,  Junwen Chen,  Haonan Cai,  Ziyi Yue,  Yuwei Yang,  Fatima Zohra Daha,  Ji Li,  Zhouhui Lian
<strong>摘要</strong> ：本文将知识图像生成作为一项新任务引入，并结合大规模多学科多层知识图像生成基准（MMMG）来探讨图像生成模型的推理能力。知识图像一直是人类文明和人类学习机制的核心-双重编码理论和图片优势效应强调了这一事实。生成这样的图像具有挑战性，需要多模态推理，将世界知识与像素级基础融合到清晰的解释性视觉效果中。为了实现全面的评估，MMMG提供了4，456个专家验证的（知识）图像提示对，涵盖10个学科，6个教育级别和多种知识格式，如图表，图表和思维导图。为了消除评估过程中的混淆复杂性，我们采用了统一的知识图（KG）表示。每个KG明确地描绘了目标图像的核心实体及其依赖关系。我们进一步引入MMMG-Score来评估生成的知识图像。该指标结合了事实保真度，通过KG之间的图形编辑距离测量，与视觉清晰度评估。对16个最先进的文本到图像生成模型的综合评估暴露出严重的推理缺陷-低实体保真度，弱关系和混乱-GPT-4 o的MMMG得分仅为50.20，强调了基准测试的难度。为了推动进一步的进展，我们发布了FLUX-Reason（MMMG得分为34.45），这是一个有效的开放基线，将推理LLM与扩散模型相结合，并在16，000个策划的知识图像提示对上进行训练。</p>
<h2 id="推理">推理</h2>
<ol start="7">
<li>2025-07-01 12:37:06 Tuesday ｜</li>
</ol>
<p><strong><a href="https://papers.cool/arxiv/2506.23563"target="_blank" rel="external nofollow noopener noreferrer">MMReason：面向 AGI 的 MLLM 的开放式多模态多步骤推理基准</a></strong> <strong>[PDF()]</strong> <strong>[Copy]</strong> <strong>[Kimi(1)]</strong> <strong>[REL]</strong></p>
<p><strong>Authors</strong> : <a href="https://arxiv.org/search/?searchtype=author&amp;query=Huanjin%20Yao"target="_blank" rel="external nofollow noopener noreferrer">Huanjin Yao</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jiaxing%20Huang"target="_blank" rel="external nofollow noopener noreferrer">Jiaxing Huang</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yawen%20Qiu"target="_blank" rel="external nofollow noopener noreferrer">Yawen Qiu</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Michael%20K.%20Chen"target="_blank" rel="external nofollow noopener noreferrer">Michael K. Chen</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wenzheng%20Liu"target="_blank" rel="external nofollow noopener noreferrer">Wenzheng Liu</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wei%20Zhang"target="_blank" rel="external nofollow noopener noreferrer">Wei Zhang</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wenjie%20Zeng"target="_blank" rel="external nofollow noopener noreferrer">Wenjie Zeng</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xikun%20Zhang"target="_blank" rel="external nofollow noopener noreferrer">Xikun Zhang</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jingyi%20Zhang"target="_blank" rel="external nofollow noopener noreferrer">Jingyi Zhang</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuxin%20Song"target="_blank" rel="external nofollow noopener noreferrer">Yuxin Song</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wenhao%20Wu"target="_blank" rel="external nofollow noopener noreferrer">Wenhao Wu</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dacheng%20Tao"target="_blank" rel="external nofollow noopener noreferrer">Dacheng Tao</a></p>
<p>推理在推动多模态大型语言模型 （MLLM） 向通用人工智能发展方面发挥着至关重要的作用。然而，现有的 MLLM 基准往往无法从三个关键方面精确、全面地评估长链推理能力：（1） 缺乏难度和多样性，（2） 易猜性和记忆力，（3） 对中间推理步骤的评估不足。为了填补这一空白，我们引入了 MMReason，这是一个新的基准测试，旨在通过多样化、开放式、具有挑战性的问题精确、全面地评估 MLLM 长链推理能力。首先，我们策划了具有挑战性的问题，需要从各个领域（即 6 个学科）和多个难度级别（即从大学预科到大学，从基础到竞赛级别）进行多步骤推理。其次，这些问题被重新表述为开放式格式，并使用多模型投票技术进行过滤，以消除与猜测和记忆相关的捷径情况，确保稳健的推理评估。第三，我们用详细的分步解决方案对问题进行注释，并设计一个基于参考的三元评分机制来可靠地评估中间推理步骤。通过 MMReason，我们对流行的领先 MLLM 进行基准测试，并对其推理能力进行深入分析。我们希望 MMReason 将成为推进 MLLM 推理研究的宝贵资源。代码将在 <a href="https://github.com/HJYao00/MMReason"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/HJYao00/MMReason</a> 上提供。</p>
<p><strong>科目</strong> :  <strong><a href="https://papers.cool/arxiv/cs.AI"target="_blank" rel="external nofollow noopener noreferrer">人工智能</a></strong> , <a href="https://papers.cool/arxiv/cs.CL"target="_blank" rel="external nofollow noopener noreferrer">计算和语言</a>, <a href="https://papers.cool/arxiv/cs.CV"target="_blank" rel="external nofollow noopener noreferrer">计算机视觉和模式识别</a></p>
<ol start="2">
<li>
<p>2025-06-24 12:54:11 Tuesday ｜</p>
<p>CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models
<strong>链接</strong> ：https://arxiv.org/abs/2506.17180</p>
</li>
</ol>
<p><strong>作者</strong> ：iu, Richard Baraniuk, Shashank Sonkar
<strong>摘要</strong> ：我们介绍了CLEAR-3 K，这是一个包含3,000个断言推理问题的数据集，旨在评估语言模型是否可以确定一个语句是否因果地解释另一个语句。每个问题都提出了一个断言-理由对，并挑战语言模型来区分语义相关性和真正的因果解释关系。通过对21个最先进的语言模型（参数范围从0.5B到72 B）的综合评估，我们发现了两个基本的发现。首先，语言模型经常混淆语义相似性与因果关系，依赖于词汇和语义重叠，而不是推断实际的因果解释关系。其次，随着参数大小的增加，模型往往会从过度怀疑因果关系转变为过度宽容地接受它们。尽管发生了这种变化，但即使是性能最好的模型，通过Matthews相关系数测量的性能也只有0.55。因此，CLEAR-3 K为开发和评估语言模型中真正的因果推理提供了一个至关重要的基准，这对于需要准确评估因果关系的应用程序来说是一个必不可少的能力。</p>
<h4 id="逻辑-gpt-5grok-4o3-pro都零分史上最难ai评测基准换它了">逻辑 ｜<a href="https://mp.weixin.qq.com/s/cyOJ_Id606REj97nCXYqhg"target="_blank" rel="external nofollow noopener noreferrer">GPT-5、Grok 4、o3 Pro都零分，史上最难AI评测基准换它了</a></h4>
<p>2025-08-15</p>
<ol>
<li>AAI，一个专注于超智能和高级 AI 系统研究的机构，近期提出的一个新基准 FormulaOne，让一众大模型集体得零分，包括 GPT-5、o3 Pro、Gemini 2.5 Pro、Grok 4 等前沿模型。</li>
<li>HuggingFace：https://huggingface.co/spaces/double-ai/FormulaOne-Leaderboard</li>
<li>FormulaOne 包含 220 个新颖的图结构动态规划问题，按难度分为三类，从中等难度直至科研级别。其中最高等级难度的题包括拓扑与几何、组合问题分析等。</li>
<li>这一大类问题的可解性由 Courcelle 提出的一个算法元定理所保证</li>
<li>问题陈述看似简单，但这背后实则掩盖了发现正确动态规划解法的非凡难度。这个过程遍布着微妙的组合与逻辑陷阱，要求（研究者）对问题的底层结构有深刻的理解。关于解决一个名为 Maximal-Cluster-Graph 的难题所需的十五个相互依赖的推理步骤，其详细的推演过程请参阅论文的附录。</li>
<li>论文地址：https://arxiv.org/pdf/2507.13337</li>
<li>AAI 的核心目标是推动「人工专家智能」（Artificial Expert Intelligence，AEI）的理论与应用，提出区别于传统窄域 AI 和 AGI 的新 AI 发展路径。这种 AEI 强调将领域知识与严密的科学推理能力相结合，旨在突破「只擅长特定任务」或「泛化无精度」的传统瓶颈，使 AI 可以像顶级人类专家一样，运用严谨推理来解决复杂科学或工程难题。</li>
</ol>
<h2 id="自动化评估">自动化评估</h2>
<ol start="8">
<li>2025-06-13 18:27:09 Friday｜</li>
</ol>
<p>Beyond Gold Standards: Epistemic Ensemble of LLM Judges for Formal  Mathematical Reasoning
<strong>标题</strong> ： 超越黄金标准：LLM形式数学推理法官的认识融合
<strong>链接</strong> ：https://arxiv.org/abs/2506.10903</p>
<p><strong>作者</strong> ： Lan Zhang,  Marco Valentino,  Andre Freitas
<strong>摘要</strong> ：自动形式化在形式化数学推理中起着至关重要的作用，它可以将自然语言语句自动翻译成形式语言。虽然使用大型语言模型（LLM）的最新进展已经显示出有希望的结果，但自动评估自动形式化的方法仍然没有得到充分的探索。当一个人移动到更复杂的领域（例如，高级数学），人类评估需要大量的时间和领域专业知识，特别是当基础陈述和背景知识的复杂性增加时。法学硕士作为一个法官提出了一个很有前途的方法自动化这样的评价。然而，现有的方法通常采用粗粒度和通用的评估标准，这限制了它们对高级形式数学推理的有效性，其中质量取决于细微的，多粒度的维度。在这项工作中，我们采取了一个步骤，通过引入一个系统的，自动的方法来评估自动化任务，以解决这个差距。所提出的方法是基于认识和正式接地合奏（EFG）的LLM法官，定义的标准，包括<strong>逻辑保存（LP），数学一致性（MC），形式有效性（FV），和形式质量（FQ）</strong>，导致一个透明的评估，占不同的贡献因素。我们验证所提出的框架，作为一个代理的自动形式化评估领域内的形式数学。总的来说，我们的实验表明，LLM法官的EFG合奏是一个合适的新兴代理评估，更强烈地与人类的评估比粗粒度的模型，特别是在评估正式的质量。这些研究结果表明，法学硕士作为法官，特别是在一组定义良好的原子属性的指导下，可以提供一个可扩展的，可解释的，可靠的支持，评估正式的数学推理。
2. 2025-06-13 18:48:05 Friday ｜</p>
<p>A Survey of Automatic Evaluation Methods on Text, Visual and Speech  Generations
<strong>标题</strong> ： 文本、视觉和语音生成自动评估方法综述
<strong>链接</strong> ：https://arxiv.org/abs/2506.10019</p>
<p><strong>作者</strong> ： Tian Lan,  Yang-Hao Zhou,  Zi-Ao Ma,  Fanshu Sun,  Rui-Qing Sun,  Junyu Luo,  Rong-Cheng Tu,  Heyan Huang,  Chen Xu,  Zhijing Wu,  Xian-Ling Mao
<strong>摘要</strong> ：深度学习的最新进展显着增强了文本、图像和音频的生成AI功能。然而，自动评估这些生成的输出的质量带来了持续的挑战。虽然存在许多自动评估方法，但目前的研究缺乏一个系统的框架，该框架将这些方法全面组织在文本，视觉和音频模式中。为了解决这个问题，我们提出了一个全面的审查和统一的分类自动生成的内容在所有三种模式的评价方法，我们确定了五个基本的范式，现有的评价方法在这些领域的特点。我们的分析首先检查评估方法的文本生成，其中技术是最成熟的。然后，我们将此框架扩展到图像和音频生成，展示了其广泛的适用性。最后，我们讨论了跨模态评估方法的未来研究方向。</p>
<h2 id="代码">代码</h2>
<ol start="9">
<li>2025-06-18 10:46:54 Wednesday ｜Humanity&rsquo;s Last Code Exam: Can Advanced LLMs Conquer Human&rsquo;s Hardest  Code Competition?
<strong>标题</strong> ： 人类最后的代码考试：高级法学硕士能否战胜人类最艰难的代码竞争？
<strong>链接</strong> ：https://arxiv.org/abs/2506.12713</li>
</ol>
<p><strong>作者</strong> ： Xiangyang Li,  Xiaopeng Li,  Kuicai Dong,  Quanhu Zhang,  Rongju Ruan,  Xinyi Dai,  Xiaoshuang Liu,  Shengchun Xu,  Yasheng Wang,  Ruiming Tang
<strong>摘要</strong> ：代码生成是大型语言模型（LLM）的核心能力，但主流基准测试（例如，APP和LiveCodeBench）包含中等难度的问题，对高级LLM没有挑战。为了更好地反映高级推理和代码生成能力，我们引入了人类最后一次代码考试（HLCE），包括2010 - 2024年国际大学生编程竞赛（ICPC世界总决赛）和国际信息学奥林匹克竞赛（IOI）的235个最具挑战性的问题。作为HLCE的一部分，我们设计了一个协调的在线-离线沙箱，以保证完全可重复的评估。通过我们的综合评估，我们观察到即使是最强的推理LLM：o 4-mini（高）和Gemini-2.5 Pro，也只能分别达到15.9%和11.4%的通过率。同时，我们提出了一种新的“自我识别”任务来衡量LLM对自己能力的意识。结果表明，LLM的自我识别能力与其代码生成性能不成比例相关。最后，我们对测试时间缩放律的经验验证表明，当前先进的LLM在复杂的编程任务上有很大的改进空间。我们希望HLCE成为代码生成的里程碑式挑战，并促进高性能推理和人工智能协作编程的进步。我们的代码和数据集也是公开的（https：//github.com/Humanity-s-Last-Code-Exam/HLCE）。</p>
<ol start="2">
<li>2025-06-19 19:56:13 Thursday ｜ 谢赛宁团队新基准让LLM集体自闭，DeepSeek R1、Gemini 2.5 Pro都是零分</li>
</ol>
<p><a href="https://mp.weixin.qq.com/s/TAhNaPBl8bDE_IeWpHDrWg"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/TAhNaPBl8bDE_IeWpHDrWg</a></p>
<p>LLMs 真的具备与顶级人类选手相当的推理能力吗？模型的高分究竟有多少来自真实的推理能力，又有多少是依赖外部工具的结果？</p>
<p>为了解答上述问题，来自纽约大学、普林斯顿大学等 8 家机构的研究者提出了 LiveCodeBench Pro，这是一个极具挑战性的竞技编程基准测试。</p>
<p>LiveCodeBench Pro 收录了 584 道截至 2025 年 4 月 25 日的高质量题目，这些题目均来自 Codeforces 、ICPC 系列赛和 IOI 系列赛等顶级赛事。并且这些问题会不断更新以降低可能的数据污染。</p>
<p>论文标题：LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?</p>
<p>论文地址：https://arxiv.org/pdf/2506.11928</p>
<p>项目主页：https://livecodebenchpro.com/</p>
<p>GitHub：https://github.com/GavinZhengOI/LiveCodeBench-Pro</p>
<p>文中展示了 6 个模型在各类编程问题中的表现。研究发现，人类在不同问题标签上的表现更为一致，而模型的评分则因标签不同而显示出更大的差异。主要发现总结如下：</p>
<p><strong>知识密集型问题是大语言模型的舒适区。</strong> 带有如线段树、图论、树和数据结构等标签的问题，在大多数模型上都表现出很高的性能。这些问题通常可以通过拼接众所周知的模板（例如，树状数组、迪杰斯特拉算法、欧拉路径）来解决。这正是大语言模型的优势所在，因为所需的模式会以字面形式出现在其训练数据中，并且生成语法正确的模板对于大语言模型来说比对人类容易得多。</p>
<p><strong>逻辑密集型问题也取得了同样好的结果。</strong> 大语言模型在逻辑密集型类别中也表现出色，例如组合数学、数学、动态规划和二分搜索。这些类别需要更有模式的思维方式（例如，在组合数学中应用组合恒等式，在动态规划中构建状态空间并推导转移函数），并且可以从记忆化的脚手架代码中受益。</p>
<p><strong>在观察密集型问题上表现不佳。</strong> 对于博弈论、特定问题特定分析（ad-hoc）、贪心算法和构造性问题，大多数模型的评分骤降至 1500 以下，明显低于其在知识密集型和逻辑密集型类别中的表现。解决这些问题通常取决于发现新颖的见解，而这是无法仅靠记忆化的代码片段来获得的。</p>
<p><strong>大语言模型在分类讨论上遇到困难。</strong> 有趣的是，所有模型都在分类讨论上表现不佳。除了 o4-mini-high 之外，每个模型的评分都低于 1500 分，即便是 o4-mini-high，其表现在此类别中也远逊于其他问题类别。人工检查显示，无法识别和处理边界情况是所有模型的一个突出失败模式。</p>
<p>**交互式问题暴露了模型的显著弱点。 **在交互式问题上，o4-mini-high 的评分骤降至 1500 左右，其他模型也表现挣扎。论文附录中讨论了这种糟糕表现背后的可能原因，并指出了 o3-mini-high 在解决交互式问题时出现的异常行为。</p>
<ol start="3">
<li>2025-06-24 13:00:08 Tuesday ｜</li>
</ol>
<p>OJBench: A Competition Level Code Benchmark For Large Language Models
<strong>链接</strong> ：https://arxiv.org/abs/2506.16395</p>
<p><strong>作者</strong> ：g, Yiping Liu, Yejie Wang, Wenyang He, Bofei Gao, Muxi Diao, Yanxu Chen, Kelin Fu, Flood Sung, Zhilin Yang, Tianyu Liu, Weiran Xu
<strong>备注</strong> ：         9 pages, 5 figures
<strong>摘要</strong> ：大型语言模型（LLM）的最新进展表明，数学和代码推理能力取得了显着进步。然而，现有的代码基准测试在评估这些功能的全方位能力方面受到限制，特别是在竞争层面。为了弥合这一差距，我们引入了OJBench，这是一个新颖且具有挑战性的基准测试，旨在评估LLM的竞争级代码推理能力。OJBench包含来自NOI和ICPC的232个编程竞赛问题，为模型的推理能力提供了更严格的测试。我们使用OJBench对37个模型进行了综合评估，包括闭源和开源模型，面向推理和非面向推理的模型。我们的研究结果表明，即使是最先进的面向推理的模型，如o 4-mini和Gemini-2.5-pro-exp，也难以解决极具挑战性的竞争级问题。这突出了模型在竞争级代码推理中面临的重大挑战。</p>
<h2 id="科学">科学</h2>
<ol start="10">
<li>2025-07-02 15:35:33 Wednesday ｜</li>
</ol>
<p><strong><a href="https://papers.cool/arxiv/2507.01001"target="_blank" rel="external nofollow noopener noreferrer">SciArena：科学文献任务中基础模型的开放评估平台</a></strong> <strong>[PDF(6)]</strong> <strong>[Copy]</strong> <strong>[Kimi(2)]</strong> <strong>[REL]</strong></p>
<p><strong>Authors</strong> : <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yilun%20Zhao"target="_blank" rel="external nofollow noopener noreferrer">Yilun Zhao</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kaiyan%20Zhang"target="_blank" rel="external nofollow noopener noreferrer">Kaiyan Zhang</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tiansheng%20Hu"target="_blank" rel="external nofollow noopener noreferrer">Tiansheng Hu</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Sihong%20Wu"target="_blank" rel="external nofollow noopener noreferrer">Sihong Wu</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ronan%20Le%20Bras"target="_blank" rel="external nofollow noopener noreferrer">Ronan Le Bras</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Taira%20Anderson"target="_blank" rel="external nofollow noopener noreferrer">Taira Anderson</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jonathan%20Bragg"target="_blank" rel="external nofollow noopener noreferrer">Jonathan Bragg</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Joseph%20Chee%20Chang"target="_blank" rel="external nofollow noopener noreferrer">Joseph Chee Chang</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jesse%20Dodge"target="_blank" rel="external nofollow noopener noreferrer">Jesse Dodge</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Matt%20Latzke"target="_blank" rel="external nofollow noopener noreferrer">Matt Latzke</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yixin%20Liu"target="_blank" rel="external nofollow noopener noreferrer">Yixin Liu</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Charles%20McGrady"target="_blank" rel="external nofollow noopener noreferrer">Charles McGrady</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiangru%20Tang"target="_blank" rel="external nofollow noopener noreferrer">Xiangru Tang</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zihang%20Wang"target="_blank" rel="external nofollow noopener noreferrer">Zihang Wang</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chen%20Zhao"target="_blank" rel="external nofollow noopener noreferrer">Chen Zhao</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hannaneh%20Hajishirzi"target="_blank" rel="external nofollow noopener noreferrer">Hannaneh Hajishirzi</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Doug%20Downey"target="_blank" rel="external nofollow noopener noreferrer">Doug Downey</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Arman%20Cohan"target="_blank" rel="external nofollow noopener noreferrer">Arman Cohan</a></p>
<p>我们介绍了 SciArena，这是一个开放的协作平台，用于评估科学文献任务的基础模型。与科学文献理解和综合的传统基准不同，SciArena 遵循社区对模型比较进行投票的 Chatbot Arena 评估方法， <strong>直接与研究社区互动</strong> 。通过利用集体智慧，SciArena 提供了社区驱动的模型在 <strong>开放式科学任务中的性能评估</strong> ，这些任务需要基于文献的长篇回答。该平台目前支持 23 个开源和专有的基础模型，并已从不同科学领域的受信任研究人员那里收集了超过 13,000 张选票。我们分析了迄今为止收集的数据，并确认提交的问题是多样化的，符合现实世界的文献需求，并且参与的研究人员在他们的评估中表现出强烈的自洽性和注释者之间的一致性。我们根据模型排名排行榜讨论结果和见解。为了进一步促进为文献任务构建基于模型的自动评价系统的研究，我们发布了 ** SciArena-Eval** ，这是一个基于我们收集的偏好数据的元评价基准。该基准测试通过将模型的成对评估与人工投票进行比较来衡量模型判断答案质量的准确性。我们的实验突出了基准测试的挑战，并强调了对更可靠的自动评估方法的需求。</p>
<p><strong>科目</strong> :  <strong><a href="https://papers.cool/arxiv/cs.CL"target="_blank" rel="external nofollow noopener noreferrer">计算和语言</a></strong> , <a href="https://papers.cool/arxiv/cs.AI"target="_blank" rel="external nofollow noopener noreferrer">人工智能</a></p>
<ol start="2">
<li>2025-06-30 20:00:23 Monday ｜</li>
</ol>
<p><strong><a href="https://papers.cool/arxiv/2506.22419"target="_blank" rel="external nofollow noopener noreferrer">自动化 LLM 速通基准测试：重现 NanoGPT 改进</a></strong> <strong>[PDF(14)]</strong> <strong>[Copy]</strong> <strong>[Kimi(16)]</strong> <strong>[REL]</strong></p>
<p><strong>Authors</strong> : <a href="https://arxiv.org/search/?searchtype=author&amp;query=Bingchen%20Zhao"target="_blank" rel="external nofollow noopener noreferrer">Bingchen Zhao</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Despoina%20Magka"target="_blank" rel="external nofollow noopener noreferrer">Despoina Magka</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Minqi%20Jiang"target="_blank" rel="external nofollow noopener noreferrer">Minqi Jiang</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xian%20Li"target="_blank" rel="external nofollow noopener noreferrer">Xian Li</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Roberta%20Raileanu"target="_blank" rel="external nofollow noopener noreferrer">Roberta Raileanu</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tatiana%20Shavrina"target="_blank" rel="external nofollow noopener noreferrer">Tatiana Shavrina</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jean-Christophe%20Gagnon-Audet"target="_blank" rel="external nofollow noopener noreferrer">Jean-Christophe Gagnon-Audet</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kelvin%20Niu"target="_blank" rel="external nofollow noopener noreferrer">Kelvin Niu</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shagun%20Sodhani"target="_blank" rel="external nofollow noopener noreferrer">Shagun Sodhani</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Michael%20Shvartsman"target="_blank" rel="external nofollow noopener noreferrer">Michael Shvartsman</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Andrei%20Lupu"target="_blank" rel="external nofollow noopener noreferrer">Andrei Lupu</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Alisia%20Lupidi"target="_blank" rel="external nofollow noopener noreferrer">Alisia Lupidi</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Edan%20Toledo"target="_blank" rel="external nofollow noopener noreferrer">Edan Toledo</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Karen%20Hambardzumyan"target="_blank" rel="external nofollow noopener noreferrer">Karen Hambardzumyan</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Martin%20Josifoski"target="_blank" rel="external nofollow noopener noreferrer">Martin Josifoski</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Thomas%20Foster"target="_blank" rel="external nofollow noopener noreferrer">Thomas Foster</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lucia%20Cipolina-Kun"target="_blank" rel="external nofollow noopener noreferrer">Lucia Cipolina-Kun</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Abhishek%20Charnalia"target="_blank" rel="external nofollow noopener noreferrer">Abhishek Charnalia</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Derek%20Dunfield"target="_blank" rel="external nofollow noopener noreferrer">Derek Dunfield</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Alexander%20H.%20Miller"target="_blank" rel="external nofollow noopener noreferrer">Alexander H. Miller</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Oisin%20Mac%20Aodha"target="_blank" rel="external nofollow noopener noreferrer">Oisin Mac Aodha</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jakob%20Foerster"target="_blank" rel="external nofollow noopener noreferrer">Jakob Foerster</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yoram%20Bachrach"target="_blank" rel="external nofollow noopener noreferrer">Yoram Bachrach</a></p>
<p>大型语言模型 （LLM） 的快速发展有可能帮助科学进步。这项工作的一个关键能力是复制现有作品的能力。为了评估 AI 代理在活跃的研究领域中重现结果的能力，我们引入了自动化 LLM 速通基准测试，利用研究社区对 NanoGPT 速通的贡献，这是一项在最短的时间内训练 GPT-2 模型的竞赛。19 个速通任务中的每一个都为代理提供了之前的记录训练脚本，可以选择与三种提示格式中的一种配对，范围从伪代码到新记录改进的类似纸张的描述。记录通过设计快速执行，而速运行改进涵盖各种代码级更改，从高级算法改进到硬件感知优化。这些特性使基准对于改进 LLM 训练的前沿问题既可访问又现实。我们发现，最近的推理 LLM 与 SoTA 支架相结合，即使给出了详细的提示，也很难在我们的基准测试中重新实现已知的创新。因此，我们的基准提供了一个简单、非饱和的 LLM 自动化科学复制能力的衡量标准，这是自主研究代理的必要（但不是充分）技能。</p>
<ol start="3">
<li>2025-06-27 14:26:54 Friday</li>
</ol>
<p><strong><a href="https://papers.cool/arxiv/2506.20803"target="_blank" rel="external nofollow noopener noreferrer">构思与执行差距：LLM 生成的与人类研究理念的执行结果</a></strong> <strong>[PDF(1)]</strong> <strong>[Copy]</strong> <strong>[Kimi()]</strong> <strong>[REL]</strong></p>
<p>作者：陈乐磊、桥本达规、杨迪一</p>
<p>大型语言模型 （LLM） 在加速科学研究管道方面显示出前景。这个过程的一个关键能力是产生新颖研究想法的能力，以前的研究发现，LLM 产生的研究想法被认为比人类专家的想法更新颖。然而，一个好主意不应该简单地看起来新颖，它还应该在执行后导致更好的研究。为了测试 AI 生成的想法是否会带来更好的研究结果，我们通过招募 43 名专家研究人员来执行随机分配的想法，这些想法要么由专家撰写，要么由 LLM 生成。每位专家都花了 100 多个小时来实施这个想法，并写了一篇 4 页的短文来记录实验。然后，所有已执行的项目都由专业的 NLP 研究人员盲目审查。比较执行前后相同想法的审查分数，LLM 生成的想法在所有评估指标（新颖性、兴奋性、有效性和整体性;p &lt; 0.05）上的下降幅度明显大于专家编写的想法，缩小了 LLM 与在构思阶段观察到的人类想法之间的差距。在比较执行研究的汇总评论分数时，我们甚至观察到，对于许多指标，人类想法得分高于 LLM 想法的排名发生了翻转。这种构思-执行差距凸显了当前 LLM 在产生真正有效的研究思想方面的局限性，以及在没有执行结果的情况下评估研究思想的挑战。</p>
<ol start="4">
<li>2025-06-27 11:41:48 Friday ｜ 首个面向科学任务、真实交互、自动评估的多模态智能体评测环境，ScienceBoard来了 <a href="https://mp.weixin.qq.com/s/naVskQ9btJFkoUyyQVr7zA"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/naVskQ9btJFkoUyyQVr7zA</a></li>
</ol>
<p>电脑智能体（Computer-Using Agents，也称 CUA）的出现，这一角色正在发生根本性转变。相比于传统的语言模型助手，这类智能体能够像人类一样操作计算机。</p>
<p>社区已提出多项 CUA 智能体评测（如 WebArena、OSWorld 等），但这些工作大多集中在日常场景和通用软件上</p>
<p>在这样的背景下，我们提出了 ScienceBoard：首个面向科学任务、真实交互、自动评估的多模态智能体评测环境，目标是从根本上推动 “会自动完成科学工作流的 AI” 的研究进展。</p>
<ul>
<li>论文题目：ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows</li>
<li>项目地址：https://qiushisun.github.io/ScienceBoard-Home/</li>
<li>研究机构：香港大学，上海人工智能实验室，复旦大学，北京大学，耶鲁大学</li>
</ul>
<p><img loading="lazy" src="https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=MGFkYTViNGNkYzAxM2JhNmJlMWZhODNiNzI3Nzc4N2RfMFRZVnpWSUpvU0NNRWo1bVZaa0lHSldYUkNrNjQwMVFfVG9rZW46WDJqcWJkelFIb0pTeDl4Z3FFMGNBSk1RbndoXzE3NTUyNjAzOTY6MTc1NTI2Mzk5Nl9WNA" srcset="https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=MGFkYTViNGNkYzAxM2JhNmJlMWZhODNiNzI3Nzc4N2RfMFRZVnpWSUpvU0NNRWo1bVZaa0lHSldYUkNrNjQwMVFfVG9rZW46WDJqcWJkelFIb0pTeDl4Z3FFMGNBSk1RbndoXzE3NTUyNjAzOTY6MTc1NTI2Mzk5Nl9WNA&amp;size=small, https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=MGFkYTViNGNkYzAxM2JhNmJlMWZhODNiNzI3Nzc4N2RfMFRZVnpWSUpvU0NNRWo1bVZaa0lHSldYUkNrNjQwMVFfVG9rZW46WDJqcWJkelFIb0pTeDl4Z3FFMGNBSk1RbndoXzE3NTUyNjAzOTY6MTc1NTI2Mzk5Nl9WNA&amp;size=medium 1.5x, https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=MGFkYTViNGNkYzAxM2JhNmJlMWZhODNiNzI3Nzc4N2RfMFRZVnpWSUpvU0NNRWo1bVZaa0lHSldYUkNrNjQwMVFfVG9rZW46WDJqcWJkelFIb0pTeDl4Z3FFMGNBSk1RbndoXzE3NTUyNjAzOTY6MTc1NTI2Mzk5Nl9WNA&amp;size=large 2x" sizes="auto" data-title="https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=MGFkYTViNGNkYzAxM2JhNmJlMWZhODNiNzI3Nzc4N2RfMFRZVnpWSUpvU0NNRWo1bVZaa0lHSldYUkNrNjQwMVFfVG9rZW46WDJqcWJkelFIb0pTeDl4Z3FFMGNBSk1RbndoXzE3NTUyNjAzOTY6MTc1NTI2Mzk5Nl9WNA" data-alt="https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=MGFkYTViNGNkYzAxM2JhNmJlMWZhODNiNzI3Nzc4N2RfMFRZVnpWSUpvU0NNRWo1bVZaa0lHSldYUkNrNjQwMVFfVG9rZW46WDJqcWJkelFIb0pTeDl4Z3FFMGNBSk1RbndoXzE3NTUyNjAzOTY6MTc1NTI2Mzk5Nl9WNA" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/LLMDailyDigestWeb/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}"/></p>
<p>通过多轮交叉验证确保指令清晰、操作合理</p>
<h2 id="工具使用">工具使用</h2>
<ol start="11">
<li>2025-07-01 12:09:31 Tuesday｜</li>
</ol>
<p><strong><a href="https://papers.cool/arxiv/2506.22853"target="_blank" rel="external nofollow noopener noreferrer">DICE-BENCH：在多轮、多方对话中评估大型语言模型的工具使用能力</a></strong> <strong>[PDF()]</strong> <strong>[Copy]</strong> <strong>[Kimi()]</strong> <strong>[REL]</strong></p>
<p><strong>Authors</strong> : <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kyochul%20Jang"target="_blank" rel="external nofollow noopener noreferrer">Kyochul Jang</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Donghyeon%20Lee"target="_blank" rel="external nofollow noopener noreferrer">Donghyeon Lee</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kyusik%20Kim"target="_blank" rel="external nofollow noopener noreferrer">Kyusik Kim</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dongseok%20Heo"target="_blank" rel="external nofollow noopener noreferrer">Dongseok Heo</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Taewhoo%20Lee"target="_blank" rel="external nofollow noopener noreferrer">Taewhoo Lee</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Woojeong%20Kim"target="_blank" rel="external nofollow noopener noreferrer">Woojeong Kim</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Bongwon%20Suh"target="_blank" rel="external nofollow noopener noreferrer">Bongwon Suh</a></p>
<p>现有的函数调用基准测试侧重于单轮交互。但是，他们忽略了现实世界场景的复杂性。为了量化现有基准测试如何满足实际应用需求，我们引入了 DICE-SCORE，这是一个评估工具相关信息（如函数名称和参数值）在整个对话中的分散程度的指标。通过 DICE-SCORE 分析现有基准揭示了明显的低分，突出了对更现实场景的需求。为了解决这一差距，我们提出了 DICE-BENCH，这是一个框架，它通过维护跨轮依赖关系的工具图和具有不同角色的多代理系统合成对话来构建实用的函数调用数据集，以增强对话的自然性。最终数据集包含 1607 个高 DICE-SCORE 实例。我们使用 DICE-BENCH 对 19 个 LLM 进行的实验表明，在将此类模型有效部署到实际环境中之前，仍需要取得重大进展。我们的代码和数据都是公开的：https://snuhcc.github.io/DICE-Bench/。</p>
<p><strong>科目</strong> :  <strong><a href="https://papers.cool/arxiv/cs.CL"target="_blank" rel="external nofollow noopener noreferrer">计算和语言</a></strong> , <a href="https://papers.cool/arxiv/cs.AI"target="_blank" rel="external nofollow noopener noreferrer">人工智能</a></p>
<p><strong>发布</strong> ： 2025-06-28 11：28：04 UTC</p>
<h2 id="创意写作">创意写作</h2>
<ol start="12">
<li>2025-07-02 17:12:16 Wednesday｜</li>
</ol>
<p><strong><a href="https://papers.cool/arxiv/2507.00769"target="_blank" rel="external nofollow noopener noreferrer">LitBench：可靠评估创意写作的基准和数据集</a></strong> <strong>[PDF(5)]</strong> <strong>[Copy]</strong> <strong>[Kimi(3)]</strong> <strong>[REL]</strong></p>
<p><strong>Authors</strong> : <a href="https://arxiv.org/search/?searchtype=author&amp;query=Daniel%20Fein"target="_blank" rel="external nofollow noopener noreferrer">Daniel Fein</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Sebastian%20Russo"target="_blank" rel="external nofollow noopener noreferrer">Sebastian Russo</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Violet%20Xiang"target="_blank" rel="external nofollow noopener noreferrer">Violet Xiang</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kabir%20Jolly"target="_blank" rel="external nofollow noopener noreferrer">Kabir Jolly</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Rafael%20Rafailov"target="_blank" rel="external nofollow noopener noreferrer">Rafael Rafailov</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Nick%20Haber"target="_blank" rel="external nofollow noopener noreferrer">Nick Haber</a></p>
<p>评估大型语言模型 （LLM） 生成的创意写作仍然具有挑战性，因为开放式叙述缺乏基本事实。如果没有高性能的自动评估方法，现成的 （OTS） 语言模型被用作零镜头评判，但在这种情况下，它们的可靠性尚不清楚。为了追求对创意写作的稳健评估，我们引入了 LitBench，这是第一个用于创意写作验证的标准化基准和配对数据集，包括一个保留的测试集，其中包含 2,480 个来自 Reddit 的去偏见、人类标记的故事比较和一个 43,827 对人类偏好标签的训练语料库。使用 LitBench，我们 （i） 对零镜头 LLM 评委进行基准测试，（ii） 训练 Bradley Terry 和生成奖励模型，以及 （iii） 进行在线人体研究，以验证新 LLM 生成的故事的奖励模型排名。我们的基准将 Claude-3.7-Sonnet 确定为最强的现成评委，与人类偏好的一致性达到 73%;在经过训练的奖励模型中，Bradley-Terry 和 Generative reward 模型的准确率均达到 78%，优于所有现成的裁判。一项在线人类研究进一步证实，我们训练有素的奖励模型始终与人类在新颖的 LLM 生成的故事中的偏好保持一致。我们在 <a href="https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461</a> 上发布 LitBench 和奖励模型，为创意写作系统的可靠、自动评估和优化提供经过审查的资源。</p>
<p><strong>科目</strong> :  <strong><a href="https://papers.cool/arxiv/cs.CL"target="_blank" rel="external nofollow noopener noreferrer">计算和语言</a></strong> , <a href="https://papers.cool/arxiv/cs.AI"target="_blank" rel="external nofollow noopener noreferrer">人工智能</a></p>
<p><strong>发布</strong> ： 2025-07-01 14：10：36 UTC</p>
</div></article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">Public - 2025</span><span class="author" itemprop="copyrightHolder">
              <a href="/LLMDailyDigestWeb/"></a></span></div><div class="footer-line statistics"></div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="Back to Top"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric d-none">0%</span>
        </div></div><div id="mask"></div><noscript>
    <div class="noscript-warning">Theme FixIt works best with JavaScript enabled.</div>
  </noscript>
</div><link rel="preload" href="/LLMDailyDigestWeb/lib/katex/katex.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/LLMDailyDigestWeb/lib/katex/katex.min.css"></noscript><link rel="stylesheet" href="/LLMDailyDigestWeb/lib/cookieconsent/cookieconsent.min.css"><script src="/LLMDailyDigestWeb/lib/sharer/sharer.min.js" async defer></script><script src="/LLMDailyDigestWeb/lib/typeit/index.umd.js" defer></script><script src="/LLMDailyDigestWeb/lib/katex/katex.min.js" defer></script><script src="/LLMDailyDigestWeb/lib/katex/auto-render.min.js" defer></script><script src="/LLMDailyDigestWeb/lib/katex/copy-tex.min.js" defer></script><script src="/LLMDailyDigestWeb/lib/katex/mhchem.min.js" defer></script><script src="/LLMDailyDigestWeb/lib/cookieconsent/cookieconsent.min.js" defer></script><script>window.config={"code":{"copyTitle":"Copy to clipboard","editLockTitle":"Lock editable code block","editUnLockTitle":"Unlock editable code block","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-desktop":"LLM-DailyDigest 大模型研究日报","typeit-header-title-mobile":"LLM-DailyDigest 大模型研究日报"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-desktop":["typeit-header-desktop"],"typeit-header-title-mobile":["typeit-header-title-mobile"]},"duration":-1,"loop":false,"speed":100}};</script><script src="/LLMDailyDigestWeb/js/theme.min.js" defer></script></body>
</html>
