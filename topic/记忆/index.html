<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head><script src="/LLMDailyDigestWeb/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=LLMDailyDigestWeb/livereload" data-no-instant defer></script>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>记忆 - LLM-DailyDigest</title><meta name="author" content="">
<meta name="author-link" content="">
<meta name="description" content="记忆

2025-07-02 17:16:24 Wednesday ｜

微调方法对大型语言模型中记忆的影响 [PDF()] [Copy] [Kimi()] [REL]
Authors : Jie Hou, Chuxiong Wu, Lannan Luo, Qiang Zeng
随着预训练大型语言模型 （LLM） 的功能不断进步，“预训练和微调”范式越来越成为主流，导致了各种微调方法的发展。然而，在微调过程中记忆所带来的隐私风险受到的关注相对较少。为了解决这一差距，我们对流行的微调方法进行了分类，并通过成员推理攻击 （MIA） 的视角评估了它们对记忆的影响。我们的结果表明，与基于参数的微调相比，基于提示的微调实现了有竞争力的性能，同时表现出较低的 MIA 脆弱性。此外，无论模型规模如何，基于提示的方法都保持低记忆。这些发现表明，基于参数的微调更容易泄露私人信息，而基于提示的微调是一种更能保护隐私的选项。
科目 :  计算和语言 , 人工智能
发布 ： 2025-06-30 20：52：15 UTC

2025-06-30 19:11:06 Monday ｜

M**emBench：对基于 LLM 的代理的内存进行更全面的评估** [PDF(1)] [Copy] [Kimi()] [REL]
Authors : Haoran Tan, Zeyu Zhang, Chen Ma, Xu Chen, Quanyu Dai, Zhenhua Dong" /><meta name="keywords" content='Hugo, FixIt' />
  <meta itemprop="name" content="记忆">
  <meta itemprop="description" content="记忆 2025-07-02 17:16:24 Wednesday ｜ 微调方法对大型语言模型中记忆的影响 [PDF()] [Copy] [Kimi()] [REL]
Authors : Jie Hou, Chuxiong Wu, Lannan Luo, Qiang Zeng
随着预训练大型语言模型 （LLM） 的功能不断进步，“预训练和微调”范式越来越成为主流，导致了各种微调方法的发展。然而，在微调过程中记忆所带来的隐私风险受到的关注相对较少。为了解决这一差距，我们对流行的微调方法进行了分类，并通过成员推理攻击 （MIA） 的视角评估了它们对记忆的影响。我们的结果表明，与基于参数的微调相比，基于提示的微调实现了有竞争力的性能，同时表现出较低的 MIA 脆弱性。此外，无论模型规模如何，基于提示的方法都保持低记忆。这些发现表明，基于参数的微调更容易泄露私人信息，而基于提示的微调是一种更能保护隐私的选项。
科目 : 计算和语言 , 人工智能
发布 ： 2025-06-30 20：52：15 UTC
2025-06-30 19:11:06 Monday ｜ M**emBench：对基于 LLM 的代理的内存进行更全面的评估** [PDF(1)] [Copy] [Kimi()] [REL]
Authors : Haoran Tan, Zeyu Zhang, Chen Ma, Xu Chen, Quanyu Dai, Zhenhua Dong">
  <meta itemprop="datePublished" content="2025-09-15T00:00:00+08:00">
  <meta itemprop="dateModified" content="2025-09-15T00:00:00+08:00">
  <meta itemprop="wordCount" content="188"><meta property="og:url" content="http://localhost:1313/LLMDailyDigestWeb/topic/%E8%AE%B0%E5%BF%86/">
  <meta property="og:site_name" content="LLM-DailyDigest">
  <meta property="og:title" content="记忆">
  <meta property="og:description" content="记忆 2025-07-02 17:16:24 Wednesday ｜ 微调方法对大型语言模型中记忆的影响 [PDF()] [Copy] [Kimi()] [REL]
Authors : Jie Hou, Chuxiong Wu, Lannan Luo, Qiang Zeng
随着预训练大型语言模型 （LLM） 的功能不断进步，“预训练和微调”范式越来越成为主流，导致了各种微调方法的发展。然而，在微调过程中记忆所带来的隐私风险受到的关注相对较少。为了解决这一差距，我们对流行的微调方法进行了分类，并通过成员推理攻击 （MIA） 的视角评估了它们对记忆的影响。我们的结果表明，与基于参数的微调相比，基于提示的微调实现了有竞争力的性能，同时表现出较低的 MIA 脆弱性。此外，无论模型规模如何，基于提示的方法都保持低记忆。这些发现表明，基于参数的微调更容易泄露私人信息，而基于提示的微调是一种更能保护隐私的选项。
科目 : 计算和语言 , 人工智能
发布 ： 2025-06-30 20：52：15 UTC
2025-06-30 19:11:06 Monday ｜ M**emBench：对基于 LLM 的代理的内存进行更全面的评估** [PDF(1)] [Copy] [Kimi()] [REL]
Authors : Haoran Tan, Zeyu Zhang, Chen Ma, Xu Chen, Quanyu Dai, Zhenhua Dong">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="topic">
    <meta property="article:published_time" content="2025-09-15T00:00:00+08:00">
    <meta property="article:modified_time" content="2025-09-15T00:00:00+08:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="记忆">
  <meta name="twitter:description" content="记忆 2025-07-02 17:16:24 Wednesday ｜ 微调方法对大型语言模型中记忆的影响 [PDF()] [Copy] [Kimi()] [REL]
Authors : Jie Hou, Chuxiong Wu, Lannan Luo, Qiang Zeng
随着预训练大型语言模型 （LLM） 的功能不断进步，“预训练和微调”范式越来越成为主流，导致了各种微调方法的发展。然而，在微调过程中记忆所带来的隐私风险受到的关注相对较少。为了解决这一差距，我们对流行的微调方法进行了分类，并通过成员推理攻击 （MIA） 的视角评估了它们对记忆的影响。我们的结果表明，与基于参数的微调相比，基于提示的微调实现了有竞争力的性能，同时表现出较低的 MIA 脆弱性。此外，无论模型规模如何，基于提示的方法都保持低记忆。这些发现表明，基于参数的微调更容易泄露私人信息，而基于提示的微调是一种更能保护隐私的选项。
科目 : 计算和语言 , 人工智能
发布 ： 2025-06-30 20：52：15 UTC
2025-06-30 19:11:06 Monday ｜ M**emBench：对基于 LLM 的代理的内存进行更全面的评估** [PDF(1)] [Copy] [Kimi()] [REL]
Authors : Haoran Tan, Zeyu Zhang, Chen Ma, Xu Chen, Quanyu Dai, Zhenhua Dong">
<meta name="application-name" content="LLM-DailyDigest">
<meta name="apple-mobile-web-app-title" content="LLM-DailyDigest"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" href="http://localhost:1313/LLMDailyDigestWeb/topic/%E8%AE%B0%E5%BF%86/" /><link rel="prev" href="http://localhost:1313/LLMDailyDigestWeb/topic/%E5%BA%94%E7%94%A8/" /><link rel="next" href="http://localhost:1313/LLMDailyDigestWeb/topic/%E6%99%BA%E8%83%BD%E4%BD%93/" /><link rel="stylesheet" href="/LLMDailyDigestWeb/css/style.min.css"><link rel="preload" href="/LLMDailyDigestWeb/lib/fontawesome-free/all.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/LLMDailyDigestWeb/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/LLMDailyDigestWeb/lib/animate/animate.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/LLMDailyDigestWeb/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "记忆",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "http:\/\/localhost:1313\/LLMDailyDigestWeb\/topic\/%E8%AE%B0%E5%BF%86\/"
    },"genre": "topic","wordcount":  188 ,
    "url": "http:\/\/localhost:1313\/LLMDailyDigestWeb\/topic\/%E8%AE%B0%E5%BF%86\/","datePublished": "2025-09-15T00:00:00+08:00","dateModified": "2025-09-15T00:00:00+08:00","publisher": {
      "@type": "Organization",
      "name": ""},"author": {
        "@type": "Person",
        "name": "Author"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="sticky" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper" data-page-style="normal"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper">
    <div class="header-title">
      <a href="/LLMDailyDigestWeb/" title="LLM-DailyDigest"><img loading="lazy" src="/LLMDailyDigestWeb/fixit.svg" data-title="LLM-DailyDigest" data-alt="LLM-DailyDigest" class="logo" style="background: url(/LLMDailyDigestWeb/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}"/><span id="typeit-header-desktop" class="typeit"></span></a><span class="header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/LLMDailyDigestWeb/posts/llmdailydigest"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 详情</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/LLMDailyDigestWeb/resources/"
                
                
              >资源</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/LLMDailyDigestWeb/topic/"
                
                
              >主题</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/LLMDailyDigestWeb/updates/"
                
                
              >日报</a></li><li class="menu-item delimiter"></li><li class="menu-item theme-switch" title="Switch Theme">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li></ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/LLMDailyDigestWeb/" title="LLM-DailyDigest"><img loading="lazy" src="/LLMDailyDigestWeb/fixit.svg" data-title="/LLMDailyDigestWeb/fixit.svg" data-alt="/LLMDailyDigestWeb/fixit.svg" class="logo" style="background: url(/LLMDailyDigestWeb/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}"/><span id="typeit-header-title-mobile" class="typeit"></span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/LLMDailyDigestWeb/posts/llmdailydigest"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 详情</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/LLMDailyDigestWeb/resources/"
                  
                  
                >资源</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/LLMDailyDigestWeb/topic/"
                  
                  
                >主题</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/LLMDailyDigestWeb/updates/"
                  
                  
                >日报</a></li><li class="menu-item menu-system">
          <span class="menu-system-item theme-switch" title="Switch Theme"><i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i></span></li>
      </ul>
    </nav>
  </div>
</header><main class="container"><aside class="toc" id="toc-auto"><h2 class="toc-title">Contents&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden="true"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom">
    </aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX"><span>记忆</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      </span></span></div>
      <div class="post-meta-line"><span title="published on 2025-09-15 00:00:00"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden="true"></i><time datetime="2025-09-15">2025-09-15</time></span>&nbsp;<span title="Updated on 2025-09-15 00:00:00"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden="true"></i><time datetime="2025-09-15">2025-09-15</time></span>&nbsp;<span title="188 words"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden="true"></i>About 200 words</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden="true"></i>One minute</span>&nbsp;</div>
    </div><div class="details toc" id="toc-static" data-kept="false">
        <div class="details-summary toc-title">
          <span>Contents</span><i class="details-icon fa-solid fa-angle-right" aria-hidden="true"></i></div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li><a href="#人大高瓴-华为诺亚大语言模型智能体记忆机制的系列研究"><a href="https://mp.weixin.qq.com/s/n_oc7X1cZ1vGwPWhXpLUjA">人大高瓴-华为诺亚：大语言模型智能体记忆机制的系列研究</a></a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><h1 id="记忆">记忆</h1>
<ol>
<li>2025-07-02 17:16:24 Wednesday ｜</li>
</ol>
<p><strong><a href="https://papers.cool/arxiv/2507.00258"target="_blank" rel="external nofollow noopener noreferrer">微调方法对大型语言模型中记忆的影响</a></strong> <strong>[PDF()]</strong> <strong>[Copy]</strong> <strong>[Kimi()]</strong> <strong>[REL]</strong></p>
<p><strong>Authors</strong> : <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jie%20Hou"target="_blank" rel="external nofollow noopener noreferrer">Jie Hou</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chuxiong%20Wu"target="_blank" rel="external nofollow noopener noreferrer">Chuxiong Wu</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lannan%20Luo"target="_blank" rel="external nofollow noopener noreferrer">Lannan Luo</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Qiang%20Zeng"target="_blank" rel="external nofollow noopener noreferrer">Qiang Zeng</a></p>
<p>随着预训练大型语言模型 （LLM） 的功能不断进步，“预训练和微调”范式越来越成为主流，导致了各种微调方法的发展。然而，在微调过程中记忆所带来的隐私风险受到的关注相对较少。为了解决这一差距，我们对流行的微调方法进行了分类，并通过成员推理攻击 （MIA） 的视角评估了它们对记忆的影响。我们的结果表明，与基于参数的微调相比，基于提示的微调实现了有竞争力的性能，同时表现出较低的 MIA 脆弱性。此外，无论模型规模如何，基于提示的方法都保持低记忆。这些发现表明，基于参数的微调更容易泄露私人信息，而基于提示的微调是一种更能保护隐私的选项。</p>
<p><strong>科目</strong> :  <strong><a href="https://papers.cool/arxiv/cs.CL"target="_blank" rel="external nofollow noopener noreferrer">计算和语言</a></strong> , <a href="https://papers.cool/arxiv/cs.AI"target="_blank" rel="external nofollow noopener noreferrer">人工智能</a></p>
<p><strong>发布</strong> ： 2025-06-30 20：52：15 UTC</p>
<ol start="2">
<li>2025-06-30 19:11:06 Monday ｜</li>
</ol>
<p><strong>M</strong>**<a href="https://papers.cool/arxiv/2506.21605"target="_blank" rel="external nofollow noopener noreferrer">emBench：对基于 LLM 的代理的内存进行更全面的评估</a>** <strong>[PDF(1)]</strong> <strong>[Copy]</strong> <strong>[Kimi()]</strong> <strong>[REL]</strong></p>
<p><strong>Authors</strong> : <a href="https://arxiv.org/search/?searchtype=author&amp;query=Haoran%20Tan"target="_blank" rel="external nofollow noopener noreferrer">Haoran Tan</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zeyu%20Zhang"target="_blank" rel="external nofollow noopener noreferrer">Zeyu Zhang</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chen%20Ma"target="_blank" rel="external nofollow noopener noreferrer">Chen Ma</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xu%20Chen"target="_blank" rel="external nofollow noopener noreferrer">Xu Chen</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Quanyu%20Dai"target="_blank" rel="external nofollow noopener noreferrer">Quanyu Dai</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhenhua%20Dong"target="_blank" rel="external nofollow noopener noreferrer">Zhenhua Dong</a></p>
<p>最近的工作强调了基于 LLM 的代理中记忆机制的重要性，这使它们能够存储观察到的信息并适应动态环境。然而，评估它们的内存能力仍然具有挑战性。以前的评估通常受到内存级别和交互式场景多样性的限制。它们也缺乏全面的指标来从多个方面反映内存能力。为了解决这些问题，在本文中，我们构建了一个更全面的数据集和基准来评估基于 LLM 的代理的内存能力。我们的数据集将事实记忆和反思记忆作为不同的层次，并提出参与和观察作为各种互动场景。基于我们的数据集，我们提出了一个名为 MemBench 的基准测试，从多个方面评估基于 LLM 的代理的内存能力，包括它们的有效性、效率和容量。为了造福研究社区，我们在 <a href="https://github.com/import-myself/Membench"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/import-myself/Membench</a> 上发布了我们的数据集和项目。</p>
<ol start="3">
<li>2025-06-30 19:55:59 Monday ｜</li>
</ol>
<p><strong><a href="https://papers.cool/arxiv/2506.21588"target="_blank" rel="external nofollow noopener noreferrer">通过电路发现了解 LLM 中的逐字记忆</a></strong> <strong>[PDF()]</strong> <strong>[Copy]</strong> <strong>[Kimi()]</strong> <strong>[REL]</strong></p>
<p><strong>Authors</strong> : <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ilya%20Lasy"target="_blank" rel="external nofollow noopener noreferrer">Ilya Lasy</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Peter%20Knees"target="_blank" rel="external nofollow noopener noreferrer">Peter Knees</a>, <a href="https://arxiv.org/search/?searchtype=author&amp;query=Stefan%20Woltran"target="_blank" rel="external nofollow noopener noreferrer">Stefan Woltran</a></p>
<p>LLM 中记忆的潜在机制 &ndash; 训练数据的逐字复制 &ndash; 仍然知之甚少。网络的确切部分决定检索我们认为作为记忆序列开始的令牌？模型在产生记忆句子和非记忆句子时的行为究竟有何不同？在这项工作中，我们利用变压器电路从机制可解释性的角度来解决这些问题——变压器电路是在模型中执行特定功能的最小计算子图。通过精心构建的对比数据集，我们确定了模型生成与记忆内容不同的点，并隔离了负责记忆的两个不同方面的特定电路。我们发现，启动记忆的电路在启动后也可以保持它，而只保持记忆的电路不能触发它的启动。有趣的是，记忆预防机制在不同的文本域之间稳健地转移，而记忆归纳似乎更依赖于上下文。</p>
<p>20250808｜协同检索器 - ICML 2025｜M+: Extending MemoryLLM with Scalable Long-Term Memory</p>
<ul>
<li><strong>论文链接：</strong><a href="https://arxiv.org/abs/2502.00592"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2502.00592</a></li>
<li><strong>代码仓库：</strong><a href="https://github.com/wangyu-ustc/MemoryLLM"target="_blank" rel="external nofollow noopener noreferrer">GitHub - wangyu-ustc/MemoryLLM: The official implementation of the ICML 2024 paper &quot;MemoryLLM: Towar</a></li>
<li><strong>开源模型：</strong><a href="https://huggingface.co/YuWangX"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/YuWangX</a></li>
</ul>
<h4 id="人大高瓴-华为诺亚大语言模型智能体记忆机制的系列研究"><a href="https://mp.weixin.qq.com/s/n_oc7X1cZ1vGwPWhXpLUjA"target="_blank" rel="external nofollow noopener noreferrer">人大高瓴-华为诺亚：大语言模型智能体记忆机制的系列研究</a></h4>
<p>20250808｜Memory+Agent：人大高瓴人工智能学院与华为诺亚方舟实验室聚焦大语言模型智能体的记忆能力，在该领域形成了一套完整的包括综述论文、数据集和工具包的研究体系，致力于推动该领域的发展。</p>
<ol>
<li>论文标题： A Survey on the Memory Mechanism of Large Language Model based Agents
<ol>
<li>论文链接： <a href="https://dl.acm.org/doi/10.1145/3748302"target="_blank" rel="external nofollow noopener noreferrer">https://dl.acm.org/doi/10.1145/3748302</a></li>
</ol>
</li>
<li>论文标题： MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants
<ol>
<li>论文链接： <a href="https://arxiv.org/abs/2409.20163"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2409.20163</a></li>
</ol>
</li>
<li>论文标题： MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents
<ol>
<li>论文链接： <a href="https://arxiv.org/abs/2506.21605"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2506.21605</a></li>
</ol>
</li>
<li><a href="https://arxiv.org/abs/2506.21605"target="_blank" rel="external nofollow noopener noreferrer">论文标题： MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents</a>
<ol>
<li>论文链接：<a href="https://dl.acm.org/doi/10.1145/3701716.3715299"target="_blank" rel="external nofollow noopener noreferrer">https://dl.acm.org/doi/10.1145/3701716.3715299</a></li>
</ol>
</li>
</ol>
<h1 id="幻觉">幻觉</h1>
<p><a href="https://mp.weixin.qq.com/s/f6pTUhzn9NjA_xWSvTmG4A"target="_blank" rel="external nofollow noopener noreferrer">OpenAI罕见发论文：我们找到了AI幻觉的罪魁祸首</a></p>
<p><a href="https://mp.weixin.qq.com/s/Tb9ZhldD53wbQ3QxKwy-Ig"target="_blank" rel="external nofollow noopener noreferrer">谢菲尔德大学：模型幻觉的数学必然性</a></p>
<p><a href="https://huggingface.co/papers/2509.04664?utm_source=digest-papers&amp;utm_medium=email&amp;utm_campaign=2025-09-08"target="_blank" rel="external nofollow noopener noreferrer">为什么语言模型会产生幻觉（78） </a><a href="https://huggingface.co/papers/2509.04664?utm_source=digest-papers&amp;utm_medium=email&amp;utm_campaign=2025-09-08"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/papers/2509.04664?utm_source=digest-papers&utm_medium=email&utm_campaign=2025-09-08</a></p>
</div></article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">Public - 2025</span><span class="author" itemprop="copyrightHolder">
              <a href="/LLMDailyDigestWeb/"></a></span></div><div class="footer-line statistics"></div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="Back to Top"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric d-none">0%</span>
        </div></div><div id="mask"></div><noscript>
    <div class="noscript-warning">Theme FixIt works best with JavaScript enabled.</div>
  </noscript>
</div><link rel="preload" href="/LLMDailyDigestWeb/lib/katex/katex.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/LLMDailyDigestWeb/lib/katex/katex.min.css"></noscript><link rel="stylesheet" href="/LLMDailyDigestWeb/lib/cookieconsent/cookieconsent.min.css"><script src="/LLMDailyDigestWeb/lib/sharer/sharer.min.js" async defer></script><script src="/LLMDailyDigestWeb/lib/typeit/index.umd.js" defer></script><script src="/LLMDailyDigestWeb/lib/katex/katex.min.js" defer></script><script src="/LLMDailyDigestWeb/lib/katex/auto-render.min.js" defer></script><script src="/LLMDailyDigestWeb/lib/katex/copy-tex.min.js" defer></script><script src="/LLMDailyDigestWeb/lib/katex/mhchem.min.js" defer></script><script src="/LLMDailyDigestWeb/lib/cookieconsent/cookieconsent.min.js" defer></script><script>window.config={"code":{"copyTitle":"Copy to clipboard","editLockTitle":"Lock editable code block","editUnLockTitle":"Unlock editable code block","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-desktop":"LLM-DailyDigest 大模型研究日报","typeit-header-title-mobile":"LLM-DailyDigest 大模型研究日报"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-desktop":["typeit-header-desktop"],"typeit-header-title-mobile":["typeit-header-title-mobile"]},"duration":-1,"loop":false,"speed":100}};</script><script src="/LLMDailyDigestWeb/js/theme.min.js" defer></script></body>
</html>
