<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head><script src="/LLMDailyDigestWeb/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=LLMDailyDigestWeb/livereload" data-no-instant defer></script>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>新模型 - LLM-DailyDigest</title><meta name="author" content="">
<meta name="author-link" content="">
<meta name="description" content="新模型
2025-09-15
Qwen3-Max-Preview （Instruct）
2025-08-15
实测Perplexity Pro平替模型，免费开源仅4B
2025-0811
智谱GLM-4.5V

相关素材

体验地址：https://chat.z.ai/
HuggingFace 开源地址：https://huggingface.co/zai-org/GLM-4.5V
GitHub 开源地址：https://github.com/zai-org/GLM-V
桌面助手下载地址：https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App
魔搭社区：https://modelscope.cn/collections/GLM-45V-8b471c8f97154e


对图像的识别与推理、视频理解：GLM-4.5V 在涵盖图像理解、视频理解、GUI、文档理解等任务的 41 个公开视觉多模态榜单中综合效果达到了开源 SOTA 水平，这和我们在实测中体验到的结果是一致的。

2025-08-08
GPT-5：博士生水平


官方介绍：https://openai.com/index/introducing-gpt-5/

gpt-5 ：专注逻辑推理和多步骤任务
gpt-5-mini ：轻量级版本，成本敏感型应用
gpt-5-nano ：速度优化版，超低延迟
gpt-5-chat ：企业级多模态对话，支持上下文感知



GPT-5 是一个一体化系统，包含三个核心部分：

一个智能高效的基础模型，可解答大多数问题
一个深度推理模型（即GPT-5思维模块），用于处理更复杂的难题
以及一个实时路由模块，能够基于对话类型、问题复杂度、工具需求及用户显式指令（如prompt含“仔细思考这个问题”）智能调度模型



快来看看GPT-5第一波实测

ARC-AGI的成绩单表示GPT-5不如Grok 4
SimpleBench上，GPT-5的水平已经超过了人类平均水平，在大模型中尚属首次。这是一个简单常识推理类的数据集，主要特点就是对于人类非常简单，但对大模型比较困难。



GPT-5来了！人人都能免费用，最强大模型只需最傻瓜式使用


GPT-5编程成绩有猫腻！自删23道测试题，关键基准还是自己提的


2025-08-07

端侧｜Qwen紧追OpenAI开源4B端侧大模型，AIME25得分超越Claude 4 Opus

Qwen3-4B-Instruct-2507：非推理模型，大幅提升通用能力
Qwen3-4B-Thinking-2507：高级推理模型，专为专家级任务设计，逻辑、数学、科学及代码中的高级推理能力——专为专家级任务设计。
更智能、更精准，并且支持256k上下文，更具上下文感知能力。
抱抱脸直通车：

[1]https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507
[2]https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507


魔搭社区直通车：

ttps://modelscope.cn/models/Qwen/Qwen3-4B-Instruct-2507
https://modelscope.cn/models/Qwen/Qwen3-4B-Thinking-2507




智能体｜Reflection AI已经发布了他们的首款AI智能体Asimov，较Claude Code Sonnet 4等模型，得到了用户更多偏好。

Asimov是一款专为代码理解打造的，它能对代码仓库、架构文档、GitHub讨论串、对话历史等多种信息进行索引，从而形成对代码库结构、历史及团队知识的全面认知。
Asimov 并非单一智能体 ，而是 由几个小型智能体协同工作 。



2025-08-06

Claude Opus 4.1

Claude Opus 4.1火速发布！坐稳编程之王，官方：马上还有大更新

编程性能再次突破天花板，超越Claude Opus 4，拿下SOTA。

在SWE-bench上，Opus 4.1超越Opus 4、Gemini 2.5 Pro、o3，将性能提升至74.5%，拿下新SOTA。


Blog：https://www.anthropic.com/news/claude-opus-4-1
System Card：https://assets.anthropic.com/m/4c024b86c698d3d4/original/Claude-4-1-System-Card.pdf




谷歌DeepMind发布了****新一代通用世界模型Genie 3

谷歌“世界模拟器”深夜上线！一句话生成3D世界，支持分钟级超长记忆

Genie 3相比上一代大幅升级，支持****720P画质，每秒24帧实时导航，以及分钟级的一致性保持。
最让谷歌引以为傲的，还要属Genie 3的****长期环境一致性。
DeepMind十多年来一直在关注模拟环境领域的研究，从训练智能体掌握实时战略游戏， 到开发用于开放式学习和机器人技术的模拟环境。


谷歌推出「G」字号第三代世界模型Genie 3，号称「宇宙模拟器」，视频生成更加符合物理定律。


OpenAI开源两个推理模型：gpt-oss-120b和gpt-oss-20b。

刚刚，OpenAI开源2个推理模型：笔记本/手机就能跑，性能接近o4-mini

gpt-oss，即Open Source Series，意思就是“开源系列”。
gpt-oss-120b：1170亿参数（MoE架构，激活参数约51亿），可在单张80GB GPU上运行，性能接近闭源的o4-mini。
gpt-oss-20b：210亿参数（Moe架构，激活参数约36亿），可在16GB内存的消费级设备上运行，性能接近o3-mini。
整体来看，这两个模型在工具使用、少样本函数调用、链式思考推理（如Tau-Bench智能评估套件的结果所示）以及HealthBench上表现强劲，甚至超越了包括OpenAI o1和GPT‑4o在内的专有模型。
gpt-oss-120b每个token激活5.1B个参数，而gpt-oss-20b激活3.6B个参数。这些模型分别具有117b和21b的总参数。
技术博客地址：https://openai.com/index/introducing-gpt-oss/
HuggingFace地址：https://huggingface.co/openai/gpt-oss-120b
GtiHub地址：https://github.com/openai/gpt-oss
OpenAI-OSS-120B用起来要谨慎，写代码特别不稳定。OpenAI-OSS-20B在这个参数量大小下反而挺好。


全网开测GPT-oss！技术架构也扒明白了


GPT-oss在架构设计上既保留了MoE Transformer的核心架构，又通过细节优化提升性能、降低复杂度，使其成为适合开源模型的基础架构。" /><meta name="keywords" content='Hugo, FixIt' />
  <meta itemprop="name" content="新模型">
  <meta itemprop="description" content="新模型 2025-09-15 Qwen3-Max-Preview （Instruct）
2025-08-15 实测Perplexity Pro平替模型，免费开源仅4B
2025-0811 智谱GLM-4.5V 相关素材 体验地址：https://chat.z.ai/ HuggingFace 开源地址：https://huggingface.co/zai-org/GLM-4.5V GitHub 开源地址：https://github.com/zai-org/GLM-V 桌面助手下载地址：https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App 魔搭社区：https://modelscope.cn/collections/GLM-45V-8b471c8f97154e 对图像的识别与推理、视频理解：GLM-4.5V 在涵盖图像理解、视频理解、GUI、文档理解等任务的 41 个公开视觉多模态榜单中综合效果达到了开源 SOTA 水平，这和我们在实测中体验到的结果是一致的。 2025-08-08 GPT-5：博士生水平 官方介绍：https://openai.com/index/introducing-gpt-5/
gpt-5 ：专注逻辑推理和多步骤任务 gpt-5-mini ：轻量级版本，成本敏感型应用 gpt-5-nano ：速度优化版，超低延迟 gpt-5-chat ：企业级多模态对话，支持上下文感知 GPT-5 是一个一体化系统，包含三个核心部分：
一个智能高效的基础模型，可解答大多数问题 一个深度推理模型（即GPT-5思维模块），用于处理更复杂的难题 以及一个实时路由模块，能够基于对话类型、问题复杂度、工具需求及用户显式指令（如prompt含“仔细思考这个问题”）智能调度模型 快来看看GPT-5第一波实测
ARC-AGI的成绩单表示GPT-5不如Grok 4 SimpleBench上，GPT-5的水平已经超过了人类平均水平，在大模型中尚属首次。这是一个简单常识推理类的数据集，主要特点就是对于人类非常简单，但对大模型比较困难。 GPT-5来了！人人都能免费用，最强大模型只需最傻瓜式使用
GPT-5编程成绩有猫腻！自删23道测试题，关键基准还是自己提的
2025-08-07 端侧｜Qwen紧追OpenAI开源4B端侧大模型，AIME25得分超越Claude 4 Opus Qwen3-4B-Instruct-2507：非推理模型，大幅提升通用能力 Qwen3-4B-Thinking-2507：高级推理模型，专为专家级任务设计，逻辑、数学、科学及代码中的高级推理能力——专为专家级任务设计。 更智能、更精准，并且支持256k上下文，更具上下文感知能力。 抱抱脸直通车： [1]https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507 [2]https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507 魔搭社区直通车： ttps://modelscope.cn/models/Qwen/Qwen3-4B-Instruct-2507 https://modelscope.cn/models/Qwen/Qwen3-4B-Thinking-2507 智能体｜Reflection AI已经发布了他们的首款AI智能体Asimov，较Claude Code Sonnet 4等模型，得到了用户更多偏好。 Asimov是一款专为代码理解打造的，它能对代码仓库、架构文档、GitHub讨论串、对话历史等多种信息进行索引，从而形成对代码库结构、历史及团队知识的全面认知。 Asimov 并非单一智能体 ，而是 由几个小型智能体协同工作 。 2025-08-06 Claude Opus 4.1 Claude Opus 4.1火速发布！坐稳编程之王，官方：马上还有大更新 编程性能再次突破天花板，超越Claude Opus 4，拿下SOTA。 在SWE-bench上，Opus 4.1超越Opus 4、Gemini 2.5 Pro、o3，将性能提升至74.5%，拿下新SOTA。 Blog：https://www.anthropic.com/news/claude-opus-4-1 System Card：https://assets.anthropic.com/m/4c024b86c698d3d4/original/Claude-4-1-System-Card.pdf 谷歌DeepMind发布了****新一代通用世界模型Genie 3 谷歌“世界模拟器”深夜上线！一句话生成3D世界，支持分钟级超长记忆 Genie 3相比上一代大幅升级，支持****720P画质，每秒24帧实时导航，以及分钟级的一致性保持。 最让谷歌引以为傲的，还要属Genie 3的****长期环境一致性。 DeepMind十多年来一直在关注模拟环境领域的研究，从训练智能体掌握实时战略游戏， 到开发用于开放式学习和机器人技术的模拟环境。 谷歌推出「G」字号第三代世界模型Genie 3，号称「宇宙模拟器」，视频生成更加符合物理定律。 OpenAI开源两个推理模型：gpt-oss-120b和gpt-oss-20b。 刚刚，OpenAI开源2个推理模型：笔记本/手机就能跑，性能接近o4-mini gpt-oss，即Open Source Series，意思就是“开源系列”。 gpt-oss-120b：1170亿参数（MoE架构，激活参数约51亿），可在单张80GB GPU上运行，性能接近闭源的o4-mini。 gpt-oss-20b：210亿参数（Moe架构，激活参数约36亿），可在16GB内存的消费级设备上运行，性能接近o3-mini。 整体来看，这两个模型在工具使用、少样本函数调用、链式思考推理（如Tau-Bench智能评估套件的结果所示）以及HealthBench上表现强劲，甚至超越了包括OpenAI o1和GPT‑4o在内的专有模型。 gpt-oss-120b每个token激活5.1B个参数，而gpt-oss-20b激活3.6B个参数。这些模型分别具有117b和21b的总参数。 技术博客地址：https://openai.com/index/introducing-gpt-oss/ HuggingFace地址：https://huggingface.co/openai/gpt-oss-120b GtiHub地址：https://github.com/openai/gpt-oss OpenAI-OSS-120B用起来要谨慎，写代码特别不稳定。OpenAI-OSS-20B在这个参数量大小下反而挺好。 全网开测GPT-oss！技术架构也扒明白了 GPT-oss在架构设计上既保留了MoE Transformer的核心架构，又通过细节优化提升性能、降低复杂度，使其成为适合开源模型的基础架构。">
  <meta itemprop="datePublished" content="2025-09-15T00:00:00+08:00">
  <meta itemprop="dateModified" content="2025-09-15T00:00:00+08:00">
  <meta itemprop="wordCount" content="449"><meta property="og:url" content="http://localhost:1313/LLMDailyDigestWeb/topic/%E6%96%B0%E6%A8%A1%E5%9E%8B/">
  <meta property="og:site_name" content="LLM-DailyDigest">
  <meta property="og:title" content="新模型">
  <meta property="og:description" content="新模型 2025-09-15 Qwen3-Max-Preview （Instruct）
2025-08-15 实测Perplexity Pro平替模型，免费开源仅4B
2025-0811 智谱GLM-4.5V 相关素材 体验地址：https://chat.z.ai/ HuggingFace 开源地址：https://huggingface.co/zai-org/GLM-4.5V GitHub 开源地址：https://github.com/zai-org/GLM-V 桌面助手下载地址：https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App 魔搭社区：https://modelscope.cn/collections/GLM-45V-8b471c8f97154e 对图像的识别与推理、视频理解：GLM-4.5V 在涵盖图像理解、视频理解、GUI、文档理解等任务的 41 个公开视觉多模态榜单中综合效果达到了开源 SOTA 水平，这和我们在实测中体验到的结果是一致的。 2025-08-08 GPT-5：博士生水平 官方介绍：https://openai.com/index/introducing-gpt-5/
gpt-5 ：专注逻辑推理和多步骤任务 gpt-5-mini ：轻量级版本，成本敏感型应用 gpt-5-nano ：速度优化版，超低延迟 gpt-5-chat ：企业级多模态对话，支持上下文感知 GPT-5 是一个一体化系统，包含三个核心部分：
一个智能高效的基础模型，可解答大多数问题 一个深度推理模型（即GPT-5思维模块），用于处理更复杂的难题 以及一个实时路由模块，能够基于对话类型、问题复杂度、工具需求及用户显式指令（如prompt含“仔细思考这个问题”）智能调度模型 快来看看GPT-5第一波实测
ARC-AGI的成绩单表示GPT-5不如Grok 4 SimpleBench上，GPT-5的水平已经超过了人类平均水平，在大模型中尚属首次。这是一个简单常识推理类的数据集，主要特点就是对于人类非常简单，但对大模型比较困难。 GPT-5来了！人人都能免费用，最强大模型只需最傻瓜式使用
GPT-5编程成绩有猫腻！自删23道测试题，关键基准还是自己提的
2025-08-07 端侧｜Qwen紧追OpenAI开源4B端侧大模型，AIME25得分超越Claude 4 Opus Qwen3-4B-Instruct-2507：非推理模型，大幅提升通用能力 Qwen3-4B-Thinking-2507：高级推理模型，专为专家级任务设计，逻辑、数学、科学及代码中的高级推理能力——专为专家级任务设计。 更智能、更精准，并且支持256k上下文，更具上下文感知能力。 抱抱脸直通车： [1]https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507 [2]https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507 魔搭社区直通车： ttps://modelscope.cn/models/Qwen/Qwen3-4B-Instruct-2507 https://modelscope.cn/models/Qwen/Qwen3-4B-Thinking-2507 智能体｜Reflection AI已经发布了他们的首款AI智能体Asimov，较Claude Code Sonnet 4等模型，得到了用户更多偏好。 Asimov是一款专为代码理解打造的，它能对代码仓库、架构文档、GitHub讨论串、对话历史等多种信息进行索引，从而形成对代码库结构、历史及团队知识的全面认知。 Asimov 并非单一智能体 ，而是 由几个小型智能体协同工作 。 2025-08-06 Claude Opus 4.1 Claude Opus 4.1火速发布！坐稳编程之王，官方：马上还有大更新 编程性能再次突破天花板，超越Claude Opus 4，拿下SOTA。 在SWE-bench上，Opus 4.1超越Opus 4、Gemini 2.5 Pro、o3，将性能提升至74.5%，拿下新SOTA。 Blog：https://www.anthropic.com/news/claude-opus-4-1 System Card：https://assets.anthropic.com/m/4c024b86c698d3d4/original/Claude-4-1-System-Card.pdf 谷歌DeepMind发布了****新一代通用世界模型Genie 3 谷歌“世界模拟器”深夜上线！一句话生成3D世界，支持分钟级超长记忆 Genie 3相比上一代大幅升级，支持****720P画质，每秒24帧实时导航，以及分钟级的一致性保持。 最让谷歌引以为傲的，还要属Genie 3的****长期环境一致性。 DeepMind十多年来一直在关注模拟环境领域的研究，从训练智能体掌握实时战略游戏， 到开发用于开放式学习和机器人技术的模拟环境。 谷歌推出「G」字号第三代世界模型Genie 3，号称「宇宙模拟器」，视频生成更加符合物理定律。 OpenAI开源两个推理模型：gpt-oss-120b和gpt-oss-20b。 刚刚，OpenAI开源2个推理模型：笔记本/手机就能跑，性能接近o4-mini gpt-oss，即Open Source Series，意思就是“开源系列”。 gpt-oss-120b：1170亿参数（MoE架构，激活参数约51亿），可在单张80GB GPU上运行，性能接近闭源的o4-mini。 gpt-oss-20b：210亿参数（Moe架构，激活参数约36亿），可在16GB内存的消费级设备上运行，性能接近o3-mini。 整体来看，这两个模型在工具使用、少样本函数调用、链式思考推理（如Tau-Bench智能评估套件的结果所示）以及HealthBench上表现强劲，甚至超越了包括OpenAI o1和GPT‑4o在内的专有模型。 gpt-oss-120b每个token激活5.1B个参数，而gpt-oss-20b激活3.6B个参数。这些模型分别具有117b和21b的总参数。 技术博客地址：https://openai.com/index/introducing-gpt-oss/ HuggingFace地址：https://huggingface.co/openai/gpt-oss-120b GtiHub地址：https://github.com/openai/gpt-oss OpenAI-OSS-120B用起来要谨慎，写代码特别不稳定。OpenAI-OSS-20B在这个参数量大小下反而挺好。 全网开测GPT-oss！技术架构也扒明白了 GPT-oss在架构设计上既保留了MoE Transformer的核心架构，又通过细节优化提升性能、降低复杂度，使其成为适合开源模型的基础架构。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="topic">
    <meta property="article:published_time" content="2025-09-15T00:00:00+08:00">
    <meta property="article:modified_time" content="2025-09-15T00:00:00+08:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="新模型">
  <meta name="twitter:description" content="新模型 2025-09-15 Qwen3-Max-Preview （Instruct）
2025-08-15 实测Perplexity Pro平替模型，免费开源仅4B
2025-0811 智谱GLM-4.5V 相关素材 体验地址：https://chat.z.ai/ HuggingFace 开源地址：https://huggingface.co/zai-org/GLM-4.5V GitHub 开源地址：https://github.com/zai-org/GLM-V 桌面助手下载地址：https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App 魔搭社区：https://modelscope.cn/collections/GLM-45V-8b471c8f97154e 对图像的识别与推理、视频理解：GLM-4.5V 在涵盖图像理解、视频理解、GUI、文档理解等任务的 41 个公开视觉多模态榜单中综合效果达到了开源 SOTA 水平，这和我们在实测中体验到的结果是一致的。 2025-08-08 GPT-5：博士生水平 官方介绍：https://openai.com/index/introducing-gpt-5/
gpt-5 ：专注逻辑推理和多步骤任务 gpt-5-mini ：轻量级版本，成本敏感型应用 gpt-5-nano ：速度优化版，超低延迟 gpt-5-chat ：企业级多模态对话，支持上下文感知 GPT-5 是一个一体化系统，包含三个核心部分：
一个智能高效的基础模型，可解答大多数问题 一个深度推理模型（即GPT-5思维模块），用于处理更复杂的难题 以及一个实时路由模块，能够基于对话类型、问题复杂度、工具需求及用户显式指令（如prompt含“仔细思考这个问题”）智能调度模型 快来看看GPT-5第一波实测
ARC-AGI的成绩单表示GPT-5不如Grok 4 SimpleBench上，GPT-5的水平已经超过了人类平均水平，在大模型中尚属首次。这是一个简单常识推理类的数据集，主要特点就是对于人类非常简单，但对大模型比较困难。 GPT-5来了！人人都能免费用，最强大模型只需最傻瓜式使用
GPT-5编程成绩有猫腻！自删23道测试题，关键基准还是自己提的
2025-08-07 端侧｜Qwen紧追OpenAI开源4B端侧大模型，AIME25得分超越Claude 4 Opus Qwen3-4B-Instruct-2507：非推理模型，大幅提升通用能力 Qwen3-4B-Thinking-2507：高级推理模型，专为专家级任务设计，逻辑、数学、科学及代码中的高级推理能力——专为专家级任务设计。 更智能、更精准，并且支持256k上下文，更具上下文感知能力。 抱抱脸直通车： [1]https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507 [2]https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507 魔搭社区直通车： ttps://modelscope.cn/models/Qwen/Qwen3-4B-Instruct-2507 https://modelscope.cn/models/Qwen/Qwen3-4B-Thinking-2507 智能体｜Reflection AI已经发布了他们的首款AI智能体Asimov，较Claude Code Sonnet 4等模型，得到了用户更多偏好。 Asimov是一款专为代码理解打造的，它能对代码仓库、架构文档、GitHub讨论串、对话历史等多种信息进行索引，从而形成对代码库结构、历史及团队知识的全面认知。 Asimov 并非单一智能体 ，而是 由几个小型智能体协同工作 。 2025-08-06 Claude Opus 4.1 Claude Opus 4.1火速发布！坐稳编程之王，官方：马上还有大更新 编程性能再次突破天花板，超越Claude Opus 4，拿下SOTA。 在SWE-bench上，Opus 4.1超越Opus 4、Gemini 2.5 Pro、o3，将性能提升至74.5%，拿下新SOTA。 Blog：https://www.anthropic.com/news/claude-opus-4-1 System Card：https://assets.anthropic.com/m/4c024b86c698d3d4/original/Claude-4-1-System-Card.pdf 谷歌DeepMind发布了****新一代通用世界模型Genie 3 谷歌“世界模拟器”深夜上线！一句话生成3D世界，支持分钟级超长记忆 Genie 3相比上一代大幅升级，支持****720P画质，每秒24帧实时导航，以及分钟级的一致性保持。 最让谷歌引以为傲的，还要属Genie 3的****长期环境一致性。 DeepMind十多年来一直在关注模拟环境领域的研究，从训练智能体掌握实时战略游戏， 到开发用于开放式学习和机器人技术的模拟环境。 谷歌推出「G」字号第三代世界模型Genie 3，号称「宇宙模拟器」，视频生成更加符合物理定律。 OpenAI开源两个推理模型：gpt-oss-120b和gpt-oss-20b。 刚刚，OpenAI开源2个推理模型：笔记本/手机就能跑，性能接近o4-mini gpt-oss，即Open Source Series，意思就是“开源系列”。 gpt-oss-120b：1170亿参数（MoE架构，激活参数约51亿），可在单张80GB GPU上运行，性能接近闭源的o4-mini。 gpt-oss-20b：210亿参数（Moe架构，激活参数约36亿），可在16GB内存的消费级设备上运行，性能接近o3-mini。 整体来看，这两个模型在工具使用、少样本函数调用、链式思考推理（如Tau-Bench智能评估套件的结果所示）以及HealthBench上表现强劲，甚至超越了包括OpenAI o1和GPT‑4o在内的专有模型。 gpt-oss-120b每个token激活5.1B个参数，而gpt-oss-20b激活3.6B个参数。这些模型分别具有117b和21b的总参数。 技术博客地址：https://openai.com/index/introducing-gpt-oss/ HuggingFace地址：https://huggingface.co/openai/gpt-oss-120b GtiHub地址：https://github.com/openai/gpt-oss OpenAI-OSS-120B用起来要谨慎，写代码特别不稳定。OpenAI-OSS-20B在这个参数量大小下反而挺好。 全网开测GPT-oss！技术架构也扒明白了 GPT-oss在架构设计上既保留了MoE Transformer的核心架构，又通过细节优化提升性能、降低复杂度，使其成为适合开源模型的基础架构。">
<meta name="application-name" content="LLM-DailyDigest">
<meta name="apple-mobile-web-app-title" content="LLM-DailyDigest"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" href="http://localhost:1313/LLMDailyDigestWeb/topic/%E6%96%B0%E6%A8%A1%E5%9E%8B/" /><link rel="prev" href="http://localhost:1313/LLMDailyDigestWeb/topic/%E6%99%BA%E8%83%BD%E4%BD%93/" /><link rel="next" href="http://localhost:1313/LLMDailyDigestWeb/topic/%E6%8E%A8%E7%90%86/" /><link rel="stylesheet" href="/LLMDailyDigestWeb/css/style.min.css"><link rel="preload" href="/LLMDailyDigestWeb/lib/fontawesome-free/all.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/LLMDailyDigestWeb/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/LLMDailyDigestWeb/lib/animate/animate.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/LLMDailyDigestWeb/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "新模型",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "http:\/\/localhost:1313\/LLMDailyDigestWeb\/topic\/%E6%96%B0%E6%A8%A1%E5%9E%8B\/"
    },"genre": "topic","wordcount":  449 ,
    "url": "http:\/\/localhost:1313\/LLMDailyDigestWeb\/topic\/%E6%96%B0%E6%A8%A1%E5%9E%8B\/","datePublished": "2025-09-15T00:00:00+08:00","dateModified": "2025-09-15T00:00:00+08:00","publisher": {
      "@type": "Organization",
      "name": ""},"author": {
        "@type": "Person",
        "name": "Author"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="sticky" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper" data-page-style="normal"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper">
    <div class="header-title">
      <a href="/LLMDailyDigestWeb/" title="LLM-DailyDigest"><img loading="lazy" src="/LLMDailyDigestWeb/fixit.svg" data-title="LLM-DailyDigest" data-alt="LLM-DailyDigest" class="logo" style="background: url(/LLMDailyDigestWeb/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}"/><span id="typeit-header-desktop" class="typeit"></span></a><span class="header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/LLMDailyDigestWeb/posts/llmdailydigest"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 详情</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/LLMDailyDigestWeb/resources/"
                
                
              >资源</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/LLMDailyDigestWeb/topic/"
                
                
              >主题</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/LLMDailyDigestWeb/updates/"
                
                
              >日报</a></li><li class="menu-item delimiter"></li><li class="menu-item theme-switch" title="Switch Theme">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li></ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/LLMDailyDigestWeb/" title="LLM-DailyDigest"><img loading="lazy" src="/LLMDailyDigestWeb/fixit.svg" data-title="/LLMDailyDigestWeb/fixit.svg" data-alt="/LLMDailyDigestWeb/fixit.svg" class="logo" style="background: url(/LLMDailyDigestWeb/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}"/><span id="typeit-header-title-mobile" class="typeit"></span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/LLMDailyDigestWeb/posts/llmdailydigest"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 详情</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/LLMDailyDigestWeb/resources/"
                  
                  
                >资源</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/LLMDailyDigestWeb/topic/"
                  
                  
                >主题</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/LLMDailyDigestWeb/updates/"
                  
                  
                >日报</a></li><li class="menu-item menu-system">
          <span class="menu-system-item theme-switch" title="Switch Theme"><i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i></span></li>
      </ul>
    </nav>
  </div>
</header><main class="container"><aside class="toc" id="toc-auto"><h2 class="toc-title">Contents&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden="true"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom">
    </aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX"><span>新模型</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      </span></span></div>
      <div class="post-meta-line"><span title="published on 2025-09-15 00:00:00"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden="true"></i><time datetime="2025-09-15">2025-09-15</time></span>&nbsp;<span title="Updated on 2025-09-15 00:00:00"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden="true"></i><time datetime="2025-09-15">2025-09-15</time></span>&nbsp;<span title="449 words"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden="true"></i>About 500 words</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden="true"></i>3 minutes</span>&nbsp;</div>
    </div><div class="details toc" id="toc-static" data-kept="false">
        <div class="details-summary toc-title">
          <span>Contents</span><i class="details-icon fa-solid fa-angle-right" aria-hidden="true"></i></div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#2025-09-15">2025-09-15</a></li>
    <li><a href="#2025-08-15">2025-08-15</a></li>
    <li><a href="#2025-0811">2025-0811</a>
      <ul>
        <li>
          <ul>
            <li><a href="#智谱glm-45v">智谱GLM-4.5V</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#2025-08-08">2025-08-08</a>
      <ul>
        <li>
          <ul>
            <li><a href="#gpt-5博士生水平">GPT-5：博士生水平</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#2025-08-07">2025-08-07</a></li>
    <li><a href="#2025-08-06">2025-08-06</a></li>
    <li><a href="#2025-07-28">2025-07-28</a>
      <ul>
        <li>
          <ul>
            <li><a href="#glm-45--智谱glm-45-系列-测评">GLM-4.5  <a href="https://mp.weixin.qq.com/s/N3OHlfyczY8PB_2IbO3dAQ?scene=1&amp;click_id=39">智谱GLM-4.5 系列 测评</a></a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#2025-7-26">2025-7-26</a></li>
    <li><a href="#历史">历史</a></li>
    <li><a href="#多模态模型">多模态模型</a>
      <ul>
        <li><a href="#vlm">VLM</a></li>
        <li><a href="#音频">音频</a></li>
        <li><a href="#智能体模型">智能体模型</a></li>
        <li><a href="#端侧模型">端侧模型</a>
          <ul>
            <li><a href="#谷歌版小钢炮开源027b大模型4个注意力头专为终端而生"><a href="https://mp.weixin.qq.com/s/UY7h2ZC_oeefrH_hWSKcag">谷歌版小钢炮开源！0.27B大模型，4个注意力头，专为终端而生</a></a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><h1 id="新模型">新模型</h1>
<h2 id="2025-09-15">2025-09-15</h2>
<p><a href="https://mp.weixin.qq.com/s/0NUx1Kokg5ueiDBd13te9g"target="_blank" rel="external nofollow noopener noreferrer">Qwen3-Max-Preview （Instruct）</a></p>
<h2 id="2025-08-15">2025-08-15</h2>
<p><a href="https://mp.weixin.qq.com/s/SvjtUE1IH1fV2kPmR2oCdQ"target="_blank" rel="external nofollow noopener noreferrer">实测Perplexity Pro平替模型，免费开源仅4B</a></p>
<h2 id="2025-0811">2025-0811</h2>
<h4 id="智谱glm-45v">智谱GLM-4.5V</h4>
<ol>
<li>相关素材
<ol>
<li>体验地址：https://chat.z.ai/</li>
<li>HuggingFace 开源地址：https://huggingface.co/zai-org/GLM-4.5V</li>
<li>GitHub 开源地址：https://github.com/zai-org/GLM-V</li>
<li>桌面助手下载地址：https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App</li>
<li>魔搭社区：https://modelscope.cn/collections/GLM-45V-8b471c8f97154e</li>
</ol>
</li>
<li>对图像的识别与推理、视频理解：GLM-4.5V 在涵盖图像理解、视频理解、GUI、文档理解等任务的 41 个公开视觉多模态榜单中综合效果达到了开源 SOTA 水平，这和我们在实测中体验到的结果是一致的。</li>
</ol>
<h2 id="2025-08-08">2025-08-08</h2>
<h4 id="gpt-5博士生水平">GPT-5：博士生水平</h4>
<ol>
<li>
<p>官方介绍：https://openai.com/index/introducing-gpt-5/</p>
<ol>
<li><strong>gpt-5</strong> ：专注逻辑推理和多步骤任务</li>
<li><strong>gpt-5-mini</strong> ：轻量级版本，成本敏感型应用</li>
<li><strong>gpt-5-nano</strong> ：速度优化版，超低延迟</li>
<li><strong>gpt-5-chat</strong> ：企业级多模态对话，支持上下文感知</li>
</ol>
</li>
<li>
<p>GPT-5 是一个一体化系统，包含三个核心部分：</p>
<ol>
<li>一个智能高效的基础模型，可解答大多数问题</li>
<li>一个深度推理模型（即GPT-5思维模块），用于处理更复杂的难题</li>
<li>以及一个实时路由模块，能够基于对话类型、问题复杂度、工具需求及用户显式指令（如prompt含“仔细思考这个问题”）智能调度模型</li>
</ol>
</li>
<li>
<p><a href="https://mp.weixin.qq.com/s/bKo5zwqCxdDXTch187tc2g"target="_blank" rel="external nofollow noopener noreferrer">快来看看GPT-5第一波实测</a></p>
<ol>
<li>ARC-AGI的成绩单表示GPT-5不如Grok 4</li>
<li>SimpleBench上，GPT-5的水平已经超过了人类平均水平，在大模型中尚属首次。这是一个简单常识推理类的数据集，主要特点就是对于人类非常简单，但对大模型比较困难。</li>
</ol>
</li>
<li>
<p><a href="https://mp.weixin.qq.com/s/ktVhcQ2gjbUMh5zX260ynA"target="_blank" rel="external nofollow noopener noreferrer">GPT-5来了！人人都能免费用，最强大模型只需最傻瓜式使用</a></p>
</li>
<li>
<p><a href="https://mp.weixin.qq.com/s/gVvvkiIFFT8GWZcVwhWS9Q"target="_blank" rel="external nofollow noopener noreferrer">GPT-5编程成绩有猫腻！自删23道测试题，关键基准还是自己提的</a></p>
</li>
</ol>
<h2 id="2025-08-07">2025-08-07</h2>
<ol>
<li><a href="https://mp.weixin.qq.com/s/No7YJsxrIWaVbFZXGd0pbQ"target="_blank" rel="external nofollow noopener noreferrer">端侧｜Qwen紧追OpenAI开源4B端侧大模型，AIME25得分超越Claude 4 Opus</a>
<ul>
<li>Qwen3-4B-Instruct-2507：非推理模型，大幅提升通用能力</li>
<li>Qwen3-4B-Thinking-2507：高级推理模型，专为专家级任务设计，逻辑、数学、科学及代码中的高级推理能力——专为专家级任务设计。</li>
<li>更智能、更精准，并且支持256k上下文，更具上下文感知能力。</li>
<li>抱抱脸直通车：
<ul>
<li>[1]https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507</li>
<li>[2]https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507</li>
</ul>
</li>
<li>魔搭社区直通车：
<ul>
<li>ttps://modelscope.cn/models/Qwen/Qwen3-4B-Instruct-2507</li>
<li><a href="https://modelscope.cn/models/Qwen/Qwen3-4B-Thinking-2507"target="_blank" rel="external nofollow noopener noreferrer">https://modelscope.cn/models/Qwen/Qwen3-4B-Thinking-2507</a></li>
</ul>
</li>
</ul>
</li>
<li>智能体｜Reflection AI已经发布了他们的首款AI智能体Asimov，较Claude Code Sonnet 4等模型，得到了用户更多偏好。
<ol>
<li>Asimov是一款专为代码理解打造的，它能对代码仓库、架构文档、GitHub讨论串、对话历史等多种信息进行索引，从而形成对代码库结构、历史及团队知识的全面认知。</li>
<li>Asimov <strong>并非单一智能体</strong> ，而是 <strong>由几个小型智能体协同工作</strong> 。</li>
</ol>
</li>
</ol>
<h2 id="2025-08-06">2025-08-06</h2>
<ol>
<li><strong>Claude Opus 4.1</strong>
<ol>
<li><a href="https://mp.weixin.qq.com/s/objwQLTeGWyrYnuy93aZFw"target="_blank" rel="external nofollow noopener noreferrer">Claude Opus 4.1火速发布！坐稳编程之王，官方：马上还有大更新</a>
<ol>
<li><strong>编程性能</strong>再次突破天花板，超越Claude Opus 4，拿下SOTA。
<ol>
<li><strong>在SWE-bench上，Opus 4.1超越Opus 4、Gemini 2.5 Pro、o3，将性能提升至74.5%，拿下新SOTA。</strong></li>
</ol>
</li>
<li><strong>Blog：</strong><a href="https://www.anthropic.com/news/claude-opus-4-1"target="_blank" rel="external nofollow noopener noreferrer">https://www.anthropic.com/news/claude-opus-4-1</a></li>
<li><strong>System Card：</strong><a href="https://assets.anthropic.com/m/4c024b86c698d3d4/original/Claude-4-1-System-Card.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://assets.anthropic.com/m/4c024b86c698d3d4/original/Claude-4-1-System-Card.pdf</a></li>
</ol>
</li>
</ol>
</li>
<li><strong>谷歌DeepMind发布了****新一代通用世界模型Genie 3</strong>
<ol>
<li><a href="https://mp.weixin.qq.com/s/ulhJGiiq301f1yuPRTl36g"target="_blank" rel="external nofollow noopener noreferrer">谷歌“世界模拟器”深夜上线！一句话生成3D世界，支持分钟级超长记忆</a>
<ol>
<li><strong>Genie 3相比上一代大幅升级，支持****720P画质，每秒24帧实时导航，以及分钟级的一致性保持</strong>。</li>
<li><strong>最让谷歌引以为傲的，还要属Genie 3的****长期环境一致性</strong>。</li>
<li><strong>DeepMind十多年来一直在关注模拟环境领域的研究，从训练智能体掌握实时战略游戏， 到开发用于开放式学习和机器人技术的模拟环境。</strong></li>
</ol>
</li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&amp;mid=2652617195&amp;idx=1&amp;sn=91f7f14b4c811e2a1cd9bb5cf5652a11&amp;scene=21#wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">谷歌推出「G」字号第三代世界模型Genie 3，号称「宇宙模拟器」，视频生成更加符合物理定律。</a></li>
</ol>
</li>
<li><strong>OpenAI开源两个推理模型：gpt-oss-120b</strong>和<strong>gpt-oss-20b</strong>。
<ol>
<li><a href="https://mp.weixin.qq.com/s/bIaUXw9XWR2Sb4dy4i37_Q"target="_blank" rel="external nofollow noopener noreferrer">刚刚，OpenAI开源2个推理模型：笔记本/手机就能跑，性能接近o4-mini</a>
<ol>
<li><strong>gpt-oss，即Open Source Series，意思就是“开源系列”。</strong></li>
<li><strong>gpt-oss-120b</strong>：1170亿参数（MoE架构，激活参数约51亿），可在单张80GB GPU上运行，性能接近闭源的o4-mini。</li>
<li><strong>gpt-oss-20b</strong>：210亿参数（Moe架构，激活参数约36亿），可在16GB内存的消费级设备上运行，性能接近o3-mini。</li>
<li><strong>整体来看，这两个模型在工具使用、少样本函数调用、链式思考推理（如Tau-Bench智能评估套件的结果所示）以及HealthBench上表现强劲，甚至超越了包括OpenAI o1和GPT‑4o在内的专有模型。</strong></li>
<li><strong>gpt-oss-120b每个token激活5.1B个参数，而gpt-oss-20b激活3.6B个参数。这些模型分别具有117b和21b的总参数。</strong></li>
<li><strong>技术博客地址：</strong><a href="https://openai.com/index/introducing-gpt-oss/"target="_blank" rel="external nofollow noopener noreferrer">https://openai.com/index/introducing-gpt-oss/</a></li>
<li><strong>HuggingFace地址：</strong><a href="https://huggingface.co/openai/gpt-oss-120b"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/openai/gpt-oss-120b</a></li>
<li><strong>GtiHub地址：</strong><a href="https://github.com/openai/gpt-oss"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/openai/gpt-oss</a></li>
<li>OpenAI-OSS-120B用起来要谨慎，写代码特别不稳定。OpenAI-OSS-20B在这个参数量大小下反而挺好。</li>
</ol>
</li>
<li><a href="https://mp.weixin.qq.com/s/U2TsYntvP9Hdlg6e9oUpoQ"target="_blank" rel="external nofollow noopener noreferrer">全网开测GPT-oss！技术架构也扒明白了</a>
<ol>
<li>
<p>GPT-oss在架构设计上既保留了MoE Transformer的核心架构，又通过细节优化提升性能、降低复杂度，使其成为适合开源模型的基础架构。</p>
<ul>
<li>对每个注意力头，设置一个可以学习的标量，然后进行softmax汇聚。</li>
<li>与GPT-3相同，交替使用滑动窗口层和全连接层。</li>
<li>对每个输入分配4个相关专家处理，再整合结果，专家之间彼此完全独立，同时使用标准负载均衡损失，确保资源高效分配。</li>
<li>使用了改进的swiglu激活函数，通过α=1.702让sigmoid的线性单元silu近似于高斯误差线性单元gelu。裁剪激活值防止梯度爆炸，通过调整“up+1”有助于梯度流动。</li>
<li>采用YaRN上下文窗口扩展技术，提升长文本处理能力。</li>
<li>移除了RMSNorm归一化过程中的可学习偏置参数，减少拟合风险。</li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
<li><strong>声音理解能力新SOTA</strong>，小米全量开源了模型<strong>MiDashengLM-7B</strong>，基于Xiaomi Dasheng作为音频编码器和Qwen2.5-Omni-7B Thinker作为自回归解码器，通过创新的通用音频描述训练策略，实现了对语音、环境声音和音乐的统一理解。<a href="https://mp.weixin.qq.com/s/NYyRBge-3eYEbXXTkx7AvA"target="_blank" rel="external nofollow noopener noreferrer">小米模型实现声音理解新SOTA！数据吞吐效率暴增20倍，推理速度快4倍 | 全量开源</a></li>
</ol>
<h2 id="2025-07-28">2025-07-28</h2>
<h4 id="glm-45--智谱glm-45-系列-测评">GLM-4.5  <a href="https://mp.weixin.qq.com/s/N3OHlfyczY8PB_2IbO3dAQ?scene=1&amp;click_id=39"target="_blank" rel="external nofollow noopener noreferrer">智谱GLM-4.5 系列 测评</a></h4>
<p><a href="https://mp.weixin.qq.com/s/4EAlA5mS3CIWCjJ7uZebbw"target="_blank" rel="external nofollow noopener noreferrer">智谱终于发布GLM-4.5技术报告，从预训练到后训练，细节大公开</a></p>
<ol>
<li>逻辑能力
<ol>
<li>幻觉过重：基础模式的幻觉是全方面的，不但对prompt输入本身存在幻觉，其输出有时也存在“梦游”现象，输出一些自己也不知道是什么的内容。比如在输出中试图引用一张图片来解释原理，但图片Url无法访问。大概是训练材料混入的脏数据。#42报告提炼问题，基础模式放弃了计算统计数据，使用占位符，对报告中核心观点的摘要也提炼不完整。在仅有的1pass中虽然计算了统计值，但计算错误。不过好在推理模式中，幻觉控制要好的多，没有出现类似问题。相关问题表现达到推理模型平均水准。</li>
<li>计算误差：受幻觉影响，基础模式数学计算误差显著偏高，以#38函数求交尤为典型，在kimi-k2的误差中，往往是小数点第3位之后的精度问题，按四舍五入算大体是对的。而GLM-4.5的误差体现在计算过程中小错误不断积累，最终答案只是看起来像，实际完全不对。同样由于推理模式对幻觉的抑制有效，同样题目在推理模式下准确率极高，#38题稳定满分，#42年报报告提炼问题中数据汇总部分也基本没有问题，偶有误差。</li>
<li>暴力倾向：对于复杂问题，基础模式2个版本有半数几率使用暴力穷举，在中等难度的#36六阶数独问题如此，在难题#23解密，#24数字规律等问题同样如此。在没有使用穷举的轮次中，二者均能正常响应，输出虽然不满分，但在同梯队中表现尚可的答案。值得一提的是，GLM前一个版本air-0414和Z1也因为过多使用暴力穷举，导致模型输出极易陷入死循环而耗尽Token，新版则只在少数（低于2%）输出死循环，大部分穷举Badcase是真的在穷举。<strong>推理模式</strong>下也存在同样问题，但概率较小，仅在个别高难度问题如#43目标数，#44工具组合，中有体现。</li>
</ol>
</li>
</ol>
<h2 id="2025-7-26">2025-7-26</h2>
<ol>
<li>
<p><a href="https://mp.weixin.qq.com/s/abvxxTefWkdRoddnREnAbg"target="_blank" rel="external nofollow noopener noreferrer">多模态卷王阶跃星辰Step 3登场，推理效率可达DeepSeek-R1 300%</a></p>
<ol>
<li>2025 WAIC大会上，阶跃星辰的新一代主力基座模型Step 3，带来了意想不到的惊喜。新一代旗舰基模Step 3的发布，标志着阶跃多模态大模型又一个新里程碑。</li>
<li>Step 3在MMMU、MathVision、SimpleVQA、AIME 2025、LiveCodeBench（2024.08-2025.05）等榜单上直接拿下了开源多模态推理模型的SOTA成绩。</li>
</ol>
</li>
<li>
<p><a href="https://mp.weixin.qq.com/s/3VkR9VBaSs6eMZEaWqfdYg"target="_blank" rel="external nofollow noopener noreferrer">九天大模型大变身：性能狂飙35%！还能一键P大象</a></p>
<ol>
<li>7月26日，在2025世界人工智能大会期间，中国移动焕新发布「九天」基础大模型3.0。本次发布的「九天」基础大模型3.0，重点聚焦模型的端到端技术升级以及生成可控性能力的增强，进一步强化九天大模型「高安全、高可控、全国产、全行业」的独特优势。</li>
</ol>
</li>
</ol>
<h2 id="历史">历史</h2>
<ol>
<li>2025-07-17 10:58:27 Thursday ｜  Kimi-2 已上线 LiveBench AI：超越 GPT-4.1，开源 AI 新王者诞生</li>
<li>2025-07-03 11:20:12 Thursday ｜ <a href="https://mp.weixin.qq.com/s/__VhGST5Qm_KI8yoc_d68A"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/__VhGST5Qm_KI8yoc_d68A</a></li>
</ol>
<p>Grok 4 (grok-4-0629)，则是一个更大、更智能的Thinking模型。官方宣称，这是他们最新、最强大的旗舰模型，在自然语言、数学和推理上性能无与伦比，是用户的最佳选择。</p>
<p>而Grok 4 Code（grok-4-code-0629）则专为编程而打造。你可以向它询问代码问题，甚至直接把它嵌入到自己的代码编辑器中，还可以一键在Cursor上使用。</p>
<ol start="3">
<li>2025-07-03 10:27:17 Thursday ｜ OpenRouter 上出现了一个神秘模型，该模型被命名为「Cypher Alpha」。其可以免费使用，100 万 token 上下文，还具有推理能力。 <a href="https://mp.weixin.qq.com/s/lmIQhT7uI9etjxGgIYqLoA"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/lmIQhT7uI9etjxGgIYqLoA</a></li>
<li>2025-07-02 14:09:31 Wednesday ｜SuperCLUE推理榜惊现黑马：原来中兴是一家AI公司？ <a href="https://mp.weixin.qq.com/s/H2urbOlVVcFR5b-GA7Rnhw"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/H2urbOlVVcFR5b-GA7Rnhw</a></li>
<li>2025-07-01 11:23:45 Tuesday ｜ 百度官宣文心大模型4.5系列正式开源，还同步提供API服务 <a href="https://mp.weixin.qq.com/s/jG0R66Uq_6kFwajb7XKM3w"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/jG0R66Uq_6kFwajb7XKM3w</a></li>
</ol>
<p>报告地址：https://yiyan.baidu.com/blog/publication/ERNIE_Technical_Report.pdf</p>
<ol start="6">
<li>2025-06-28 18:48:36 Saturday ｜ 一手实测有道14B「子曰3」数学模型，击败满血版DeepSeek R1 <a href="https://mp.weixin.qq.com/s/x56TbKXzijRxaJLXKvKV_g"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/x56TbKXzijRxaJLXKvKV_g</a></li>
</ol>
<p>网易有道开源了「 <strong>子曰3</strong> 」 <strong>数学模型</strong> （Confucius3-Math），以14B参数的轻量级模型在多项数学推理任务上超越了满血参数的DeepSeek-R1。</p>
<ol start="7">
<li>2025-06-27 14:04:27 Friday ｜AI秒懂短视频，快手大模型Keye-VL理解力爆表！技术细节全开源 <a href="https://mp.weixin.qq.com/s/hFO2TQNcn3IK3E1F1QQObw"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/hFO2TQNcn3IK3E1F1QQObw</a></li>
<li>2025-06-26 12:06:05 Thursday ｜ Gemini Robotics On-Device，谷歌 DeepMind 首个可以直接部署在机器人上的视觉-语言-动作（VLA）模型https://mp.weixin.qq.com/s/mjZAAvVtPevYDD5HfexN6g</li>
<li>2025-07-01 10:44:45 Tuesday ｜ 华为正式宣布开源盘古 70 亿参数的稠密模型「 <strong>盘古 Embedded</strong> 」、盘古 Pro MoE 720 亿参数的混合专家模型（参见机器之心报道：<a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650971050&amp;idx=1&amp;sn=93a499f2a2bcb83302ad201b8d193bda&amp;scene=142#wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">华为盘古首次露出，昇腾原生72B MoE架构，SuperCLUE千亿内模型并列国内第一</a> ）和基于昇腾的模型推理技术。
开源链接：https://gitcode.com/ascend-tribe</li>
</ol>
<p><a href="https://mp.weixin.qq.com/s/v1NNVaH9oDufqkrkyVLnVw"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/v1NNVaH9oDufqkrkyVLnVw</a></p>
<p>更重要的是，这些模型采用了一些领先的技术来实现高效的训练和推理，比如分组混合专家 MoGE 算法、自适应快慢思考合一以及全链路的高性能推理系统优化。</p>
<ol start="10">
<li>2025-06-23 11:46:54 Monday｜ 在华为开发者大会 2025（HDC 2025）上，华为重磅发布了盘古大模型 5.5 <a href="https://mp.weixin.qq.com/s/Ie824EYirtd3gqpog786Nw"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/Ie824EYirtd3gqpog786Nw</a>
<ol>
<li>盘古 Ultra MoE 的技术 报告地址：https://arxiv.org/pdf/2505.04519</li>
<li>盘古 Pro MoE 的技术报告 项目地址：https://gitcode.com/ascend-tribe/pangu-pro-moe</li>
<li>小模型**盘古 Embedding **报告地址：https://arxiv.org/pdf/2505.22375</li>
<li>华为发布了开放域信息获取 Agent—— **盘古 DeepDiver **报告地址：https://arxiv.org/pdf/2505.24332</li>
</ol>
</li>
<li>2025-06-19 20:05:29 Thursday ｜ MiniMax刚刚发布海螺2.0版本，能处理极端物理情况，原生支持1080P。</li>
<li>2025-06-19 19:40:54 Thursday ｜ Gemini 2.5 Pro 稳定版发布且已全面可用，其与 6 月 5 日的预览版相比无变化。
<ol>
<li>报告地址：https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf</li>
<li>AI玩宝可梦找出30年前代码Bug！谷歌论文介绍AI通关全过程，复杂任务都能解 <a href="https://mp.weixin.qq.com/s/fOGbijWnqEloziC3TISz_w"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/fOGbijWnqEloziC3TISz_w</a></li>
</ol>
</li>
<li>2025-06-17 11:20:40 Tuesday ｜开源代码模型 <strong>Kimi-Dev</strong> ，在SWE-bench Verified上以60.4%的成绩 <strong>取得开源SOTA：</strong> 项目主页：https://moonshotai.github.io/Kimi-Dev/ GitHub：https://github.com/MoonshotAI/Kimi-Dev HuggingFace：https://huggingface.co/moonshotai/Kimi-Dev-72B</li>
<li>2025-06-17 11:11:46 Tuesday ｜ MiniMax开源MiniMax-M1，目前模型权重已可在HuggingFace下载，技术报告同步公开。</li>
<li>2025-06-12 10:53:36 Thursday | 豆包大模型1.6发布</li>
<li>2025-06-11 11:15:20 Wednesday｜o3-pro发布，严格的「4/4 可靠性」评估，即只有在四次尝试中（而不仅仅是一次）正确回答问题，模型才被视为成功</li>
<li>20250606｜gemini-2.5-pro-0605发布</li>
<li>20250606｜Qwen3-Embedding系列发布：Qwen3-Embedding系列支持119种语言，涵盖主流自然语言及多种编程语言。</li>
</ol>
<h2 id="多模态模型">多模态模型</h2>
<p>2025-07-01 11:20:23 Tuesday ｜Black Forest Labs刚刚宣布开源旗舰图像模型 <strong>FLUX.1 Kontext[dev]</strong> ，专为图像编辑打造，还能直接在消费级芯片上运行。 <a href="https://mp.weixin.qq.com/s/Cu-58gySRJ0-bWCwO8ViuQ"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/Cu-58gySRJ0-bWCwO8ViuQ</a></p>
<p>2025-06-30 18:00:22 Monday ｜ 阿里多模态模型Qwen-VLo <a href="https://mp.weixin.qq.com/s/RiAnvEhp0lkPpC-ED24Tgw"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/RiAnvEhp0lkPpC-ED24Tgw</a></p>
<h3 id="vlm">VLM</h3>
<p>2025-07-03 10:59:19 Thursday ｜ 9B“小”模型干了票“大”的：性能超8倍参数模型，拿下23项SOTA | 智谱开源 <a href="https://mp.weixin.qq.com/s/5jcSAR6I7MyHc4INo7f9BQ"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/5jcSAR6I7MyHc4INo7f9BQ</a></p>
<p><strong>智谱发布并开源了一个仅9B大小的模型——GLM-4.1V-9B-Thinking</strong></p>
<p>引入了 <strong>思维链</strong> （Chain-of-Thought）推理机制，并通过 <strong>课程采样强化学习</strong> （RLCS，Reinforcement Learning with Curriculum Sampling）来全面提升模型能力。</p>
<p>团队采用“课程学习”的方式进行大规模强化训练，也就是先让模型从简单任务开始，逐步挑战更难的任务。通过这种由浅入深的训练策略，模型在实用性、准确性以及稳定性方面都有了明显的提升。</p>
<ol start="2">
<li>2025-07-03 11:01:58 Thursday ｜ 字节最新发布多主体控制生成模型 <strong>Xverse</strong> ——</li>
</ol>
<p>既可以对设定好的每个主体进行精确控制，也不会破坏图像的生成质量 <a href="https://mp.weixin.qq.com/s/JzuyHDfRGd-hFoL_VOXCAg"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/JzuyHDfRGd-hFoL_VOXCAg</a></p>
<p>XVerse的核心是通过学习DiT（Diffusion Transformer，一种扩散模型和Transformer架构的生成模型）中文本流调制机制中的偏移量，<strong>实现对多个主体身份和语义属性的****一致控制</strong>。</p>
<h3 id="音频">音频</h3>
<p>2025-07-02 14:54:01 Wednesday ｜ 阿里通义开源首个CoT音频模型，音·画同步被狠狠拿捏了 <a href="https://mp.weixin.qq.com/s/NPb2iQvAiTJb0LZG8CSdXg"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/NPb2iQvAiTJb0LZG8CSdXg</a></p>
<p>没错，这就是阿里通义语音团队最新开源的 <strong>泛音频生成模型ThinkSound</strong> ，主要用于视频配音，主打 <strong>让每一帧画面都有专属匹配音效</strong> 。</p>
<p>就在上个月，团队发布了语音生成大模型 <strong>Cosyvoice 3.0</strong> ，通过大规模数据预训练和特殊设计的强化学习后训练，它能提供多语言语音生成、零样本语音复刻等功能。</p>
<p>更早之前，团队还推出了基于模态对齐实现的端到端音频多模态大模型 <strong>MinMo</strong> 。</p>
<h3 id="智能体模型">智能体模型</h3>
<p>2025-06-28 17:18:09 Saturday ｜ AI自动修bug，解决率达44%！这是全球开源模型的最新最强水平。来自蚂蚁的开源新模型，在SWE-bench Lite上超越所有开源方案，性能媲美闭源模型。 <a href="https://mp.weixin.qq.com/s/Y-vqZG2dQMOwvXTinDbT1Q"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/Y-vqZG2dQMOwvXTinDbT1Q</a></p>
<h3 id="端侧模型">端侧模型</h3>
<p>vivo突破手机AI部署难题，绕开MoE架构限制，骁龙8 Elite流畅运行｜ICCV 2025<a href="https://mp.weixin.qq.com/s/ztTdARR4Q0opOGP139NQkQ"target="_blank" rel="external nofollow noopener noreferrer"> https://mp.weixin.qq.com/s/ztTdARR4Q0opOGP139NQkQ</a></p>
<p>2025-06-28 17:02:38 Saturday ｜ 最低仅需2G显存，谷歌开源端侧模型刷新竞技场纪录，原生支持图像视频 ，今天凌晨，谷歌正式官宣了 <strong>Gemma 3n</strong> ，原生支持文本、图像和音视频等多种模态。 <a href="https://mp.weixin.qq.com/s/iN4Fir3tSt96vPJufn5PSQ"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/iN4Fir3tSt96vPJufn5PSQ</a></p>
<p>模型、权重：https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4</p>
<p>文档：https://ai.google.dev/gemma/docs/gemma-3n</p>
<p>博客：https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/</p>
<ol start="3">
<li>2025-06-28 17:11:15 Saturday ｜ <a href="https://mp.weixin.qq.com/s/1jIi40A9Jm9zFLuuCuE3OA"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/1jIi40A9Jm9zFLuuCuE3OA</a></li>
</ol>
<p>早在两个月前的阿里云AI势能大会上，阿里云百炼就透露了要做Agent Store的计划。</p>
<p>现在，这个**<a href="https://bailian.console.aliyun.com/?spm=5176.29619931.J_AHgvE-XDhTWrtotIBlDQQ.13.74cd521cK99oYc&amp;tab=app#/app-market/newTemplate"target="_blank" rel="external nofollow noopener noreferrer">Agent Store正式上线</a>**了，提供了覆盖各行各业的上百个Agent模板。</p>
<p><strong>CGM的技术论文、核心代码、模型权重与训练数据均已开源</strong> ，感兴趣的同学可进一步了解详情。</p>
<ul>
<li>技术论文：https://arxiv.org/abs/2505.16901</li>
<li>开源代码：https://github.com/codefuse-ai/CodeFuse-CGM</li>
<li>模型权重：https://huggingface.co/codefuse-ai/CodeFuse-CGM-72B</li>
<li>训练数据：https://huggingface.co/datasets/codefuse-ai/CodeGraph</li>
</ul>
<p>😎团队此前工作：</p>
<ul>
<li>Code LLM综述：Awesome-Code-LLM（TMLR）https://github.com/codefuse-ai/Awesome-Code-LLM</li>
<li>Graph+LLM前序研究：GALLa（ACL 2025）https://github.com/codefuse-ai/GALLa</li>
<li>高效注意力架构：Rodimus（ICLR 2025）https://arxiv.org/abs/2410.06577</li>
<li>代码多任务微调框架：MFTCoder（KDD 2024）https://arxiv.org/abs/2311.02303</li>
</ul>
<h4 id="谷歌版小钢炮开源027b大模型4个注意力头专为终端而生"><a href="https://mp.weixin.qq.com/s/UY7h2ZC_oeefrH_hWSKcag"target="_blank" rel="external nofollow noopener noreferrer">谷歌版小钢炮开源！0.27B大模型，4个注意力头，专为终端而生</a></h4>
<p>2025-8-15</p>
<ol>
<li>谷歌开源Gemma 3 270M</li>
<li>值得一提的是，新模型只有 <strong>4个注意力头</strong> ，比Qwen 3 0.6B少12个，真是切实符合其轻量化的定位。</li>
<li>新模型的核心功能可概括为以下4部分：
<ol>
<li>紧凑且高效的架构</li>
<li>极致的能源效率</li>
<li>指令遵循</li>
<li>可用于生产的量化支持</li>
<li><a href="https://developers.googleblog.com/en/introducing-gemma-3-270m/"target="_blank" rel="external nofollow noopener noreferrer">https://developers.googleblog.com/en/introducing-gemma-3-270m/</a></li>
</ol>
</li>
</ol>
</div></article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">Public - 2025</span><span class="author" itemprop="copyrightHolder">
              <a href="/LLMDailyDigestWeb/"></a></span></div><div class="footer-line statistics"></div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="Back to Top"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric d-none">0%</span>
        </div></div><div id="mask"></div><noscript>
    <div class="noscript-warning">Theme FixIt works best with JavaScript enabled.</div>
  </noscript>
</div><link rel="preload" href="/LLMDailyDigestWeb/lib/katex/katex.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/LLMDailyDigestWeb/lib/katex/katex.min.css"></noscript><link rel="stylesheet" href="/LLMDailyDigestWeb/lib/cookieconsent/cookieconsent.min.css"><script src="/LLMDailyDigestWeb/lib/sharer/sharer.min.js" async defer></script><script src="/LLMDailyDigestWeb/lib/typeit/index.umd.js" defer></script><script src="/LLMDailyDigestWeb/lib/katex/katex.min.js" defer></script><script src="/LLMDailyDigestWeb/lib/katex/auto-render.min.js" defer></script><script src="/LLMDailyDigestWeb/lib/katex/copy-tex.min.js" defer></script><script src="/LLMDailyDigestWeb/lib/katex/mhchem.min.js" defer></script><script src="/LLMDailyDigestWeb/lib/cookieconsent/cookieconsent.min.js" defer></script><script>window.config={"code":{"copyTitle":"Copy to clipboard","editLockTitle":"Lock editable code block","editUnLockTitle":"Unlock editable code block","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-desktop":"LLM-DailyDigest 大模型研究日报","typeit-header-title-mobile":"LLM-DailyDigest 大模型研究日报"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-desktop":["typeit-header-desktop"],"typeit-header-title-mobile":["typeit-header-title-mobile"]},"duration":-1,"loop":false,"speed":100}};</script><script src="/LLMDailyDigestWeb/js/theme.min.js" defer></script></body>
</html>
