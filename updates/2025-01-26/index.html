<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>2025-01-26科研追新(公众号) - LLM-DailyDigest</title><meta name="author" content="">
<meta name="author-link" content="">
<meta name="description" content="2025-01-26
统计1-25-14:00～1-26-14:00的相关新进展
1. 公众号
1.1 量子位

  
      
          标题
          内容
          链接
      
  
  
      
          Open R1
          HuggingFace开源项目，目的是构建DeepSeek R1 pipeline中确实的部分，一遍所有人都能在此之上复制和构建R1。
          DeepSeek-R1持续刷屏，连Open R1都来了！抱抱脸发起，1天狂揽1.9k星 https://mp.weixin.qq.com/s/BX2iTak6bPAKdj6Lv1Lt3A
      
  

https://github.com/huggingface/open-r1
研究团队以DeepSeek-R1技术报告为指导，将整个复刻过程划分为三个关键步骤。

**步骤 1：**通过从DeepSeek-R1蒸馏高质量语料库，复现R1-Distill模型。
**步骤 2：**复现DeepSeek用于创建R1-Zero的纯强化学习（RL）流程。这可能需要为数学、推理和代码任务策划新的大规模数据集。
**步骤 3：**展示我们如何通过多阶段训练，从基础模型发展到经过RL调优的模型。

2. Arxiv
只显示1.24周五的相关工作
2.1 计算和语言
研究主要探讨了大型语言模型（LLMs）在如下领域的应用和挑战，包括参数高效微调、几何结构理解、数学推理评估、领域调优、长期计划生成、自引用因果循环、代码转换、模型一致性、预训练优化、干扰项生成、从弱到强的概括、零样本验证、双链思维链以及推理能力量化。

  
      
          标题
          内容
          链接
      
  
  
      
          基础模型的参数高效微调
          本调查旨在全面概述应用于不同 FM 的 PEFT 技术，并解决理解技术、趋势和应用方面的关键差距
          https://arxiv.org/abs/2501.13787
      
      
          大型语言模型真的理解几何结构吗？
          1. 引入了 GeomRel 数据集，旨在通过分离问题解决中几何关系识别的核心步骤来评估LLMs对几何结构的理解；2. 进一步提出了几何思维链 （GeoCoT） 方法，该方法增强了LLMs识别几何关系的能力，从而显着提高了性能
          https://arxiv.org/abs/2501.13773
      
      
          UGMathBench：使用大型语言模型进行本科生数学推理的多样化和动态基准
          引入了 UGMathBench，这是一个多样化且动态的基准测试，专门用于评估本科生水平的数学推理
          https://arxiv.org/abs/2501.13766
      
      
          如何在保持通用能力的同时完成域调优LLM：自适应分层和元素正则化
          1. 利用双目标优化策略：（1） 正则化损失以保留对一般知识至关重要的参数;（2） 交叉熵损失以适应特定领域的任务。；2. 引入了逐层系数来解释不同层的不同贡献，动态平衡双目标优化
          https://arxiv.org/abs/2501.13669
      
      
          LLMs只有我们告诉他们才能计划
          是否可以LLMs独立生成与人类基线相媲美的长期计划。我们对思想算法 （AoT） 的新颖增强，我们称之为 AoT&#43;，有助于在规划基准方面取得最先进的结果，从而超越了竞争性先前的方法和人类基线
          https://arxiv.org/abs/2501.13545
      
      
          回顾：语言模型中的类似库的行为通过自引用因果循环得到增强
          引入了自引用因果循环（缩写为 RECALL）的概念——一种使大型语言模型 能够绕过单向因果关系限制的机制，这是被称为反转诅咒的现象的基础
          https://arxiv.org/abs/2501.13491
      
      
          代码转换阿拉伯语 NLP 调查：进展、挑战和未来方向
          多样化的语言景观导致了阿拉伯语变体内部以及阿拉伯语和外语之间的代码转换。代码转换在整个地区的广泛发生使得在开发语言技术时满足这些语言需求变得至关重要。在本文中，我们回顾了代码转换阿拉伯语 NLP 领域的当前文献，为正在进行的努力、挑战、研究差距提供了广阔的视角，并对未来的研究方向提出了建议。
          https://arxiv.org/abs/2501.13419
      
      
          按照我们所做的做，而不是按照您的想法做：大型语言模型的一致性
          引入了 BenchForm，这是一种新的面向一致性的基准，具有推理密集型任务和五种不同的交互协议，旨在探测LLMs协作场景中的行为。
          https://arxiv.org/abs/2501.13381
      
      
          偏好课程：LLMs应始终使用其首选数据进行预训练
          目前的大型语言模型（LLMs）通常在整个预训练过程中使用一致的数据分布。然而，随着模型能力的提高，它应该直观地用差异化的数据进行预训练。为了实现这一目标，我们提出了基于困惑差异的偏好课程学习（PDPC）框架，该框架始终感知并使用首选数据LLMs来训练和提升它们。首先，我们引入PD指标来衡量强模型和弱模型对样本的拟合程度的差异。PD高的样本对弱模型来说学习起来更具挑战性，更适合在预训练的后期进行安排。其次，我们提出了PD偏好函数来逼近模型，随时预测模型的数据偏好LLM，从而离线完成整个数据的排列，保证连续训练不中断。
          https://arxiv.org/abs/2501.13126
      
      
          通过学生选择预测为多项选择题生成合理的干扰项
          本研究提出了一个管道，用于训练模型以生成更有可能被学生选择的干扰项。首先，我们训练一个成对排名器来推理学生的误解并评估两个干扰项的相对合理性。使用这个模型，我们创建了一个成对干扰项排名的数据集，然后通过直接偏好优化 （DPO） 训练干扰项生成器，以生成更合理的干扰项。
          https://arxiv.org/abs/2501.13125
      
      
          辩论有助于从弱到强的概括
          将已经有能力的模型与所需行为保持一致的常见方法依赖于人类提供监督的能力。然而，未来的超人模型将超过人类的能力。因此，人类将只能弱地监督超人模型。人类评估的这种预期缺陷将削弱未来人工智能系统的安全性。可扩展的监督和从弱到强的泛化是解决这个问题的两种互补方法。在本文中，我们试图结合这两种方法的优势来进一步提高对齐。具体来说，我们研究了用强预训练模型改进人类监督的方法，然后用增强的弱人类监督来监督强模型。为了进行迭代实证进展，我们考虑一个类比：我们可以使用强模型来改进弱模型监督，然后用它来监督强模型吗？我们通过在大型强模型的额外帮助下在地面实况标签上微调一个小的弱模型来实证测试它，然后在弱模型生成的标签上微调强模型。我们发现，辩论可以帮助弱模型从不可信的强模型中提取可信信息，从而在训练弱模型时提供杠杆作为样本的上下文。我们还表明，弱模型的集合有助于利用强模型辩论者生成的长论点并获得更稳健的监督估计。对 OpenAI 从弱到强的 NLP 基准测试的广泛实验表明，组合方法会导致更好的对齐，这表明辩论有可能帮助从弱到强的泛化。
          https://arxiv.org/abs/2501.13124
      
      
          零样本验证引导的思路链
          专注于LLM在完全零镜头状态下通过 COT 提示对自生成推理步骤进行基于自我验证
          https://arxiv.org/abs/2501.13122
      
      
          MyGO Multiplex CoT：一种通过双链思维在大型语言模型中进行自我反思的方
          Multiplex CoT（思维链），这是一种通过启动双思维链 （CoT） 思维来模拟LLMs推理时自我审查的方法。Multiplex CoT 利用迭代推理的力量，其中模型生成初始思维链，然后通过第二轮思维生成来批评和完善该推理。这种递归方法可以得到更连贯、更合乎逻辑和更稳健的答案，从而改善整体决策过程。——我们演示了如何在现有LLM架构中使用简单的提示工程有效地实现这种方法，从而获得类似于学习-细化模型 （LRM） 的效果，而无需额外的培训。
          https://arxiv.org/abs/2501.13117
      
      
          关于 AI 模型的推理能力以及如何量化它
          以多项选择推理任务中的位置偏差为案例研究，我们展示了系统性扰动如何揭示模型决策的基本方面。为了分析这些行为，我们开发了两个互补的现象学模型：一个概率混合模型 （PMM），它将模型响应分解为推理、记忆和猜测组件，以及一个信息论一致性 （ITC） 分析，量化模型置信度和策略选择之间的关系。通过对推理基准的对照实验，我们表明真正的推理对于当前的模型来说仍然具有挑战性，明显的成功往往依赖于记忆和模式匹配的复杂组合，而不是真正的逻辑推理。更根本的是，我们证明了准确性本身往往夸大了模型的推理能力，因为模型行为可以通过认知策略相空间中的潜在机制来表征，揭示了模型在响应查询时如何动态平衡不同的方法。 此框架为实际部署启用定量标准，允许应用程序根据策略分布而不是聚合性能指标指定可靠性阈值。
          https://arxiv.org/abs/2501.13833
      
  
" /><meta name="keywords" content='Hugo, FixIt' />
  <meta itemprop="name" content="2025-01-26科研追新(公众号)">
  <meta itemprop="description" content="2025-01-26 统计1-25-14:00～1-26-14:00的相关新进展
1. 公众号 1.1 量子位 标题 内容 链接 Open R1 HuggingFace开源项目，目的是构建DeepSeek R1 pipeline中确实的部分，一遍所有人都能在此之上复制和构建R1。 DeepSeek-R1持续刷屏，连Open R1都来了！抱抱脸发起，1天狂揽1.9k星 https://mp.weixin.qq.com/s/BX2iTak6bPAKdj6Lv1Lt3A https://github.com/huggingface/open-r1
研究团队以DeepSeek-R1技术报告为指导，将整个复刻过程划分为三个关键步骤。
**步骤 1：**通过从DeepSeek-R1蒸馏高质量语料库，复现R1-Distill模型。 **步骤 2：**复现DeepSeek用于创建R1-Zero的纯强化学习（RL）流程。这可能需要为数学、推理和代码任务策划新的大规模数据集。 **步骤 3：**展示我们如何通过多阶段训练，从基础模型发展到经过RL调优的模型。 2. Arxiv 只显示1.24周五的相关工作
2.1 计算和语言 研究主要探讨了大型语言模型（LLMs）在如下领域的应用和挑战，包括参数高效微调、几何结构理解、数学推理评估、领域调优、长期计划生成、自引用因果循环、代码转换、模型一致性、预训练优化、干扰项生成、从弱到强的概括、零样本验证、双链思维链以及推理能力量化。
标题 内容 链接 基础模型的参数高效微调 本调查旨在全面概述应用于不同 FM 的 PEFT 技术，并解决理解技术、趋势和应用方面的关键差距 https://arxiv.org/abs/2501.13787 大型语言模型真的理解几何结构吗？ 1. 引入了 GeomRel 数据集，旨在通过分离问题解决中几何关系识别的核心步骤来评估LLMs对几何结构的理解；2. 进一步提出了几何思维链 （GeoCoT） 方法，该方法增强了LLMs识别几何关系的能力，从而显着提高了性能 https://arxiv.org/abs/2501.13773 UGMathBench：使用大型语言模型进行本科生数学推理的多样化和动态基准 引入了 UGMathBench，这是一个多样化且动态的基准测试，专门用于评估本科生水平的数学推理 https://arxiv.org/abs/2501.13766 如何在保持通用能力的同时完成域调优LLM：自适应分层和元素正则化 1. 利用双目标优化策略：（1） 正则化损失以保留对一般知识至关重要的参数;（2） 交叉熵损失以适应特定领域的任务。；2. 引入了逐层系数来解释不同层的不同贡献，动态平衡双目标优化 https://arxiv.org/abs/2501.13669 LLMs只有我们告诉他们才能计划 是否可以LLMs独立生成与人类基线相媲美的长期计划。我们对思想算法 （AoT） 的新颖增强，我们称之为 AoT&#43;，有助于在规划基准方面取得最先进的结果，从而超越了竞争性先前的方法和人类基线 https://arxiv.org/abs/2501.13545 回顾：语言模型中的类似库的行为通过自引用因果循环得到增强 引入了自引用因果循环（缩写为 RECALL）的概念——一种使大型语言模型 能够绕过单向因果关系限制的机制，这是被称为反转诅咒的现象的基础 https://arxiv.org/abs/2501.13491 代码转换阿拉伯语 NLP 调查：进展、挑战和未来方向 多样化的语言景观导致了阿拉伯语变体内部以及阿拉伯语和外语之间的代码转换。代码转换在整个地区的广泛发生使得在开发语言技术时满足这些语言需求变得至关重要。在本文中，我们回顾了代码转换阿拉伯语 NLP 领域的当前文献，为正在进行的努力、挑战、研究差距提供了广阔的视角，并对未来的研究方向提出了建议。 https://arxiv.org/abs/2501.13419 按照我们所做的做，而不是按照您的想法做：大型语言模型的一致性 引入了 BenchForm，这是一种新的面向一致性的基准，具有推理密集型任务和五种不同的交互协议，旨在探测LLMs协作场景中的行为。 https://arxiv.org/abs/2501.13381 偏好课程：LLMs应始终使用其首选数据进行预训练 目前的大型语言模型（LLMs）通常在整个预训练过程中使用一致的数据分布。然而，随着模型能力的提高，它应该直观地用差异化的数据进行预训练。为了实现这一目标，我们提出了基于困惑差异的偏好课程学习（PDPC）框架，该框架始终感知并使用首选数据LLMs来训练和提升它们。首先，我们引入PD指标来衡量强模型和弱模型对样本的拟合程度的差异。PD高的样本对弱模型来说学习起来更具挑战性，更适合在预训练的后期进行安排。其次，我们提出了PD偏好函数来逼近模型，随时预测模型的数据偏好LLM，从而离线完成整个数据的排列，保证连续训练不中断。 https://arxiv.org/abs/2501.13126 通过学生选择预测为多项选择题生成合理的干扰项 本研究提出了一个管道，用于训练模型以生成更有可能被学生选择的干扰项。首先，我们训练一个成对排名器来推理学生的误解并评估两个干扰项的相对合理性。使用这个模型，我们创建了一个成对干扰项排名的数据集，然后通过直接偏好优化 （DPO） 训练干扰项生成器，以生成更合理的干扰项。 https://arxiv.org/abs/2501.13125 辩论有助于从弱到强的概括 将已经有能力的模型与所需行为保持一致的常见方法依赖于人类提供监督的能力。然而，未来的超人模型将超过人类的能力。因此，人类将只能弱地监督超人模型。人类评估的这种预期缺陷将削弱未来人工智能系统的安全性。可扩展的监督和从弱到强的泛化是解决这个问题的两种互补方法。在本文中，我们试图结合这两种方法的优势来进一步提高对齐。具体来说，我们研究了用强预训练模型改进人类监督的方法，然后用增强的弱人类监督来监督强模型。为了进行迭代实证进展，我们考虑一个类比：我们可以使用强模型来改进弱模型监督，然后用它来监督强模型吗？我们通过在大型强模型的额外帮助下在地面实况标签上微调一个小的弱模型来实证测试它，然后在弱模型生成的标签上微调强模型。我们发现，辩论可以帮助弱模型从不可信的强模型中提取可信信息，从而在训练弱模型时提供杠杆作为样本的上下文。我们还表明，弱模型的集合有助于利用强模型辩论者生成的长论点并获得更稳健的监督估计。对 OpenAI 从弱到强的 NLP 基准测试的广泛实验表明，组合方法会导致更好的对齐，这表明辩论有可能帮助从弱到强的泛化。 https://arxiv.org/abs/2501.13124 零样本验证引导的思路链 专注于LLM在完全零镜头状态下通过 COT 提示对自生成推理步骤进行基于自我验证 https://arxiv.org/abs/2501.13122 MyGO Multiplex CoT：一种通过双链思维在大型语言模型中进行自我反思的方 Multiplex CoT（思维链），这是一种通过启动双思维链 （CoT） 思维来模拟LLMs推理时自我审查的方法。Multiplex CoT 利用迭代推理的力量，其中模型生成初始思维链，然后通过第二轮思维生成来批评和完善该推理。这种递归方法可以得到更连贯、更合乎逻辑和更稳健的答案，从而改善整体决策过程。——我们演示了如何在现有LLM架构中使用简单的提示工程有效地实现这种方法，从而获得类似于学习-细化模型 （LRM） 的效果，而无需额外的培训。 https://arxiv.org/abs/2501.13117 关于 AI 模型的推理能力以及如何量化它 以多项选择推理任务中的位置偏差为案例研究，我们展示了系统性扰动如何揭示模型决策的基本方面。为了分析这些行为，我们开发了两个互补的现象学模型：一个概率混合模型 （PMM），它将模型响应分解为推理、记忆和猜测组件，以及一个信息论一致性 （ITC） 分析，量化模型置信度和策略选择之间的关系。通过对推理基准的对照实验，我们表明真正的推理对于当前的模型来说仍然具有挑战性，明显的成功往往依赖于记忆和模式匹配的复杂组合，而不是真正的逻辑推理。更根本的是，我们证明了准确性本身往往夸大了模型的推理能力，因为模型行为可以通过认知策略相空间中的潜在机制来表征，揭示了模型在响应查询时如何动态平衡不同的方法。 此框架为实际部署启用定量标准，允许应用程序根据策略分布而不是聚合性能指标指定可靠性阈值。 https://arxiv.org/abs/2501.13833">
  <meta itemprop="datePublished" content="2025-01-26T00:00:00+08:00">
  <meta itemprop="dateModified" content="2025-01-26T00:00:00+08:00">
  <meta itemprop="wordCount" content="124"><meta property="og:url" content="http://localhost:1313/updates/2025-01-26/">
  <meta property="og:site_name" content="LLM-DailyDigest">
  <meta property="og:title" content="2025-01-26科研追新(公众号)">
  <meta property="og:description" content="2025-01-26 统计1-25-14:00～1-26-14:00的相关新进展
1. 公众号 1.1 量子位 标题 内容 链接 Open R1 HuggingFace开源项目，目的是构建DeepSeek R1 pipeline中确实的部分，一遍所有人都能在此之上复制和构建R1。 DeepSeek-R1持续刷屏，连Open R1都来了！抱抱脸发起，1天狂揽1.9k星 https://mp.weixin.qq.com/s/BX2iTak6bPAKdj6Lv1Lt3A https://github.com/huggingface/open-r1
研究团队以DeepSeek-R1技术报告为指导，将整个复刻过程划分为三个关键步骤。
**步骤 1：**通过从DeepSeek-R1蒸馏高质量语料库，复现R1-Distill模型。 **步骤 2：**复现DeepSeek用于创建R1-Zero的纯强化学习（RL）流程。这可能需要为数学、推理和代码任务策划新的大规模数据集。 **步骤 3：**展示我们如何通过多阶段训练，从基础模型发展到经过RL调优的模型。 2. Arxiv 只显示1.24周五的相关工作
2.1 计算和语言 研究主要探讨了大型语言模型（LLMs）在如下领域的应用和挑战，包括参数高效微调、几何结构理解、数学推理评估、领域调优、长期计划生成、自引用因果循环、代码转换、模型一致性、预训练优化、干扰项生成、从弱到强的概括、零样本验证、双链思维链以及推理能力量化。
标题 内容 链接 基础模型的参数高效微调 本调查旨在全面概述应用于不同 FM 的 PEFT 技术，并解决理解技术、趋势和应用方面的关键差距 https://arxiv.org/abs/2501.13787 大型语言模型真的理解几何结构吗？ 1. 引入了 GeomRel 数据集，旨在通过分离问题解决中几何关系识别的核心步骤来评估LLMs对几何结构的理解；2. 进一步提出了几何思维链 （GeoCoT） 方法，该方法增强了LLMs识别几何关系的能力，从而显着提高了性能 https://arxiv.org/abs/2501.13773 UGMathBench：使用大型语言模型进行本科生数学推理的多样化和动态基准 引入了 UGMathBench，这是一个多样化且动态的基准测试，专门用于评估本科生水平的数学推理 https://arxiv.org/abs/2501.13766 如何在保持通用能力的同时完成域调优LLM：自适应分层和元素正则化 1. 利用双目标优化策略：（1） 正则化损失以保留对一般知识至关重要的参数;（2） 交叉熵损失以适应特定领域的任务。；2. 引入了逐层系数来解释不同层的不同贡献，动态平衡双目标优化 https://arxiv.org/abs/2501.13669 LLMs只有我们告诉他们才能计划 是否可以LLMs独立生成与人类基线相媲美的长期计划。我们对思想算法 （AoT） 的新颖增强，我们称之为 AoT&#43;，有助于在规划基准方面取得最先进的结果，从而超越了竞争性先前的方法和人类基线 https://arxiv.org/abs/2501.13545 回顾：语言模型中的类似库的行为通过自引用因果循环得到增强 引入了自引用因果循环（缩写为 RECALL）的概念——一种使大型语言模型 能够绕过单向因果关系限制的机制，这是被称为反转诅咒的现象的基础 https://arxiv.org/abs/2501.13491 代码转换阿拉伯语 NLP 调查：进展、挑战和未来方向 多样化的语言景观导致了阿拉伯语变体内部以及阿拉伯语和外语之间的代码转换。代码转换在整个地区的广泛发生使得在开发语言技术时满足这些语言需求变得至关重要。在本文中，我们回顾了代码转换阿拉伯语 NLP 领域的当前文献，为正在进行的努力、挑战、研究差距提供了广阔的视角，并对未来的研究方向提出了建议。 https://arxiv.org/abs/2501.13419 按照我们所做的做，而不是按照您的想法做：大型语言模型的一致性 引入了 BenchForm，这是一种新的面向一致性的基准，具有推理密集型任务和五种不同的交互协议，旨在探测LLMs协作场景中的行为。 https://arxiv.org/abs/2501.13381 偏好课程：LLMs应始终使用其首选数据进行预训练 目前的大型语言模型（LLMs）通常在整个预训练过程中使用一致的数据分布。然而，随着模型能力的提高，它应该直观地用差异化的数据进行预训练。为了实现这一目标，我们提出了基于困惑差异的偏好课程学习（PDPC）框架，该框架始终感知并使用首选数据LLMs来训练和提升它们。首先，我们引入PD指标来衡量强模型和弱模型对样本的拟合程度的差异。PD高的样本对弱模型来说学习起来更具挑战性，更适合在预训练的后期进行安排。其次，我们提出了PD偏好函数来逼近模型，随时预测模型的数据偏好LLM，从而离线完成整个数据的排列，保证连续训练不中断。 https://arxiv.org/abs/2501.13126 通过学生选择预测为多项选择题生成合理的干扰项 本研究提出了一个管道，用于训练模型以生成更有可能被学生选择的干扰项。首先，我们训练一个成对排名器来推理学生的误解并评估两个干扰项的相对合理性。使用这个模型，我们创建了一个成对干扰项排名的数据集，然后通过直接偏好优化 （DPO） 训练干扰项生成器，以生成更合理的干扰项。 https://arxiv.org/abs/2501.13125 辩论有助于从弱到强的概括 将已经有能力的模型与所需行为保持一致的常见方法依赖于人类提供监督的能力。然而，未来的超人模型将超过人类的能力。因此，人类将只能弱地监督超人模型。人类评估的这种预期缺陷将削弱未来人工智能系统的安全性。可扩展的监督和从弱到强的泛化是解决这个问题的两种互补方法。在本文中，我们试图结合这两种方法的优势来进一步提高对齐。具体来说，我们研究了用强预训练模型改进人类监督的方法，然后用增强的弱人类监督来监督强模型。为了进行迭代实证进展，我们考虑一个类比：我们可以使用强模型来改进弱模型监督，然后用它来监督强模型吗？我们通过在大型强模型的额外帮助下在地面实况标签上微调一个小的弱模型来实证测试它，然后在弱模型生成的标签上微调强模型。我们发现，辩论可以帮助弱模型从不可信的强模型中提取可信信息，从而在训练弱模型时提供杠杆作为样本的上下文。我们还表明，弱模型的集合有助于利用强模型辩论者生成的长论点并获得更稳健的监督估计。对 OpenAI 从弱到强的 NLP 基准测试的广泛实验表明，组合方法会导致更好的对齐，这表明辩论有可能帮助从弱到强的泛化。 https://arxiv.org/abs/2501.13124 零样本验证引导的思路链 专注于LLM在完全零镜头状态下通过 COT 提示对自生成推理步骤进行基于自我验证 https://arxiv.org/abs/2501.13122 MyGO Multiplex CoT：一种通过双链思维在大型语言模型中进行自我反思的方 Multiplex CoT（思维链），这是一种通过启动双思维链 （CoT） 思维来模拟LLMs推理时自我审查的方法。Multiplex CoT 利用迭代推理的力量，其中模型生成初始思维链，然后通过第二轮思维生成来批评和完善该推理。这种递归方法可以得到更连贯、更合乎逻辑和更稳健的答案，从而改善整体决策过程。——我们演示了如何在现有LLM架构中使用简单的提示工程有效地实现这种方法，从而获得类似于学习-细化模型 （LRM） 的效果，而无需额外的培训。 https://arxiv.org/abs/2501.13117 关于 AI 模型的推理能力以及如何量化它 以多项选择推理任务中的位置偏差为案例研究，我们展示了系统性扰动如何揭示模型决策的基本方面。为了分析这些行为，我们开发了两个互补的现象学模型：一个概率混合模型 （PMM），它将模型响应分解为推理、记忆和猜测组件，以及一个信息论一致性 （ITC） 分析，量化模型置信度和策略选择之间的关系。通过对推理基准的对照实验，我们表明真正的推理对于当前的模型来说仍然具有挑战性，明显的成功往往依赖于记忆和模式匹配的复杂组合，而不是真正的逻辑推理。更根本的是，我们证明了准确性本身往往夸大了模型的推理能力，因为模型行为可以通过认知策略相空间中的潜在机制来表征，揭示了模型在响应查询时如何动态平衡不同的方法。 此框架为实际部署启用定量标准，允许应用程序根据策略分布而不是聚合性能指标指定可靠性阈值。 https://arxiv.org/abs/2501.13833">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="updates">
    <meta property="article:published_time" content="2025-01-26T00:00:00+08:00">
    <meta property="article:modified_time" content="2025-01-26T00:00:00+08:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="2025-01-26科研追新(公众号)">
  <meta name="twitter:description" content="2025-01-26 统计1-25-14:00～1-26-14:00的相关新进展
1. 公众号 1.1 量子位 标题 内容 链接 Open R1 HuggingFace开源项目，目的是构建DeepSeek R1 pipeline中确实的部分，一遍所有人都能在此之上复制和构建R1。 DeepSeek-R1持续刷屏，连Open R1都来了！抱抱脸发起，1天狂揽1.9k星 https://mp.weixin.qq.com/s/BX2iTak6bPAKdj6Lv1Lt3A https://github.com/huggingface/open-r1
研究团队以DeepSeek-R1技术报告为指导，将整个复刻过程划分为三个关键步骤。
**步骤 1：**通过从DeepSeek-R1蒸馏高质量语料库，复现R1-Distill模型。 **步骤 2：**复现DeepSeek用于创建R1-Zero的纯强化学习（RL）流程。这可能需要为数学、推理和代码任务策划新的大规模数据集。 **步骤 3：**展示我们如何通过多阶段训练，从基础模型发展到经过RL调优的模型。 2. Arxiv 只显示1.24周五的相关工作
2.1 计算和语言 研究主要探讨了大型语言模型（LLMs）在如下领域的应用和挑战，包括参数高效微调、几何结构理解、数学推理评估、领域调优、长期计划生成、自引用因果循环、代码转换、模型一致性、预训练优化、干扰项生成、从弱到强的概括、零样本验证、双链思维链以及推理能力量化。
标题 内容 链接 基础模型的参数高效微调 本调查旨在全面概述应用于不同 FM 的 PEFT 技术，并解决理解技术、趋势和应用方面的关键差距 https://arxiv.org/abs/2501.13787 大型语言模型真的理解几何结构吗？ 1. 引入了 GeomRel 数据集，旨在通过分离问题解决中几何关系识别的核心步骤来评估LLMs对几何结构的理解；2. 进一步提出了几何思维链 （GeoCoT） 方法，该方法增强了LLMs识别几何关系的能力，从而显着提高了性能 https://arxiv.org/abs/2501.13773 UGMathBench：使用大型语言模型进行本科生数学推理的多样化和动态基准 引入了 UGMathBench，这是一个多样化且动态的基准测试，专门用于评估本科生水平的数学推理 https://arxiv.org/abs/2501.13766 如何在保持通用能力的同时完成域调优LLM：自适应分层和元素正则化 1. 利用双目标优化策略：（1） 正则化损失以保留对一般知识至关重要的参数;（2） 交叉熵损失以适应特定领域的任务。；2. 引入了逐层系数来解释不同层的不同贡献，动态平衡双目标优化 https://arxiv.org/abs/2501.13669 LLMs只有我们告诉他们才能计划 是否可以LLMs独立生成与人类基线相媲美的长期计划。我们对思想算法 （AoT） 的新颖增强，我们称之为 AoT&#43;，有助于在规划基准方面取得最先进的结果，从而超越了竞争性先前的方法和人类基线 https://arxiv.org/abs/2501.13545 回顾：语言模型中的类似库的行为通过自引用因果循环得到增强 引入了自引用因果循环（缩写为 RECALL）的概念——一种使大型语言模型 能够绕过单向因果关系限制的机制，这是被称为反转诅咒的现象的基础 https://arxiv.org/abs/2501.13491 代码转换阿拉伯语 NLP 调查：进展、挑战和未来方向 多样化的语言景观导致了阿拉伯语变体内部以及阿拉伯语和外语之间的代码转换。代码转换在整个地区的广泛发生使得在开发语言技术时满足这些语言需求变得至关重要。在本文中，我们回顾了代码转换阿拉伯语 NLP 领域的当前文献，为正在进行的努力、挑战、研究差距提供了广阔的视角，并对未来的研究方向提出了建议。 https://arxiv.org/abs/2501.13419 按照我们所做的做，而不是按照您的想法做：大型语言模型的一致性 引入了 BenchForm，这是一种新的面向一致性的基准，具有推理密集型任务和五种不同的交互协议，旨在探测LLMs协作场景中的行为。 https://arxiv.org/abs/2501.13381 偏好课程：LLMs应始终使用其首选数据进行预训练 目前的大型语言模型（LLMs）通常在整个预训练过程中使用一致的数据分布。然而，随着模型能力的提高，它应该直观地用差异化的数据进行预训练。为了实现这一目标，我们提出了基于困惑差异的偏好课程学习（PDPC）框架，该框架始终感知并使用首选数据LLMs来训练和提升它们。首先，我们引入PD指标来衡量强模型和弱模型对样本的拟合程度的差异。PD高的样本对弱模型来说学习起来更具挑战性，更适合在预训练的后期进行安排。其次，我们提出了PD偏好函数来逼近模型，随时预测模型的数据偏好LLM，从而离线完成整个数据的排列，保证连续训练不中断。 https://arxiv.org/abs/2501.13126 通过学生选择预测为多项选择题生成合理的干扰项 本研究提出了一个管道，用于训练模型以生成更有可能被学生选择的干扰项。首先，我们训练一个成对排名器来推理学生的误解并评估两个干扰项的相对合理性。使用这个模型，我们创建了一个成对干扰项排名的数据集，然后通过直接偏好优化 （DPO） 训练干扰项生成器，以生成更合理的干扰项。 https://arxiv.org/abs/2501.13125 辩论有助于从弱到强的概括 将已经有能力的模型与所需行为保持一致的常见方法依赖于人类提供监督的能力。然而，未来的超人模型将超过人类的能力。因此，人类将只能弱地监督超人模型。人类评估的这种预期缺陷将削弱未来人工智能系统的安全性。可扩展的监督和从弱到强的泛化是解决这个问题的两种互补方法。在本文中，我们试图结合这两种方法的优势来进一步提高对齐。具体来说，我们研究了用强预训练模型改进人类监督的方法，然后用增强的弱人类监督来监督强模型。为了进行迭代实证进展，我们考虑一个类比：我们可以使用强模型来改进弱模型监督，然后用它来监督强模型吗？我们通过在大型强模型的额外帮助下在地面实况标签上微调一个小的弱模型来实证测试它，然后在弱模型生成的标签上微调强模型。我们发现，辩论可以帮助弱模型从不可信的强模型中提取可信信息，从而在训练弱模型时提供杠杆作为样本的上下文。我们还表明，弱模型的集合有助于利用强模型辩论者生成的长论点并获得更稳健的监督估计。对 OpenAI 从弱到强的 NLP 基准测试的广泛实验表明，组合方法会导致更好的对齐，这表明辩论有可能帮助从弱到强的泛化。 https://arxiv.org/abs/2501.13124 零样本验证引导的思路链 专注于LLM在完全零镜头状态下通过 COT 提示对自生成推理步骤进行基于自我验证 https://arxiv.org/abs/2501.13122 MyGO Multiplex CoT：一种通过双链思维在大型语言模型中进行自我反思的方 Multiplex CoT（思维链），这是一种通过启动双思维链 （CoT） 思维来模拟LLMs推理时自我审查的方法。Multiplex CoT 利用迭代推理的力量，其中模型生成初始思维链，然后通过第二轮思维生成来批评和完善该推理。这种递归方法可以得到更连贯、更合乎逻辑和更稳健的答案，从而改善整体决策过程。——我们演示了如何在现有LLM架构中使用简单的提示工程有效地实现这种方法，从而获得类似于学习-细化模型 （LRM） 的效果，而无需额外的培训。 https://arxiv.org/abs/2501.13117 关于 AI 模型的推理能力以及如何量化它 以多项选择推理任务中的位置偏差为案例研究，我们展示了系统性扰动如何揭示模型决策的基本方面。为了分析这些行为，我们开发了两个互补的现象学模型：一个概率混合模型 （PMM），它将模型响应分解为推理、记忆和猜测组件，以及一个信息论一致性 （ITC） 分析，量化模型置信度和策略选择之间的关系。通过对推理基准的对照实验，我们表明真正的推理对于当前的模型来说仍然具有挑战性，明显的成功往往依赖于记忆和模式匹配的复杂组合，而不是真正的逻辑推理。更根本的是，我们证明了准确性本身往往夸大了模型的推理能力，因为模型行为可以通过认知策略相空间中的潜在机制来表征，揭示了模型在响应查询时如何动态平衡不同的方法。 此框架为实际部署启用定量标准，允许应用程序根据策略分布而不是聚合性能指标指定可靠性阈值。 https://arxiv.org/abs/2501.13833">
<meta name="application-name" content="LLM-DailyDigest">
<meta name="apple-mobile-web-app-title" content="LLM-DailyDigest"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" href="http://localhost:1313/updates/2025-01-26/" /><link rel="prev" href="http://localhost:1313/updates/readme/" /><link rel="next" href="http://localhost:1313/updates/2025-01-27/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "2025-01-26科研追新(公众号)",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "http:\/\/localhost:1313\/updates\/2025-01-26\/"
    },"genre": "updates","wordcount":  124 ,
    "url": "http:\/\/localhost:1313\/updates\/2025-01-26\/","datePublished": "2025-01-26T00:00:00+08:00","dateModified": "2025-01-26T00:00:00+08:00","publisher": {
      "@type": "Organization",
      "name": ""},"author": {
        "@type": "Person",
        "name": "Author"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="sticky" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper" data-page-style="normal"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper">
    <div class="header-title">
      <a href="/" title="LLM-DailyDigest"><img loading="lazy" src="/fixit.svg" data-title="LLM-DailyDigest" data-alt="LLM-DailyDigest" class="logo" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}"/><span id="typeit-header-desktop" class="typeit"></span></a><span class="header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/posts/llm-dailydigest"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 详情</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/resources/"
                
                
              >资源</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/topic/"
                
                
              >主题</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/updates/"
                
                
              >日报</a></li><li class="menu-item delimiter"></li><li class="menu-item theme-switch" title="Switch Theme">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li></ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="LLM-DailyDigest"><img loading="lazy" src="/fixit.svg" data-title="/fixit.svg" data-alt="/fixit.svg" class="logo" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const i of ['style', 'data-title','data-alt','onerror','onload']){this.removeAttribute(i);}"/><span id="typeit-header-title-mobile" class="typeit"></span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/posts/llm-dailydigest"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 详情</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/resources/"
                  
                  
                >资源</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/topic/"
                  
                  
                >主题</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/updates/"
                  
                  
                >日报</a></li><li class="menu-item menu-system">
          <span class="menu-system-item theme-switch" title="Switch Theme"><i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i></span></li>
      </ul>
    </nav>
  </div>
</header><main class="container"><aside class="toc" id="toc-auto"><h2 class="toc-title">Contents&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden="true"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom">
    </aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX"><span>2025-01-26科研追新(公众号)</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      </span></span></div>
      <div class="post-meta-line"><span title="published on 2025-01-26 00:00:00"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden="true"></i><time datetime="2025-01-26">2025-01-26</time></span>&nbsp;<span title="Updated on 2025-01-26 00:00:00"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden="true"></i><time datetime="2025-01-26">2025-01-26</time></span>&nbsp;<span title="124 words"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden="true"></i>About 200 words</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden="true"></i>One minute</span>&nbsp;</div>
    </div><div class="details toc" id="toc-static" data-kept="false">
        <div class="details-summary toc-title">
          <span>Contents</span><i class="details-icon fa-solid fa-angle-right" aria-hidden="true"></i></div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#1-公众号">1. 公众号</a>
      <ul>
        <li><a href="#11-量子位">1.1 量子位</a></li>
      </ul>
    </li>
    <li><a href="#2-arxiv">2. Arxiv</a></li>
    <li><a href="#21-计算和语言">2.1 计算和语言</a></li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><h1 id="2025-01-26">2025-01-26</h1>
<p>统计1-25-14:00～1-26-14:00的相关新进展</p>
<h2 id="1-公众号">1. 公众号</h2>
<h3 id="11-量子位">1.1 量子位</h3>
<table>
  <thead>
      <tr>
          <th>标题</th>
          <th>内容</th>
          <th>链接</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Open R1</td>
          <td>HuggingFace开源项目，目的是构建DeepSeek R1 pipeline中确实的部分，一遍所有人都能在此之上复制和构建R1。</td>
          <td>DeepSeek-R1持续刷屏，连Open R1都来了！抱抱脸发起，1天狂揽1.9k星 <a href="https://mp.weixin.qq.com/s/BX2iTak6bPAKdj6Lv1Lt3A"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/BX2iTak6bPAKdj6Lv1Lt3A</a></td>
      </tr>
  </tbody>
</table>
<p><a href="https://github.com/huggingface/open-r1"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/huggingface/open-r1</a></p>
<p>研究团队以DeepSeek-R1技术报告为指导，将整个复刻过程划分为三个关键步骤。</p>
<ul>
<li>**步骤 1：**通过从DeepSeek-R1蒸馏高质量语料库，复现R1-Distill模型。</li>
<li>**步骤 2：**复现DeepSeek用于创建R1-Zero的纯强化学习（RL）流程。这可能需要为数学、推理和代码任务策划新的大规模数据集。</li>
<li>**步骤 3：**展示我们如何通过多阶段训练，从基础模型发展到经过RL调优的模型。</li>
</ul>
<h2 id="2-arxiv">2. Arxiv</h2>
<p>只显示1.24周五的相关工作</p>
<h2 id="21-计算和语言">2.1 计算和语言</h2>
<p>研究主要探讨了大型语言模型（LLMs）在如下领域的应用和挑战，包括参数高效微调、几何结构理解、数学推理评估、领域调优、长期计划生成、自引用因果循环、代码转换、模型一致性、预训练优化、干扰项生成、从弱到强的概括、零样本验证、双链思维链以及推理能力量化。</p>
<table>
  <thead>
      <tr>
          <th>标题</th>
          <th>内容</th>
          <th>链接</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>基础模型的参数高效微调</strong></td>
          <td>本调查旨在全面概述应用于不同 FM 的 PEFT 技术，并解决理解技术、趋势和应用方面的关键差距</td>
          <td><a href="https://arxiv.org/abs/2501.13787"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2501.13787</a></td>
      </tr>
      <tr>
          <td><strong>大型语言模型真的理解几何结构吗？</strong></td>
          <td>1. 引入了 GeomRel 数据集，旨在通过分离问题解决中几何关系识别的核心步骤来评估LLMs对几何结构的理解；2. 进一步提出了几何思维链 （GeoCoT） 方法，该方法增强了LLMs识别几何关系的能力，从而显着提高了性能</td>
          <td><a href="https://arxiv.org/abs/2501.13773"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2501.13773</a></td>
      </tr>
      <tr>
          <td><strong>UGMathBench：使用大型语言模型进行本科生数学推理的多样化和动态基准</strong></td>
          <td>引入了 UGMathBench，这是一个多样化且动态的基准测试，专门用于评估本科生水平的数学推理</td>
          <td><a href="https://arxiv.org/abs/2501.13766"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2501.13766</a></td>
      </tr>
      <tr>
          <td><strong>如何在保持通用能力的同时完成域调优LLM：自适应分层和元素正则化</strong></td>
          <td>1. 利用双目标优化策略：（1） 正则化损失以保留对一般知识至关重要的参数;（2） 交叉熵损失以适应特定领域的任务。；2. 引入了逐层系数来解释不同层的不同贡献，动态平衡双目标优化</td>
          <td><a href="https://arxiv.org/abs/2501.13669"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2501.13669</a></td>
      </tr>
      <tr>
          <td><strong>LLMs只有我们告诉他们才能计划</strong></td>
          <td>是否可以LLMs独立生成与人类基线相媲美的长期计划。我们对思想算法 （AoT） 的新颖增强，我们称之为 AoT+，有助于在规划基准方面取得最先进的结果，从而超越了竞争性先前的方法和人类基线</td>
          <td><a href="https://arxiv.org/abs/2501.13545"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2501.13545</a></td>
      </tr>
      <tr>
          <td><strong>回顾：语言模型中的类似库的行为通过自引用因果循环得到增强</strong></td>
          <td>引入了自引用因果循环（缩写为 RECALL）的概念——一种使大型语言模型 能够绕过单向因果关系限制的机制，这是被称为反转诅咒的现象的基础</td>
          <td><a href="https://arxiv.org/abs/2501.13491"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2501.13491</a></td>
      </tr>
      <tr>
          <td><strong>代码转换阿拉伯语 NLP 调查：进展、挑战和未来方向</strong></td>
          <td>多样化的语言景观导致了阿拉伯语变体内部以及阿拉伯语和外语之间的代码转换。代码转换在整个地区的广泛发生使得在开发语言技术时满足这些语言需求变得至关重要。在本文中，我们回顾了代码转换阿拉伯语 NLP 领域的当前文献，为正在进行的努力、挑战、研究差距提供了广阔的视角，并对未来的研究方向提出了建议。</td>
          <td><a href="https://arxiv.org/abs/2501.13419"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2501.13419</a></td>
      </tr>
      <tr>
          <td><strong>按照我们所做的做，而不是按照您的想法做：大型语言模型的一致性</strong></td>
          <td>引入了 BenchForm，这是一种新的面向一致性的基准，具有推理密集型任务和五种不同的交互协议，旨在探测LLMs协作场景中的行为。</td>
          <td><a href="https://arxiv.org/abs/2501.13381"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2501.13381</a></td>
      </tr>
      <tr>
          <td><strong>偏好课程：LLMs应始终使用其首选数据进行预训练</strong></td>
          <td>目前的大型语言模型（LLMs）通常在整个预训练过程中使用一致的数据分布。然而，随着模型能力的提高，它应该直观地用差异化的数据进行预训练。为了实现这一目标，我们提出了基于困惑差异的偏好课程学习（PDPC）框架，该框架始终感知并使用首选数据LLMs来训练和提升它们。首先，我们引入PD指标来衡量强模型和弱模型对样本的拟合程度的差异。PD高的样本对弱模型来说学习起来更具挑战性，更适合在预训练的后期进行安排。其次，我们提出了PD偏好函数来逼近模型，随时预测模型的数据偏好LLM，从而离线完成整个数据的排列，保证连续训练不中断。</td>
          <td><a href="https://arxiv.org/abs/2501.13126"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2501.13126</a></td>
      </tr>
      <tr>
          <td><strong>通过学生选择预测为多项选择题生成合理的干扰项</strong></td>
          <td>本研究提出了一个管道，用于训练模型以生成更有可能被学生选择的干扰项。首先，我们训练一个成对排名器来推理学生的误解并评估两个干扰项的相对合理性。使用这个模型，我们创建了一个成对干扰项排名的数据集，然后通过直接偏好优化 （DPO） 训练干扰项生成器，以生成更合理的干扰项。</td>
          <td><a href="https://arxiv.org/abs/2501.13125"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2501.13125</a></td>
      </tr>
      <tr>
          <td><strong>辩论有助于从弱到强的概括</strong></td>
          <td>将已经有能力的模型与所需行为保持一致的常见方法依赖于人类提供监督的能力。然而，未来的超人模型将超过人类的能力。因此，人类将只能弱地监督超人模型。人类评估的这种预期缺陷将削弱未来人工智能系统的安全性。可扩展的监督和从弱到强的泛化是解决这个问题的两种互补方法。在本文中，我们试图结合这两种方法的优势来进一步提高对齐。具体来说，我们研究了用强预训练模型改进人类监督的方法，然后用增强的弱人类监督来监督强模型。为了进行迭代实证进展，我们考虑一个类比：我们可以使用强模型来改进弱模型监督，然后用它来监督强模型吗？我们通过在大型强模型的额外帮助下在地面实况标签上微调一个小的弱模型来实证测试它，然后在弱模型生成的标签上微调强模型。我们发现，辩论可以帮助弱模型从不可信的强模型中提取可信信息，从而在训练弱模型时提供杠杆作为样本的上下文。我们还表明，弱模型的集合有助于利用强模型辩论者生成的长论点并获得更稳健的监督估计。对 OpenAI 从弱到强的 NLP 基准测试的广泛实验表明，组合方法会导致更好的对齐，这表明辩论有可能帮助从弱到强的泛化。</td>
          <td><a href="https://arxiv.org/abs/2501.13124"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2501.13124</a></td>
      </tr>
      <tr>
          <td><strong>零样本验证引导的思路链</strong></td>
          <td>专注于LLM在完全零镜头状态下通过 COT 提示对自生成推理步骤进行基于自我验证</td>
          <td><a href="https://arxiv.org/abs/2501.13122"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2501.13122</a></td>
      </tr>
      <tr>
          <td><strong>MyGO Multiplex CoT：一种通过双链思维在大型语言模型中进行自我反思的方</strong></td>
          <td>Multiplex CoT（思维链），这是一种通过启动双思维链 （CoT） 思维来模拟LLMs推理时自我审查的方法。Multiplex CoT 利用迭代推理的力量，其中模型生成初始思维链，然后通过第二轮思维生成来批评和完善该推理。这种递归方法可以得到更连贯、更合乎逻辑和更稳健的答案，从而改善整体决策过程。——我们演示了如何在现有LLM架构中使用简单的提示工程有效地实现这种方法，从而获得类似于学习-细化模型 （LRM） 的效果，而无需额外的培训。</td>
          <td><a href="https://arxiv.org/abs/2501.13117"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2501.13117</a></td>
      </tr>
      <tr>
          <td><strong>关于 AI 模型的推理能力以及如何量化它</strong></td>
          <td>以多项选择推理任务中的位置偏差为案例研究，我们展示了系统性扰动如何揭示模型决策的基本方面。为了分析这些行为，我们开发了两个互补的现象学模型：一个概率混合模型 （PMM），它将模型响应分解为推理、记忆和猜测组件，以及一个信息论一致性 （ITC） 分析，量化模型置信度和策略选择之间的关系。通过对推理基准的对照实验，我们表明真正的推理对于当前的模型来说仍然具有挑战性，明显的成功往往依赖于记忆和模式匹配的复杂组合，而不是真正的逻辑推理。更根本的是，我们证明了准确性本身往往夸大了模型的推理能力，因为模型行为可以通过认知策略相空间中的潜在机制来表征，揭示了模型在响应查询时如何动态平衡不同的方法。 此框架为实际部署启用定量标准，允许应用程序根据策略分布而不是聚合性能指标指定可靠性阈值。</td>
          <td><a href="https://arxiv.org/abs/2501.13833"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2501.13833</a></td>
      </tr>
  </tbody>
</table>
</div></article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">Public - 2025</span><span class="author" itemprop="copyrightHolder">
              <a href="/"></a></span></div><div class="footer-line statistics"></div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="Back to Top"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric d-none">0%</span>
        </div></div><div id="mask"></div><noscript>
    <div class="noscript-warning">Theme FixIt works best with JavaScript enabled.</div>
  </noscript>
</div><link rel="preload" href="/lib/katex/katex.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/katex/katex.min.css"></noscript><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/typeit/index.umd.js" defer></script><script src="/lib/katex/katex.min.js" defer></script><script src="/lib/katex/auto-render.min.js" defer></script><script src="/lib/katex/copy-tex.min.js" defer></script><script src="/lib/katex/mhchem.min.js" defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script>window.config={"code":{"copyTitle":"Copy to clipboard","editLockTitle":"Lock editable code block","editUnLockTitle":"Unlock editable code block","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-desktop":"LLM-DailyDigest 大模型研究日报","typeit-header-title-mobile":"LLM-DailyDigest 大模型研究日报"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-desktop":["typeit-header-desktop"],"typeit-header-title-mobile":["typeit-header-title-mobile"]},"duration":-1,"loop":false,"speed":100}};</script><script src="/js/theme.min.js" defer></script></body>
</html>
