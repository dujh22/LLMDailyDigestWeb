import pandas as pd
from datetime import datetime
import argparse
import os
from collections import defaultdict
import textwrap
import ast
import csv

# é…ç½®å‚æ•°
DAILY_REPORT_DIR = "/Users/djh/Documents/GitHub/LLM-DailyDigest/updates"
TOP_N_TRENDING = 5 # æ˜¾ç¤ºæœ€çƒ­é—¨è®ºæ–‡çš„æ•°é‡
THEME_KEYWORDS_OR = {
    "å¤§æ¨¡å‹": [
        "å¤§æ¨¡å‹", "è¯­è¨€æ¨¡å‹", "LLM", "ç”Ÿæˆå¼æ¨¡å‹", "è‡ªç›‘ç£å­¦ä¹ ", "Transformer", 
        "é¢„è®­ç»ƒ", "å¾®è°ƒ", "å¼ºåŒ–å­¦ä¹ ", "å¤šæ¨¡æ€"
    ],
    "æ¨ç†": [
        "æ¨ç†", "o1", "æ•°å­¦", "ä»£ç "
    ],
    "æµ‹è¯„": [
        "æµ‹è¯„", "è¯„ä¼°", "è¯„æµ‹", "éªŒè¯"
    ],
    "æ³›åŒ–": [
        "æ³›åŒ–", "è¿ç§»å­¦ä¹ ", "é¢†åŸŸé€‚åº”", "å¤šä»»åŠ¡å­¦ä¹ ", "å¤šæ¨¡æ€å­¦ä¹ ", "è‡ªå›å½’å­¦ä¹ "
    ],
    "æ•°æ®": [
        "æ•°æ®å¢å¼º", "æ•°æ®", "æ ‡æ³¨", "å¤šæ¨¡æ€æ•°æ®", "æ¸…æ´—"
    ],
    "äººå·¥æ™ºèƒ½ä¼¦ç†ä¸ç¤¾ä¼š": [
        "ä¼¦ç†", "å¯è§£é‡Šæ€§", "å…¬å¹³æ€§", "ç®—æ³•åè§", "è‡ªåŠ¨åŒ–å†³ç­–", "éšç§ä¿æŠ¤", 
        "ç¤¾ä¼šå½±å“", "AIæ²»ç†", "ç®—æ³•é€æ˜åº¦"
    ],
    "å¤šæ¨¡æ€": [
        "å¤šæ¨¡æ€", "è§†è§‰", "è¯­éŸ³", "å›¾åƒ", "è§†é¢‘", "éŸ³é¢‘", "å›¾è°±"
    ]
}
THEME_KEYWORDS_AND = {
    "å¤§æ¨¡å‹æ¨ç†": [
        {"å¤§æ¨¡å‹", "æ¨ç†"}, 
        {"è¯­è¨€æ¨¡å‹", "æ¨ç†"}, 
        {"LLM", "æ¨ç†"},
    ],
    "å¤§æ¨¡å‹æ•°å­¦æ¨ç†": [
        {"å¤§æ¨¡å‹", "æ•°å­¦", "æ¨ç†"},
        {"è¯­è¨€æ¨¡å‹", "æ•°å­¦", "æ¨ç†"},
        {"LLM", "æ•°å­¦", "æ¨ç†"}
    ],
    "å¤§æ¨¡å‹ä»£ç æ¨ç†": [
        {"å¤§æ¨¡å‹", "ä»£ç ", "æ¨ç†"},
        {"è¯­è¨€æ¨¡å‹", "ä»£ç ", "æ¨ç†"},
        {"LLM", "ä»£ç ", "æ¨ç†"}
    ],
    "å¤§æ¨¡å‹ç±»o1æ¨ç†": [
        {"å¤§æ¨¡å‹", "o1", "æ¨ç†"},
        {"è¯­è¨€æ¨¡å‹", "o1", "æ¨ç†"},
        {"LLM", "o1", "æ¨ç†"}
    ],
    "å¤§æ¨¡å‹æ•°å­¦æ¨ç†çš„æ³›åŒ–æ€§": [
        {"å¤§æ¨¡å‹", "æ•°å­¦", "æ¨ç†", "æ³›åŒ–"},
        {"è¯­è¨€æ¨¡å‹", "æ•°å­¦", "æ¨ç†", "æ³›åŒ–"},
        {"LLM", "æ•°å­¦", "æ¨ç†", "æ³›åŒ–"}
    ],
}

# é¢„å…ˆè¯»å…¥åˆ†ç±»
father_categories = {}
with open('category.csv', mode='r', encoding='utf-8') as infile:
    reader = csv.DictReader(infile)
    for row in reader:
        if row['æ‰€å±å­¦ç§‘'] not in father_categories:
            father_categories[row['æ‰€å±å­¦ç§‘']] = [row['å­¦ç§‘ï¼ˆä¸­æ–‡ï¼‰']]
        else:
            father_categories[row['æ‰€å±å­¦ç§‘']].append(row['å­¦ç§‘ï¼ˆä¸­æ–‡ï¼‰'])

def load_data(file_path):
    """
    åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®

    å‚æ•°:
    file_path (str): æ•°æ®æ–‡ä»¶çš„è·¯å¾„

    è¿”å›:
    DataFrame: é¢„å¤„ç†åçš„æ•°æ®æ¡†
    """
    # ä»CSVæ–‡ä»¶ä¸­è¯»å–æ•°æ®ï¼ŒæŒ‡å®šåˆ†éš”ç¬¦ä¸ºåˆ¶è¡¨ç¬¦ï¼Œå¹¶å°†æ—¥æœŸåˆ—è§£æä¸ºæ—¥æœŸç±»å‹
    df = pd.read_csv(file_path, sep=',', parse_dates=['Publish Date', 'Update Date'])
    
    # å°†Starsåˆ—è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼Œé”™è¯¯å€¼å¡«å……ä¸º0
    df['Stars'] = pd.to_numeric(df['Stars'], errors='coerce').fillna(0)
    
    # è¿”å›é¢„å¤„ç†åçš„æ•°æ®æ¡†
    return df

def classify_theme(summary):
    """é€šè¿‡å…³é”®è¯åŒ¹é…è¿›è¡Œä¸»é¢˜åˆ†ç±»"""
    # å®šä¹‰ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨åŒ¹é…åˆ°çš„ä¸»é¢˜
    themes = []
    # éå†THEME_KEYWORDS_ORå­—å…¸ï¼Œè·å–ä¸»é¢˜å’Œå…³é”®è¯
    for theme, keywords in THEME_KEYWORDS_OR.items():
        # å¦‚æœsummaryä¸­åŒ…å«ä»»æ„ä¸€ä¸ªå…³é”®è¯ï¼Œåˆ™å°†ä¸»é¢˜æ·»åŠ åˆ°themesåˆ—è¡¨ä¸­
        if any(kw in summary for kw in keywords):
            themes.append(theme)
    # éå†THEME_KEYWORDS_ANDå­—å…¸ï¼Œè·å–ä¸»é¢˜å’Œå…³é”®è¯å¯¹
    for theme, keyword_pairs in THEME_KEYWORDS_AND.items():
        # å¦‚æœsummaryä¸­åŒ…å«æ‰€æœ‰å…³é”®è¯å¯¹ä¸­çš„å…³é”®è¯ï¼Œåˆ™å°†ä¸»é¢˜æ·»åŠ åˆ°themesåˆ—è¡¨ä¸­
        if any(all(kw in summary for kw in pair) for pair in keyword_pairs):
            themes.append(theme)
    # å¦‚æœthemesåˆ—è¡¨ä¸ºç©ºï¼Œåˆ™è¿”å›["å…¶ä»–"]
    return themes if themes else ["å…¶ä»–"]

def generate_daily_report(target_date, df, is_summary=False):
    """ç”Ÿæˆæ—¥æŠ¥æ ¸å¿ƒå†…å®¹"""
    # å°†ç›®æ ‡æ—¥æœŸæ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
    date_str = target_date.strftime("%Y-%m-%d")
    
    # ç­›é€‰å½“æ—¥å‘å¸ƒçš„æ–°è®ºæ–‡
    new_papers = df[df['Publish Date'].dt.date == target_date.date()]
    # ç­›é€‰å½“æ—¥æ›´æ–°çš„è®ºæ–‡
    updated_papers = df[
        (df['Update Date'].dt.date == target_date.date()) & 
        (df['Publish Date'].dt.date != target_date.date())
    ]
    if is_summary == False:
        # df = new_papers + updated_papers
        df = pd.concat([new_papers, updated_papers], ignore_index=True)
    
    # ç”Ÿæˆè¶‹åŠ¿åˆ†æï¼ŒæŒ‰æ˜Ÿæ˜Ÿæ•°å’Œæ›´æ–°æ—¥æœŸæ’åºï¼Œå–å‰TOP_N_TRENDINGç¯‡
    trending = df.sort_values(by=['Stars', 'Update Date'], ascending=False).head(TOP_N_TRENDING)
    
    # ä¸»é¢˜åˆ†ç±»ç»Ÿè®¡å’Œarxivåˆ†ç±»ç»Ÿè®¡
    theme_dist = defaultdict(list) # æ•°æ®ç»“æ„ï¼š{ä¸»é¢˜: [è®ºæ–‡1, è®ºæ–‡2, ...]}
    arxiv_theme_dist = defaultdict(list)
    for _, row in df.iterrows():
        # å¯¹æ¯ç¯‡è®ºæ–‡çš„æ‘˜è¦è¿›è¡Œä¸»é¢˜åˆ†ç±»
        themes = classify_theme(row['Summary'])
        arxiv_themes = ast.literal_eval(row['Categories'])
        for theme in themes:
            # å°†è®ºæ–‡æ·»åŠ åˆ°å¯¹åº”çš„ä¸»é¢˜åˆ—è¡¨ä¸­
            theme_dist[theme].append(row)
        for arxiv_theme in arxiv_themes:
            # å°†è®ºæ–‡æ·»åŠ åˆ°å¯¹åº”çš„arxivåˆ†ç±»åˆ—è¡¨ä¸­
            arxiv_theme_dist[arxiv_theme].append(row)
    
    # æ„å»ºMarkdownå†…å®¹
    content = []
    # æ·»åŠ æ—¥æŠ¥æ ‡é¢˜
    content.append(f"# å­¦æœ¯æ—¥æŠ¥ {date_str}\n")
    
    # å½“æ—¥æ¦‚è§ˆ
    content.append("## ğŸ“Š å½“æ—¥æ¦‚è§ˆ")
    # æ·»åŠ æ–°å¢è®ºæ–‡æ•°é‡
    content.append(f"- æ–°å¢è®ºæ–‡: {len(new_papers)} ç¯‡")
    # æ·»åŠ æ›´æ–°è®ºæ–‡æ•°é‡
    content.append(f"- æ›´æ–°è®ºæ–‡: {len(updated_papers)} ç¯‡")
    # æ·»åŠ æœ€çƒ­é—¨è®ºæ–‡æ ‡é¢˜å’Œæ˜Ÿæ˜Ÿæ•°
    content.append(f"- æœ€çƒ­é—¨è®ºæ–‡: {trending.iloc[0]['Title'][:30]}... (â­{trending.iloc[0]['Stars']})\n")

    # è·å–æœ€æ—©çš„æ—¶é—´
    earliest_date = df['Publish Date'].min().date()
    # è·å–æœ€æ™šçš„æ—¶é—´
    latest_date = df['Publish Date'].max().date()
    # æ·»åŠ æ€»ç»“ä¿¡æ¯
    content.append(f"## ğŸ“… æ€»ç»“ {earliest_date} è‡³ {latest_date}")

    # ä¸»é¢˜åˆ†å¸ƒ
    content.append("## ğŸ§© ä¸»é¢˜åˆ†å¸ƒ")
    # è·å–è¿™ä¸€å¥è¯çš„åºå·
    seq = content.index("## ğŸ§© ä¸»é¢˜åˆ†å¸ƒ")
    for theme, papers in sorted(theme_dist.items(), key=lambda x: len(x[1]), reverse=True):
        # æ·»åŠ ä¸»é¢˜æ ‡é¢˜å’Œè®ºæ–‡æ•°é‡
        content.append(f"### {theme} ({len(papers)}ç¯‡)")
        # åŒæ—¶æ’å…¥åˆ°seqä¹‹å‰
        content.insert(seq, f"  -- {theme} ({len(papers)}ç¯‡)")

        # æ·»åŠ ä»£è¡¨æ€§è®ºæ–‡æ ‡é¢˜
        content.append(f"**ä»£è¡¨æ€§è®ºæ–‡**: {papers[0]['Title'][:50]}...")
        # æ·»åŠ æœ€æ–°è¿›å±•
        content.append("**æœ€æ–°è¿›å±•**:")
        # æ·»åŠ æ‘˜è¦çš„ç¬¬ä¸€è¡Œ
        content.append(textwrap.wrap(papers[0]['Summary'], width=200)[0] + "...\n")
        # å…¨éƒ¨è®ºæ–‡æ ‡é¢˜
        content.append("**å…¨éƒ¨è®ºæ–‡**:")
        for paper in papers:
            # æ ¼å¼åŒ–ä¸»é¢˜è®ºæ–‡ä¿¡æ¯
            content.append(f"- {paper['Title']} ({paper['First Author']}) [è·³è½¬]({paper['URL']})")

    # arXivåˆ†ç±»åˆ†å¸ƒ
    content.append("## ğŸ—‚ arXivåˆ†ç±»åˆ†å¸ƒ")
    for arxiv_father_theme in father_categories.keys():
        content.append(f"### {arxiv_father_theme}")
        for arxiv_theme in father_categories[arxiv_father_theme]:
            papers = arxiv_theme_dist.get(arxiv_theme, [])
            if not papers:
                continue
            # æ·»åŠ arXivåˆ†ç±»æ ‡é¢˜å’Œè®ºæ–‡æ•°é‡
            content.append(f"#### {arxiv_theme} ({len(papers)}ç¯‡)")
            # æ·»åŠ ä»£è¡¨æ€§è®ºæ–‡æ ‡é¢˜
            content.append(f"**ä»£è¡¨æ€§è®ºæ–‡**: {papers[0]['Title'][:50]}...")
            # æ·»åŠ æœ€æ–°è¿›å±•
            content.append("**æœ€æ–°è¿›å±•**:")
            # æ·»åŠ æ‘˜è¦çš„ç¬¬ä¸€è¡Œ
            content.append(textwrap.wrap(papers[0]['Summary'], width=200)[0] + "...\n")
            # å…¨éƒ¨è®ºæ–‡æ ‡é¢˜
            content.append("**å…¨éƒ¨è®ºæ–‡**:")
            for paper in papers:
                # æ ¼å¼åŒ–arXivåˆ†ç±»è®ºæ–‡ä¿¡æ¯
                content.append(f"- {paper['Title']} ({paper['First Author']}) [è·³è½¬]({paper['URL']})")

    # è¶‹åŠ¿è®ºæ–‡
    content.append("## ğŸ“ˆ è¶‹åŠ¿è®ºæ–‡")
    for _, paper in trending.iterrows():
        # æ ¼å¼åŒ–è¶‹åŠ¿è®ºæ–‡ä¿¡æ¯
        content.append(format_paper(paper, "çƒ­é—¨"))

    # æ–°å¢è®ºæ–‡
    if not new_papers.empty:
        # æ·»åŠ æ–°å¢è®ºæ–‡æ ‡é¢˜
        content.append("## ğŸ†• æ–°å¢è®ºæ–‡")
        for _, paper in new_papers.iterrows():
            # æ ¼å¼åŒ–æ–°å¢è®ºæ–‡ä¿¡æ¯
            content.append(format_paper(paper, "æ–°è®ºæ–‡"))
    
    # æ›´æ–°è®ºæ–‡
    if not updated_papers.empty:
        # æ·»åŠ æ›´æ–°è®ºæ–‡æ ‡é¢˜
        content.append("## ğŸ”„ æ›´æ–°è®ºæ–‡")
        for _, paper in updated_papers.iterrows():
            # æ ¼å¼åŒ–æ›´æ–°è®ºæ–‡ä¿¡æ¯
            content.append(format_paper(paper, "æ›´æ–°")) 

    # å°†å†…å®¹åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶è¿”å›
    return "\n".join(content)

def format_paper(paper, badge):
    """æ ¼å¼åŒ–å•ç¯‡è®ºæ–‡ä¿¡æ¯"""
    code_link = f"[ä»£ç ]({paper['Code URL']})" if pd.notna(paper['Code URL']) else "æ— ä»£ç "
    return f"""
### {paper['Title']}
**{badge}** â­{paper['Stars']} | {paper['Publish Date'].date()} | {code_link}  
**ä½œè€…**: {paper['First Author']}  
**æ‘˜è¦**: {textwrap.shorten(paper['Summary'], width=200, placeholder='...')}  
[é˜…è¯»å…¨æ–‡]({paper['URL']})
"""

def arxiv_to_daily_report():
    """ä¸»å‡½æ•°ï¼Œç”Ÿæˆå­¦æœ¯æ—¥æŠ¥"""
    parser = argparse.ArgumentParser(description="ç”Ÿæˆå­¦æœ¯æ—¥æŠ¥")
    parser.add_argument("--data_file", help="è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„", required=True)
    parser.add_argument("--date", help="æŒ‡å®šæ—¥æœŸ (YYYY-MM-DD)", default=datetime.today().date())
    parser.add_argument("--is_summary", help="æ˜¯å¦æ€»ç»“ä¹‹å‰çš„æ—¥æŠ¥", default=False)
    parser.add_argument("--dairy_report_dir", help="æ—¥æŠ¥ä¿å­˜ç›®å½•", default=DAILY_REPORT_DIR)
    args = parser.parse_args()

    df = load_data(args.data_file)
    report_content = generate_daily_report(pd.to_datetime(args.date), df, args.is_summary)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.dairy_report_dir, exist_ok=True)
    
    # ä¿å­˜æ–‡ä»¶
    if args.is_summary:
        filename = f"arxiv_daily_report_summary_{args.date}.md"
    else:
        filename = f"arxiv_daily_report_{args.date}.md"
    with open(os.path.join(args.dairy_report_dir, filename), 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"æ—¥æŠ¥å·²ç”Ÿæˆï¼š{os.path.join(args.dairy_report_dir, filename)}")

if __name__ == "__main__":
    arxiv_to_daily_report()